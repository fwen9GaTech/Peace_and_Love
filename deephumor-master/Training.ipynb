{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf2cfeb-d92b-48d6-8520-e4f8ae7a28c7",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1ayyWPuOw8ET2SRZ5KD-r4dwMH4jBn-B8#scrollTo=h2GTb3WrrH7y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f91b7-e785-41d7-ae27-030d439b32dc",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3f11a0-e241-4a37-b9d3-d00dafa15388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f315ceb-d072-4687-8cfb-1da119dbe456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\frank\\\\OneDrive\\\\Desktop\\\\7643_Project\\\\deephumor-master'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1943903-8fb5-4603-b413-4f178505cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephumor.data.vocab import build_vocab_from_file\n",
    "from deephumor.data.tokenizers import WordPunctTokenizer, CharTokenizer\n",
    "\n",
    "DATA_DIR = 'C:/Users/frank/OneDrive/Desktop/7643_Project/dataset/memes900k'\n",
    "CAPTIONS_FILE = os.path.join(DATA_DIR, 'captions_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df7be7f-091c-4f30-9835-c4f3f49d4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CAPTIONS_FILE, on_bad_lines = 'skip', header = None, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962bfe4c-0d32-4d4c-b6c6-e70e7ba4eb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y U No</td>\n",
       "      <td>984</td>\n",
       "      <td>Victoria &lt;sep&gt; y u no tell us your secret?!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y U No</td>\n",
       "      <td>908</td>\n",
       "      <td>KONY &lt;sep&gt; Y u no take justin bieber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y U No</td>\n",
       "      <td>823</td>\n",
       "      <td>Google &lt;sep&gt; Y U NO LET ME FINISH TYPING?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y U No</td>\n",
       "      <td>727</td>\n",
       "      <td>universal remote &lt;sep&gt; y u no work on universe?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y U No</td>\n",
       "      <td>707</td>\n",
       "      <td>pink floyd &lt;sep&gt; y u no need no education?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1                                                2\n",
       "0  Y U No  984      Victoria <sep> y u no tell us your secret?!\n",
       "1  Y U No  908             KONY <sep> Y u no take justin bieber\n",
       "2  Y U No  823        Google <sep> Y U NO LET ME FINISH TYPING?\n",
       "3  Y U No  727  universal remote <sep> y u no work on universe?\n",
       "4  Y U No  707       pink floyd <sep> y u no need no education?"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd8fa51-0c3f-4f74-b5ad-a7821ff1ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cap = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "639748be-8aec-44cf-9b93-0da447ab70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CAPTIONS_FILE, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        label, _, caption = line.strip().split('\\t')\n",
    "        list_cap.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd1659e-5427-4947-9f87-dfec6d607688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joined isis to meet girls <sep> they made him a suicide bomber',\n",
       " \"Appeared in a girl's dream <sep> nightmare\",\n",
       " 'tried to step on a spider <sep> The spider was actully a nail...',\n",
       " 'Finds out who put the bomp in the bomp-shabomp-shabomp <sep> Forgets',\n",
       " 'gives dollar to bum <sep> needs it back',\n",
       " \"There are 3.2 Billion women on earth <sep> Still can't get laid\",\n",
       " 'front row tickets for a comedy night <sep> pete prodge is the headline act',\n",
       " 'reached for the last slice <sep> got my hand slapped',\n",
       " 'Spends 6500 credits on repairs <sep> Picks up an Instant Repair PowerUp',\n",
       " 'Gets a girlfriend <sep> its his long lost cousin']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_cap[4000:4010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c68259-8405-4a34-b38b-6bc087153114",
   "metadata": {},
   "source": [
    "# Word-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa5015c-2f3e-4b65-9e16-b0c2a22f9b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36541"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIN_DOC_COUNT = 5\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "vocab = build_vocab_from_file(CAPTIONS_FILE, tokenizer, min_df=MIN_DOC_COUNT)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4289cc1-b8b2-4a16-82c0-ae3732e0e25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '<bos>', '<eos>', '<sep>', '<emp>', '!', '!!', '!!!', '!!!\"', \"!!!'\", '!!!)', '!!!*', '!!\"', \"!!'\", \"!!''\", '!!)', '!!,', '!!.', '!!?', '!\"', \"!'\", \"!''\", '!)', '!*', '!,', '!-', '!.', '!..', '!:', '!=', '!?', '!?!', '!?\"', '!??', '\"', '\"!', '\"!!', '\"!!!', '\"!?', '\"#', '\"$', '\"\\'', '\".', '\"..', '\"...', '\"...\"', '\"?', '\"?!', '\"??', '\"???', '#', '#!', '##', '###', '#%', \"#'\", '#*', '#?', '$', '$$', '$$$', '$&', '$,', '$.', '$?', '%', '%!', '%#', '%.', '%...', '%?', '%?!', '&', \"'\", \"''\", \"''i\", \"''the\", \"''you\", \"'12\", \"'86\", \"'a\", \"'a'\", \"'all\", \"'and\", \"'b'\", \"'bad\", \"'bout\", \"'c'\", \"'cause\", \"'come\", \"'cuz\", \"'d\", \"'d'\", \"'do\", \"'don't\", \"'e'\", \"'em\", \"'er\", \"'free\"]\n"
     ]
    }
   ],
   "source": [
    "print(vocab.tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139a75f-0c74-406f-9704-6057d2b28ea0",
   "metadata": {},
   "source": [
    "# Character-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d2528b0-7967-4ee9-b98d-fa6c2b67fb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_chars = CharTokenizer()\n",
    "vocab_chars = build_vocab_from_file(CAPTIONS_FILE, tokenizer_chars, min_df=5)\n",
    "len(vocab_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9546ee-84a1-4fd1-a98f-01caab27661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '<bos>', '<eos>', '<sep>', '<emp>', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', '[', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "print(vocab_chars.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daadb79-a987-44a5-81ce-84829dd0c92b",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7428234c-8ebe-46e9-913a-de1de1ed6a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 5000\n",
      "val 500\n",
      "test 500\n"
     ]
    }
   ],
   "source": [
    "from deephumor.data import MemeDataset\n",
    "\n",
    "# NUM_CLASSES = 200  # use this to limit the dataset size\n",
    "NUM_CLASSES = 2  # use this to limit the dataset size\n",
    "PAD_IDX = vocab.stoi['<pad>']\n",
    "\n",
    "from torchvision import transforms\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "splits = ['train', 'val', 'test']\n",
    "datasets = {\n",
    "    # WORD-LEVEL\n",
    "    split: MemeDataset(DATA_DIR, vocab, tokenizer, image_transform=image_transform,\n",
    "                       num_classes=NUM_CLASSES, split=split, preload_images=True)\n",
    "    \n",
    "    # CHAR-LEVEL\n",
    "    # split: MemeDataset(DATA_DIR, vocab_chars, tokenizer_chars, image_transform=image_transform,\n",
    "    #                    num_classes=NUM_CLASSES, split=split, preload_images=True)\n",
    "    for split in splits\n",
    "}\n",
    "\n",
    "for split in splits:\n",
    "    print(split, len(datasets[split]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f56465-1e7f-4104-99a7-a6185a697ca6",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73020f6b-9486-4473-9a73-c8bb045e5138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4]) torch.Size([128, 23]) torch.Size([128, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([11, 13,  9, 13,  9,  8, 10, 13, 11, 12, 18, 10,  8,  6, 11, 16, 12, 12,\n",
       "        11, 12,  9, 12,  9, 15, 11, 14, 10,  6, 14, 12, 17, 10, 19,  9, 13, 10,\n",
       "        12, 16,  9,  8, 10, 10, 11, 13, 11,  7, 10, 12, 14,  9, 11, 10,  9, 10,\n",
       "        12, 13, 10,  9, 13, 11, 11,  8, 13, 10, 11, 12,  8, 14, 10, 11, 12, 11,\n",
       "         9, 10, 14,  9,  9, 12, 11, 10,  7, 10, 12, 13,  9, 12,  8, 14, 10, 14,\n",
       "         9,  7, 16,  9,  9,  9,  8, 23, 12,  8, 16, 10,  9, 11, 11, 10, 13,  9,\n",
       "        12,  8,  9,  8, 11,  9, 12, 11, 12, 11,  9,  9, 14, 11,  9, 12,  9, 10,\n",
       "        10,  7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from deephumor.data import pad_collate\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "dataloaders = {\n",
    "    split:  DataLoader(dataset=datasets[split], batch_size=BATCH_SIZE, \n",
    "                       shuffle=split == 'train', collate_fn=pad_collate)\n",
    "    for split in splits\n",
    "}\n",
    "\n",
    "for (labels, captions, images) in dataloaders['val']:\n",
    "    print(labels.size(), captions.size(), images.size())\n",
    "    break\n",
    "\n",
    "lengths = captions.size(1) - (captions == PAD_IDX).sum(dim=1)\n",
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7224124-7b7d-454f-a8ed-626588abe2a3",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0275d-7580-405a-a9ce-55af8838089e",
   "metadata": {},
   "source": [
    "### Load models and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d47aa448-2064-4b92-9e65-c88b792e95e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from deephumor.models import CaptioningLSTM, CaptioningLSTMWithLabels, CaptioningTransformer, CaptioningTransformerBase\n",
    "from deephumor.experiments import Trainer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189db78a-5582-45bf-a550-549cb5c63412",
   "metadata": {},
   "source": [
    "# Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fa08f32-e7f4-49f2-9ed9-e939a1833f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOKENS = len(vocab)\n",
    "LEARNING_RATE = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db71f90a-bf38-4ca8-ae86-67b1a4606610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 5000\n",
      "val 500\n",
      "test 500\n",
      "torch.Size([128, 4]) torch.Size([128, 23]) torch.Size([128, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 2\n",
    "# NUM_CLASSES = 200\n",
    "\n",
    "datasets = {\n",
    "    # WORD-LEVEL\n",
    "    split: MemeDataset(DATA_DIR, vocab, tokenizer, image_transform=image_transform,\n",
    "                       num_classes=NUM_CLASSES, split=split, preload_images=True)\n",
    "    \n",
    "    # CHAR-LEVEL\n",
    "    # split: MemeDataset(DATA_DIR, vocab_chars, tokenizer_chars, image_transform=image_transform,\n",
    "    #                    num_classes=NUM_CLASSES, split=split, preload_images=True)\n",
    "    for split in splits\n",
    "}\n",
    "\n",
    "for split in splits:\n",
    "    print(split, len(datasets[split]))\n",
    "\n",
    "dataloaders = {\n",
    "    split:  DataLoader(dataset=datasets[split], batch_size=BATCH_SIZE, \n",
    "                       shuffle=split == 'train', collate_fn=pad_collate)\n",
    "    for split in splits\n",
    "}\n",
    "\n",
    "for (labels, captions, images) in dataloaders['val']:\n",
    "    lengths = captions.size(1) - (captions == PAD_IDX).sum(dim=1)\n",
    "    print(labels.size(), captions.size(), images.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4e1c6-bc89-479d-98cc-469dc919bc07",
   "metadata": {},
   "source": [
    "### Transformer Decoder with Global Image embedding (base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc20a397-8f51-47e6-9c66-61cb457a85c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# parameters: 47240893\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model_transformer = CaptioningTransformerBase(\n",
    "    num_tokens=len(vocab),\n",
    "    hid_dim=512, \n",
    "    n_layers=3, \n",
    "    n_heads=8, \n",
    "    pf_dim=2048,\n",
    "    enc_dropout=0.3, \n",
    "    dec_dropout=0.1, \n",
    "    pad_index=0, \n",
    "    max_len=128\n",
    ").to(device)\n",
    "\n",
    "# out = transformer_model(images.to(device), captions.to(device))\n",
    "# out.size()\n",
    "\n",
    "print('# parameters:', count_parameters(model_transformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8b00dfc-32d8-4825-9567-07c96496b63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 24, 36541])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model_transformer(images.cuda(), captions.cuda())\n",
    "\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6484d313-e5e9-466f-9b96-162add384505",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_transformer = Trainer('TransformerDecoderBaseWords', log_dir='C:\\\\Users\\\\frank\\\\OneDrive\\\\Desktop\\\\7643_Project\\\\codes\\\\deephumor-master\\\\logs', text_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e38a8236-29f2-49a5-a0b6-c38293a74675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 23484), started 5:34:14 ago. (Use '!kill 23484' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-62801f1e4bafbe0b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-62801f1e4bafbe0b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1b3621c-eaa3-4fca-bc7c-bfbfcec52def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20\n",
      "  train loss: 8.10094, perplexity: 6364.940\n",
      "  val   loss: 6.95657, perplexity: 1566.304\n",
      "  epoch time: 28.73s\n",
      "Epoch 02/20\n",
      "  train loss: 6.43369, perplexity: 929.733\n",
      "  val   loss: 6.13372, perplexity: 685.602\n",
      "  epoch time: 28.51s\n",
      "Epoch 03/20\n",
      "  train loss: 5.86891, perplexity: 508.978\n",
      "  val   loss: 5.88458, perplexity: 545.196\n",
      "  epoch time: 39.79s\n",
      "Epoch 04/20\n",
      "  train loss: 5.65066, perplexity: 416.722\n",
      "  val   loss: 5.71437, perplexity: 474.061\n",
      "  epoch time: 39.93s\n",
      "Epoch 05/20\n",
      "  train loss: 5.48487, perplexity: 359.243\n",
      "  val   loss: 5.80910, perplexity: 581.573\n",
      "  epoch time: 39.68s\n",
      "Epoch 06/20\n",
      "  train loss: 5.42726, perplexity: 349.468\n",
      "  val   loss: 5.54040, perplexity: 438.253\n",
      "  epoch time: 46.26s\n",
      "Epoch 07/20\n",
      "  train loss: 5.29142, perplexity: 312.167\n",
      "  val   loss: 5.45215, perplexity: 419.549\n",
      "  epoch time: 42.22s\n",
      "Epoch 08/20\n",
      "  train loss: 5.21179, perplexity: 293.723\n",
      "  val   loss: 5.39442, perplexity: 407.312\n",
      "  epoch time: 44.41s\n",
      "Epoch 09/20\n",
      "  train loss: 5.15006, perplexity: 280.441\n",
      "  val   loss: 5.35468, perplexity: 399.872\n",
      "  epoch time: 38.38s\n",
      "Epoch 10/20\n",
      "  train loss: 5.09787, perplexity: 268.815\n",
      "  val   loss: 5.32249, perplexity: 392.132\n",
      "  epoch time: 38.49s\n",
      "Epoch 11/20\n",
      "  train loss: 5.05782, perplexity: 260.159\n",
      "  val   loss: 5.29617, perplexity: 386.603\n",
      "  epoch time: 37.00s\n",
      "Epoch 12/20\n",
      "  train loss: 5.01603, perplexity: 251.159\n",
      "  val   loss: 5.27516, perplexity: 380.491\n",
      "  epoch time: 39.53s\n",
      "Epoch 13/20\n",
      "  train loss: 4.98491, perplexity: 242.495\n",
      "  val   loss: 5.24923, perplexity: 371.133\n",
      "  epoch time: 38.38s\n",
      "Epoch 14/20\n",
      "  train loss: 4.95533, perplexity: 235.014\n",
      "  val   loss: 5.23543, perplexity: 366.057\n",
      "  epoch time: 37.33s\n",
      "Epoch 15/20\n",
      "  train loss: 4.92851, perplexity: 229.098\n",
      "  val   loss: 5.21631, perplexity: 363.337\n",
      "  epoch time: 38.98s\n",
      "Epoch 16/20\n",
      "  train loss: 4.90463, perplexity: 222.947\n",
      "  val   loss: 5.20724, perplexity: 360.679\n",
      "  epoch time: 39.12s\n",
      "Epoch 17/20\n",
      "  train loss: 4.88166, perplexity: 218.881\n",
      "  val   loss: 5.19779, perplexity: 359.410\n",
      "  epoch time: 41.11s\n",
      "Epoch 18/20\n",
      "  train loss: 4.86206, perplexity: 213.461\n",
      "  val   loss: 5.18814, perplexity: 356.871\n",
      "  epoch time: 37.86s\n",
      "Epoch 19/20\n",
      "  train loss: 4.84469, perplexity: 209.743\n",
      "  val   loss: 5.18582, perplexity: 359.557\n",
      "  epoch time: 40.45s\n",
      "Epoch 20/20\n",
      "  train loss: 4.82826, perplexity: 207.311\n",
      "  val   loss: 5.17626, perplexity: 358.165\n",
      "  epoch time: 38.27s\n",
      "Best val_loss: 5.176257381439209 (epoch: 20)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model_transformer.parameters(), lr=3e-4) #LEARNING_RATE)\n",
    "optimizer = torch.optim.Adam(model_transformer.parameters(), lr=8e-5) #LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "exp_data_transf = trainer_transformer.train_model(\n",
    "    model_transformer, dataloaders, optimizer, \n",
    "    criterion, scheduler=scheduler, \n",
    "    n_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3052b7d-1ce6-4440-b4fc-2a505c252d1f",
   "metadata": {},
   "source": [
    "# CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f00dcb77-9357-41dd-9e8a-ee54375fefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOKENS = len(vocab_chars)\n",
    "# LEARNING_RATE = 5e-4\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9320f8a6-a7f1-4ef2-97b2-52b30c1ae62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 5000\n",
      "val 500\n",
      "test 500\n",
      "torch.Size([128, 7]) torch.Size([128, 94]) torch.Size([128, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# NUM_CLASSES = 200\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "datasets = {\n",
    "    # WORD-LEVEL\n",
    "    #split: MemeDataset(DATA_DIR, vocab, tokenizer, image_transform=image_transform,\n",
    "    #                   num_classes=NUM_CLASSES, split=split, preload_images=True)\n",
    "    \n",
    "    # CHAR-LEVEL\n",
    "    split: MemeDataset(DATA_DIR, vocab_chars, tokenizer_chars, image_transform=image_transform,\n",
    "                        num_classes=NUM_CLASSES, split=split, preload_images=True)\n",
    "    for split in splits\n",
    "}\n",
    "\n",
    "for split in splits:\n",
    "    print(split, len(datasets[split]))\n",
    "\n",
    "dataloaders = {\n",
    "    split:  DataLoader(dataset=datasets[split], batch_size=BATCH_SIZE, \n",
    "                       shuffle=split == 'train', collate_fn=pad_collate)\n",
    "    for split in splits\n",
    "}\n",
    "\n",
    "for (labels, captions, images) in dataloaders['val']:\n",
    "    lengths = captions.size(1) - (captions == PAD_IDX).sum(dim=1)\n",
    "    print(labels.size(), captions.size(), images.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3affd939-0f9c-453d-9dee-e8b35043049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# parameters: 10645575\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model_transformer = CaptioningTransformerBase(\n",
    "    num_tokens=len(vocab_chars),\n",
    "    hid_dim=512, \n",
    "    n_layers=3, \n",
    "    n_heads=8, \n",
    "    pf_dim=2048,\n",
    "    enc_dropout=0.3, \n",
    "    dec_dropout=0.1, \n",
    "    pad_index=0, \n",
    "    max_len=128\n",
    ").to(device)\n",
    "\n",
    "# out = transformer_model(images.to(device), captions.to(device))\n",
    "# out.size()\n",
    "\n",
    "print('# parameters:', count_parameters(model_transformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c09eac38-5929-4b7b-ae53-201f719db2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_transformer = Trainer('TransformerDecoderBaseChars', log_dir='./logs', text_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e54dfe90-83fb-421d-989c-d37c7ac0605e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 23484), started 4:46:36 ago. (Use '!kill 23484' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2de1e15f92f24d6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2de1e15f92f24d6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !rm -rf ./logs\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fa1aec8-e4f2-4f09-a281-b446216ccdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50\n",
      "  train loss: 3.17052, perplexity: 25.408\n",
      "  val   loss: 2.99453, perplexity: 20.176\n",
      "  epoch time: 51.05s\n",
      "Epoch 02/50\n",
      "  train loss: 2.98905, perplexity: 20.067\n",
      "  val   loss: 2.96844, perplexity: 19.675\n",
      "  epoch time: 50.00s\n",
      "Epoch 03/50\n",
      "  train loss: 2.96921, perplexity: 19.693\n",
      "  val   loss: 2.95120, perplexity: 19.350\n",
      "  epoch time: 50.89s\n",
      "Epoch 04/50\n",
      "  train loss: 2.93631, perplexity: 19.029\n",
      "  val   loss: 2.84124, perplexity: 17.263\n",
      "  epoch time: 52.03s\n",
      "Epoch 05/50\n",
      "  train loss: 2.87794, perplexity: 17.912\n",
      "  val   loss: 2.80054, perplexity: 16.532\n",
      "  epoch time: 51.88s\n",
      "Epoch 06/50\n",
      "  train loss: 2.85185, perplexity: 17.429\n",
      "  val   loss: 2.78078, perplexity: 16.313\n",
      "  epoch time: 50.70s\n",
      "Epoch 07/50\n",
      "  train loss: 2.83282, perplexity: 17.125\n",
      "  val   loss: 2.75142, perplexity: 15.774\n",
      "  epoch time: 51.18s\n",
      "Epoch 08/50\n",
      "  train loss: 2.81266, perplexity: 16.770\n",
      "  val   loss: 2.73329, perplexity: 15.474\n",
      "  epoch time: 53.12s\n",
      "Epoch 09/50\n",
      "  train loss: 2.79755, perplexity: 16.526\n",
      "  val   loss: 2.71434, perplexity: 15.224\n",
      "  epoch time: 54.11s\n",
      "Epoch 10/50\n",
      "  train loss: 2.78022, perplexity: 16.226\n",
      "  val   loss: 2.67641, perplexity: 14.625\n",
      "  epoch time: 51.53s\n",
      "Epoch 11/50\n",
      "  train loss: 2.75836, perplexity: 15.867\n",
      "  val   loss: 2.64468, perplexity: 14.181\n",
      "  epoch time: 50.52s\n",
      "Epoch 12/50\n",
      "  train loss: 2.74057, perplexity: 15.592\n",
      "  val   loss: 2.61328, perplexity: 13.741\n",
      "  epoch time: 51.96s\n",
      "Epoch 13/50\n",
      "  train loss: 2.72400, perplexity: 15.334\n",
      "  val   loss: 2.59312, perplexity: 13.497\n",
      "  epoch time: 51.82s\n",
      "Epoch 14/50\n",
      "  train loss: 2.70647, perplexity: 15.064\n",
      "  val   loss: 2.57068, perplexity: 13.254\n",
      "  epoch time: 52.48s\n",
      "Epoch 15/50\n",
      "  train loss: 2.69219, perplexity: 14.874\n",
      "  val   loss: 2.55716, perplexity: 13.047\n",
      "  epoch time: 52.58s\n",
      "Epoch 16/50\n",
      "  train loss: 2.67866, perplexity: 14.668\n",
      "  val   loss: 2.53356, perplexity: 12.777\n",
      "  epoch time: 52.07s\n",
      "Epoch 17/50\n",
      "  train loss: 2.66547, perplexity: 14.472\n",
      "  val   loss: 2.51671, perplexity: 12.580\n",
      "  epoch time: 50.80s\n",
      "Epoch 18/50\n",
      "  train loss: 2.65540, perplexity: 14.331\n",
      "  val   loss: 2.50039, perplexity: 12.396\n",
      "  epoch time: 50.75s\n",
      "Epoch 19/50\n",
      "  train loss: 2.64230, perplexity: 14.137\n",
      "  val   loss: 2.48575, perplexity: 12.245\n",
      "  epoch time: 51.60s\n",
      "Epoch 20/50\n",
      "  train loss: 2.62999, perplexity: 13.964\n",
      "  val   loss: 2.47295, perplexity: 12.063\n",
      "  epoch time: 51.84s\n",
      "Epoch 21/50\n",
      "  train loss: 2.61997, perplexity: 13.835\n",
      "  val   loss: 2.46763, perplexity: 12.020\n",
      "  epoch time: 52.57s\n",
      "Epoch 22/50\n",
      "  train loss: 2.61265, perplexity: 13.741\n",
      "  val   loss: 2.44691, perplexity: 11.805\n",
      "  epoch time: 52.31s\n",
      "Epoch 23/50\n",
      "  train loss: 2.60419, perplexity: 13.630\n",
      "  val   loss: 2.44054, perplexity: 11.739\n",
      "  epoch time: 51.04s\n",
      "Epoch 24/50\n",
      "  train loss: 2.59679, perplexity: 13.527\n",
      "  val   loss: 2.43024, perplexity: 11.606\n",
      "  epoch time: 50.59s\n",
      "Epoch 25/50\n",
      "  train loss: 2.58897, perplexity: 13.429\n",
      "  val   loss: 2.42259, perplexity: 11.507\n",
      "  epoch time: 52.06s\n",
      "Epoch 26/50\n",
      "  train loss: 2.58467, perplexity: 13.368\n",
      "  val   loss: 2.41412, perplexity: 11.428\n",
      "  epoch time: 51.61s\n",
      "Epoch 27/50\n",
      "  train loss: 2.57401, perplexity: 13.227\n",
      "  val   loss: 2.40963, perplexity: 11.376\n",
      "  epoch time: 51.97s\n",
      "Epoch 28/50\n",
      "  train loss: 2.56823, perplexity: 13.145\n",
      "  val   loss: 2.40106, perplexity: 11.286\n",
      "  epoch time: 53.15s\n",
      "Epoch 29/50\n",
      "  train loss: 2.56561, perplexity: 13.118\n",
      "  val   loss: 2.39938, perplexity: 11.288\n",
      "  epoch time: 53.54s\n",
      "Epoch 30/50\n",
      "  train loss: 2.55786, perplexity: 13.039\n",
      "  val   loss: 2.38897, perplexity: 11.174\n",
      "  epoch time: 53.71s\n",
      "Epoch 31/50\n",
      "  train loss: 2.55557, perplexity: 13.009\n",
      "  val   loss: 2.38496, perplexity: 11.129\n",
      "  epoch time: 51.52s\n",
      "Epoch 32/50\n",
      "  train loss: 2.54759, perplexity: 12.889\n",
      "  val   loss: 2.38411, perplexity: 11.112\n",
      "  epoch time: 52.38s\n",
      "Epoch 33/50\n",
      "  train loss: 2.54607, perplexity: 12.873\n",
      "  val   loss: 2.37725, perplexity: 11.050\n",
      "  epoch time: 51.29s\n",
      "Epoch 34/50\n",
      "  train loss: 2.53987, perplexity: 12.796\n",
      "  val   loss: 2.37302, perplexity: 10.975\n",
      "  epoch time: 50.44s\n",
      "Epoch 35/50\n",
      "  train loss: 2.54671, perplexity: 12.887\n",
      "  val   loss: 2.36982, perplexity: 10.967\n",
      "  epoch time: 50.68s\n",
      "Epoch 36/50\n",
      "  train loss: 2.53281, perplexity: 12.705\n",
      "  val   loss: 2.36519, perplexity: 10.932\n",
      "  epoch time: 50.45s\n",
      "Epoch 37/50\n",
      "  train loss: 2.52896, perplexity: 12.666\n",
      "  val   loss: 2.36156, perplexity: 10.892\n",
      "  epoch time: 50.85s\n",
      "Epoch 38/50\n",
      "  train loss: 2.52940, perplexity: 12.673\n",
      "  val   loss: 2.35869, perplexity: 10.861\n",
      "  epoch time: 50.10s\n",
      "Epoch 39/50\n",
      "  train loss: 2.52407, perplexity: 12.606\n",
      "  val   loss: 2.35565, perplexity: 10.817\n",
      "  epoch time: 50.41s\n",
      "Epoch 40/50\n",
      "  train loss: 2.52176, perplexity: 12.586\n",
      "  val   loss: 2.35389, perplexity: 10.814\n",
      "  epoch time: 50.40s\n",
      "Epoch 41/50\n",
      "  train loss: 2.51816, perplexity: 12.524\n",
      "  val   loss: 2.35044, perplexity: 10.770\n",
      "  epoch time: 50.78s\n",
      "Epoch 42/50\n",
      "  train loss: 2.51675, perplexity: 12.518\n",
      "  val   loss: 2.34929, perplexity: 10.755\n",
      "  epoch time: 52.30s\n",
      "Epoch 43/50\n",
      "  train loss: 2.51740, perplexity: 12.529\n",
      "  val   loss: 2.34531, perplexity: 10.733\n",
      "  epoch time: 51.47s\n",
      "Epoch 44/50\n",
      "  train loss: 2.51266, perplexity: 12.466\n",
      "  val   loss: 2.34430, perplexity: 10.709\n",
      "  epoch time: 50.79s\n",
      "Epoch 45/50\n",
      "  train loss: 2.50906, perplexity: 12.412\n",
      "  val   loss: 2.34026, perplexity: 10.664\n",
      "  epoch time: 50.50s\n",
      "Epoch 46/50\n",
      "  train loss: 2.50993, perplexity: 12.433\n",
      "  val   loss: 2.34056, perplexity: 10.671\n",
      "  epoch time: 50.39s\n",
      "Epoch 47/50\n",
      "  train loss: 2.50407, perplexity: 12.354\n",
      "  val   loss: 2.33987, perplexity: 10.657\n",
      "  epoch time: 50.61s\n",
      "Epoch 48/50\n",
      "  train loss: 2.50330, perplexity: 12.345\n",
      "  val   loss: 2.33785, perplexity: 10.633\n",
      "  epoch time: 50.15s\n",
      "Epoch 49/50\n",
      "  train loss: 2.50225, perplexity: 12.332\n",
      "  val   loss: 2.33524, perplexity: 10.618\n",
      "  epoch time: 50.69s\n",
      "Epoch 50/50\n",
      "  train loss: 2.50080, perplexity: 12.320\n",
      "  val   loss: 2.33285, perplexity: 10.596\n",
      "  epoch time: 50.59s\n",
      "Best val_loss: 2.3328507862091064 (epoch: 50)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_transformer.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "exp_data_transf = trainer_transformer.train_model(\n",
    "    model_transformer, dataloaders, optimizer, \n",
    "    criterion, scheduler=scheduler, \n",
    "    n_epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f5b8c-14a7-446e-9f64-5f8c8c6b2fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1543a6-6a96-48df-bda4-2ef522452382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
