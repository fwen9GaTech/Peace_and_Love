{"cells":[{"cell_type":"markdown","metadata":{"id":"xR7g92NcMOUK"},"source":["# **Preprocessing**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":977,"status":"ok","timestamp":1651566163553,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"mhxPauUIJPvw","outputId":"047e3fd9-1ee6-437a-f85f-ec6e0781ac10"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount into drive\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":264,"status":"ok","timestamp":1651566163815,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"zVZFqqo8JVA7","outputId":"4e09a217-f1d2-4713-d37d-8a9834232079"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/DL7643_Group_project/vilbert\n","annotations.zip  Ensemble.ipynb\t\t   hateful_memes.zip  setup.sh\n","datasets.zip\t hateful-memes\t\t   log_plot.ipynb\n","Detectron.ipynb  hateful_memes_demo.ipynb  __MACOSX\n"]}],"source":["%cd 'drive/MyDrive/DL7643_Group_project/vilbert/'\n","# Verify the contents of the current folder\n","!ls"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36822,"status":"ok","timestamp":1651566200635,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"Wxxqk6uGWgUG","outputId":"228e125f-ee31-437d-8dfc-817843a91468"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.5.0+cu101\n","  Using cached https://download.pytorch.org/whl/cu101/torch-1.5.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (703.8 MB)\n","Collecting torchvision==0.6.0+cu101\n","  Using cached https://download.pytorch.org/whl/cu101/torchvision-0.6.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.6 MB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0+cu101) (1.21.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0+cu101) (0.16.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.0+cu101) (9.0.1)\n","Installing collected packages: torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0\n","    Uninstalling torch-1.9.0:\n","      Successfully uninstalled torch-1.9.0\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.10.0\n","    Uninstalling torchvision-0.10.0:\n","      Successfully uninstalled torchvision-0.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 0.9.0 requires torch==1.9.0, but you have torch 1.5.0+cu101 which is incompatible.\n","pytorch-lightning 1.6.0.dev0 requires torch>=1.7.*, but you have torch 1.5.0+cu101 which is incompatible.\n","mmf 1.0.0rc12 requires torch<=1.9.0,>=1.6.0, but you have torch 1.5.0+cu101 which is incompatible.\n","mmf 1.0.0rc12 requires torchvision<=0.10.0,>=0.7.0, but you have torchvision 0.6.0+cu101 which is incompatible.\u001b[0m\n","Successfully installed torch-1.5.0+cu101 torchvision-0.6.0+cu101\n","Requirement already satisfied: urllib3==1.25.10 in /usr/local/lib/python3.7/dist-packages (1.25.10)\n"]}],"source":["# %%writefile setup.sh\n","\n","!pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","# urllib3 is only required in colab\n","!pip install urllib3==1.25.10\n","\n","# # import hateful-memes\n","# !git clone https://github.com/czh4/hateful-memes.git\n","# %cd hateful-memes/\n","\n","# !git clone https://github.com/facebookresearch/mmf.git\n","# install mmf\n","# !git clone https://github.com/czh4/mmf-hateful-memes.git\n","# %cd mmf-hateful-memes/\n","# !pip install --editable .\n","# %cd ..\n","\n","# unzip dataset and move to hateful-memes\n","# !unzip datasets.zip\n","# !mv datasets hateful-memes/datasets"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1651566200636,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"rp6Vu7bnXJng"},"outputs":[],"source":["# !sh setup.sh"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1651566200637,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"KIXZE5hDNCxb"},"outputs":[],"source":["# # !pwd\n","# # !unzip datasets.zip\n","# !mv datasets hateful-memes/datasets"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67363,"status":"ok","timestamp":1651566267994,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"REVflQ2Y2IK8","outputId":"f37b4791-4052-4085-c8e9-3c49d237428e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mmf@ https://github.com/facebookresearch/mmf/tarball/master\n","  Using cached https://github.com/facebookresearch/mmf/tarball/master\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f\n","  Cloning https://github.com/PyTorchLightning/pytorch-lightning (to revision 9b011606f) to /tmp/pip-install-beogxrln/pytorch-lightning_737a1beabf684f65ad03c173966454d7\n","  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-install-beogxrln/pytorch-lightning_737a1beabf684f65ad03c173966454d7\n","\u001b[33m  WARNING: Did not find branch or tag '9b011606f', assuming revision or ref.\u001b[0m\n","  Running command git checkout -q 9b011606f\n","  Running command git submodule update --init --recursive -q\n","  From https://github.com/PyTorchLightning/lightning-tutorials\n","   * branch            290fb466de1fcc2ac6025f74b56906592911e856 -> FETCH_HEAD\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torchaudio<=0.9.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.9.0)\n","Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.4.5)\n","Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.5.3)\n","Requirement already satisfied: iopath==0.1.8 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.1.8)\n","Requirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.0.2)\n","Requirement already satisfied: omegaconf<=2.1,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.1.0)\n","Requirement already satisfied: numpy<=1.21.4,>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.21.4)\n","Requirement already satisfied: transformers<=4.10.1,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.10.1)\n","Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (5.4.8)\n","Requirement already satisfied: GitPython==3.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.1.0)\n","Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.1.0)\n","Requirement already satisfied: pillow==9.0.1 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (9.0.1)\n","Collecting torch<=1.9.0,>=1.6.0\n","  Using cached torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","Requirement already satisfied: tqdm<4.50.0,>=4.43.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.49.0)\n","Requirement already satisfied: matplotlib==3.3.4 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.3.4)\n","Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.23.0)\n","Requirement already satisfied: fasttext==0.9.1 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.9.1)\n","Requirement already satisfied: ftfy==5.8 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (5.8)\n","Requirement already satisfied: lmdb==0.98 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.98)\n","Collecting torchvision<=0.10.0,>=0.7.0\n","  Using cached torchvision-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n","Requirement already satisfied: torchtext==0.5.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.5.0)\n","Requirement already satisfied: datasets==1.2.1 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.2.1)\n","Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.8.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.2.0)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2022.3.0)\n","Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.3.2)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (21.3)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (6.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.8.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.70.12.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.11.3)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.3.4)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (6.0.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.3.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.0.0)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (57.4.0)\n","Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.9.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==5.8->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.2.5)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython==3.1.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.0.9)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath==0.1.8->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.4.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.0.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.15.0)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.29.28)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.25.10)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.0.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.1.96)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.8.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython==3.1.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (5.0.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from omegaconf<=2.1,>=2.0.6->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.8)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.35.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.8.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.44.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.4.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.37.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.3.6)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.0.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.0.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.17.3)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.3.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.0.53)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.10.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2019.12.20)\n","Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.5.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.0.12)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.13.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (6.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.3.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (21.4.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.7.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2022.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.4.1)\n","Installing collected packages: torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.5.0+cu101\n","    Uninstalling torch-1.5.0+cu101:\n","      Successfully uninstalled torch-1.5.0+cu101\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.6.0+cu101\n","    Uninstalling torchvision-0.6.0+cu101:\n","      Successfully uninstalled torchvision-0.6.0+cu101\n","Successfully installed torch-1.9.0 torchvision-0.10.0\n"]}],"source":["# !pip install git+https://github.com/facebookresearch/mmf.git\n","# !pip install git+https://github.com/rizavelioglu/mmf.git\n","# !pip uninstall mmf\n","!pip install mmf@https://github.com/facebookresearch/mmf/tarball/master"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-gcD_FxyZkk"},"outputs":[],"source":["# %cd /usr/local/lib/python3.7/dist-packages/mmf_cli/hm_convert.py"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73900,"status":"ok","timestamp":1651566614215,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"6WD-iMI8NP8q","outputId":"c9f87c2e-448c-40c1-d4a2-81f372c2be4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","Data folder is /root/.cache/torch/mmf/data\n","Zip path is hateful_memes.zip\n","Copying hateful_memes.zip\n","Unzipping hateful_memes.zip\n","Extracting the zip can take time. Sit back and relax.\n","Moving train.jsonl\n","Moving dev_seen.jsonl\n","Moving test_seen.jsonl\n","Moving dev_unseen.jsonl\n","Moving test_unseen.jsonl\n","Moving img\n"]}],"source":["!mmf_convert_hm --zip_file=\"hateful_memes.zip\" --password=\"1\" --bypass_checksum 1\n","# !mmf_convert_hm --zip_file=\"../no_text_50.zip\" --password=\"1\" --bypass_checksum 1"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1651566614217,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"AuTpgT4ItFz8","outputId":"ae9ee41e-5f3a-425f-ebd5-0651316aff73"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/DL7643_Group_project/vilbert'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["pwd"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1651566614217,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"B2UQtrgEN_Ni","outputId":"f6aa3006-f9fc-4d98-d009-120aecf175a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes\n"]}],"source":["%cd hateful-memes"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1651566614218,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"7hjiR1Ot6RGW","outputId":"f7f76fbf-a3a1-475d-a507-08245f638229"},"outputs":[{"output_type":"stream","name":"stdout","text":["configs\t\t\t\t  save_visual_bert_coco_bs32_50_da\n","datasets\t\t\t  save_visual_bert_coco_bs32_da\n","ensemble\t\t\t  save_visual_bert_coco_bs32_da_softlabelce\n","mmf\t\t\t\t  save_visual_bert_coco_bs32_lr1_50_da\n","mmf-hateful-memes\t\t  save_visual_bert_coco_bs32_lr1_da\n","README.md\t\t\t  save_visual_bert_coco_bs32_softlabel\n","save_vilbert_cc\t\t\t  save_visual_bert_coco_da_label_smoothing\n","save_vilbert_default\t\t  save_visual_bert_coco_da_resize\n","save_vilbert_default_no_text_100  save_visual_bert_default_bs32\n","save_visual_bert_coco_bs2_lr1\t  save_visual_bert_softloss\n","save_visual_bert_coco_bs32\t  tools\n"]}],"source":["!ls"]},{"cell_type":"markdown","metadata":{"id":"zc0-uckEmSiT"},"source":["# **Vilbert with roberta**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6267,"status":"ok","timestamp":1650957876084,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"y098B017mbN_","outputId":"f4c6b8da-912d-4754-e400-5d07273f1e83"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","Traceback (most recent call last):\n","  File \"tools/run.py\", line 126, in <module>\n","    run()\n","  File \"tools/run.py\", line 91, in run\n","    configuration = Configuration(args)\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/utils/configuration.py\", line 339, in __init__\n","    other_configs = self._build_other_configs()\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/utils/configuration.py\", line 362, in _build_other_configs\n","    user_config = self._build_user_config(opts_config)\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/utils/configuration.py\", line 388, in _build_user_config\n","    user_config = load_yaml(self.config_path)\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/utils/configuration.py\", line 26, in load_yaml\n","    mapping = OmegaConf.load(PathManager.get_local_path(abs_f))\n","  File \"/usr/local/lib/python3.7/dist-packages/omegaconf/omegaconf.py\", line 184, in load\n","    obj = yaml.load(f, Loader=get_yaml_loader())\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/__init__.py\", line 81, in load\n","    return loader.get_single_data()\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/constructor.py\", line 49, in get_single_data\n","    node = self.get_single_node()\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/composer.py\", line 36, in get_single_node\n","    document = self.compose_document()\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/composer.py\", line 55, in compose_document\n","    node = self.compose_node(None, None)\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/composer.py\", line 84, in compose_node\n","    node = self.compose_mapping_node(anchor)\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/composer.py\", line 133, in compose_mapping_node\n","    item_value = self.compose_node(node, item_key)\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/composer.py\", line 84, in compose_node\n","    node = self.compose_mapping_node(anchor)\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/composer.py\", line 133, in compose_mapping_node\n","    item_value = self.compose_node(node, item_key)\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/composer.py\", line 84, in compose_node\n","    node = self.compose_mapping_node(anchor)\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/composer.py\", line 127, in compose_mapping_node\n","    while not self.check_event(MappingEndEvent):\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/parser.py\", line 98, in check_event\n","    self.current_event = self.state()\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/parser.py\", line 428, in parse_block_mapping_key\n","    if self.check_token(KeyToken):\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/scanner.py\", line 116, in check_token\n","    self.fetch_more_tokens()\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/scanner.py\", line 223, in fetch_more_tokens\n","    return self.fetch_value()\n","  File \"/usr/local/lib/python3.7/dist-packages/yaml/scanner.py\", line 579, in fetch_value\n","    self.get_mark())\n","yaml.scanner.ScannerError: mapping values are not allowed here\n","  in \"/content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes/mmf/projects/hateful_memes/configs/vilbert/defaults.yaml\", line 14, column 22\n"]}],"source":["!python tools/run.py config=mmf/projects/hateful_memes/configs/vilbert/defaults.yaml model=vilbert dataset=hateful_memes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rltd8HKCmb5E"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"9RiCxE1zdnV9"},"source":["# **Vilbert with no_text_100**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2838072,"status":"ok","timestamp":1650942115793,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"0qxAXLRrduwZ","outputId":"b632af6f-c6fa-4b2f-d78b-1cec1967c7f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-25T20:43:37 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\n","\u001b[32m2022-04-25T20:43:37 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2022-04-25T20:43:37 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-25T20:43:37 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-25T20:43:37 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes'])\n","\u001b[32m2022-04-25T20:43:37 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-25T20:43:37 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-04-25T20:43:37 | mmf_cli.run: \u001b[0mUsing seed 37951550\n","\u001b[32m2022-04-25T20:43:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [07:17<00:00, 23.5MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 143kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3riotwrh\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 23.8kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0l9zqqae\n","Downloading: 100% 570/570 [00:00<00:00, 461kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_6olsj_v\n","Downloading: 100% 232k/232k [00:00<00:00, 694kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpng4vd5e_\n","Downloading: 100% 466k/466k [00:00<00:00, 1.11MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-25T20:53:50 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T20:53:50 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T20:53:50 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T20:53:50 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bi_attention_type\": 1,\n","  \"bi_hidden_size\": 1024,\n","  \"bi_intermediate_size\": 1024,\n","  \"bi_num_attention_heads\": 8,\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"cut_first\": \"text\",\n","  \"dynamic_attention\": false,\n","  \"embedding_strategy\": \"plain\",\n","  \"fast_mode\": false,\n","  \"finetune_lr_multiplier\": 1,\n","  \"fixed_t_layer\": 0,\n","  \"fixed_v_layer\": 0,\n","  \"freeze_base\": false,\n","  \"fusion_method\": \"mul\",\n","  \"gradient_checkpointing\": false,\n","  \"hard_cap_seq_len\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"in_batch_pairs\": false,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"vilbert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_negative\": 128,\n","  \"objective\": 0,\n","  \"pad_token_id\": 0,\n","  \"pooling_method\": \"mul\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"t_biattention_id\": [\n","    6,\n","    7,\n","    8,\n","    9,\n","    10,\n","    11\n","  ],\n","  \"task_specific_tokens\": false,\n","  \"text_only\": false,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"v_attention_probs_dropout_prob\": 0.1,\n","  \"v_biattention_id\": [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","  ],\n","  \"v_feature_size\": 2048,\n","  \"v_hidden_act\": \"gelu\",\n","  \"v_hidden_dropout_prob\": 0.1,\n","  \"v_hidden_size\": 1024,\n","  \"v_initializer_range\": 0.02,\n","  \"v_intermediate_size\": 1024,\n","  \"v_num_attention_heads\": 8,\n","  \"v_num_hidden_layers\": 6,\n","  \"v_target_size\": 1601,\n","  \"visual_embedding_dim\": 2048,\n","  \"visual_target\": 0,\n","  \"visualization\": false,\n","  \"vocab_size\": 30522,\n","  \"with_coattention\": true\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmpd2u927bg\n","Downloading: 100% 440M/440M [00:05<00:00, 73.8MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.t_pooler.dense.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.v_pooler.dense.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.v_pooler.dense.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value2.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-25T20:54:06 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-25T20:54:06 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-25T20:54:06 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-25T20:54:06 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n","  (model): ViLBERTForClassification(\n","    (bert): ViLBERTBase(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (v_embeddings): BertImageFeatureEmbeddings(\n","        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n","        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (v_layer): ModuleList(\n","          (0): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (c_layer): ModuleList(\n","          (0): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (t_pooler): BertTextPooler(\n","        (dense): Linear(in_features=768, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","      (v_pooler): BertImagePooler(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=1024, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-25T20:54:06 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n","\u001b[32m2022-04-25T20:54:06 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-04-25T20:55:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.7771, train/hateful_memes/cross_entropy/avg: 0.7771, train/total_loss: 0.7771, train/total_loss/avg: 0.7771, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 552ms, time_since_start: 01m 36s 576ms, eta: 05h 58m 24s 491ms\n","\u001b[32m2022-04-25T20:57:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6601, train/hateful_memes/cross_entropy/avg: 0.7186, train/total_loss: 0.6601, train/total_loss/avg: 0.7186, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 527ms, time_since_start: 03m 12s 104ms, eta: 05h 52m 59s 036ms\n","\u001b[32m2022-04-25T20:58:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.7007, train/hateful_memes/cross_entropy/avg: 0.7126, train/total_loss: 0.7007, train/total_loss/avg: 0.7126, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 344ms, time_since_start: 04m 47s 448ms, eta: 05h 50m 41s 495ms\n","\u001b[32m2022-04-25T21:00:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6899, train/hateful_memes/cross_entropy/avg: 0.7069, train/total_loss: 0.6899, train/total_loss/avg: 0.7069, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 429ms, time_since_start: 06m 22s 878ms, eta: 05h 49m 23s 173ms\n","\u001b[32m2022-04-25T21:02:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.7007, train/hateful_memes/cross_entropy/avg: 0.7167, train/total_loss: 0.7007, train/total_loss/avg: 0.7167, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 424ms, time_since_start: 07m 58s 302ms, eta: 05h 47m 45s 023ms\n","\u001b[32m2022-04-25T21:03:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.6899, train/hateful_memes/cross_entropy/avg: 0.6950, train/total_loss: 0.6899, train/total_loss/avg: 0.6950, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 943ms, time_since_start: 09m 33s 246ms, eta: 05h 44m 23s 340ms\n","\u001b[32m2022-04-25T21:05:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6899, train/hateful_memes/cross_entropy/avg: 0.6865, train/total_loss: 0.6899, train/total_loss/avg: 0.6865, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 625ms, time_since_start: 11m 08s 872ms, eta: 05h 45m 14s 580ms\n","\u001b[32m2022-04-25T21:06:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.6601, train/hateful_memes/cross_entropy/avg: 0.6742, train/total_loss: 0.6601, train/total_loss/avg: 0.6742, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 334ms, time_since_start: 12m 44s 206ms, eta: 05h 42m 34s 413ms\n","\u001b[32m2022-04-25T21:08:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.6601, train/hateful_memes/cross_entropy/avg: 0.6497, train/total_loss: 0.6601, train/total_loss/avg: 0.6497, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 593ms, time_since_start: 14m 19s 799ms, eta: 05h 41m 53s 124ms\n","\u001b[32m2022-04-25T21:10:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-25T21:10:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T21:10:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T21:10:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T21:10:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.6350, train/hateful_memes/cross_entropy/avg: 0.6474, train/total_loss: 0.6350, train/total_loss/avg: 0.6474, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00001, ups: 0.84, time: 01m 59s 568ms, time_since_start: 16m 19s 368ms, eta: 07h 05m 36s 287ms\n","\u001b[32m2022-04-25T21:10:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-25T21:10:25 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-25T21:10:32 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-25T21:10:32 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T21:10:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T21:10:43 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-25T21:10:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T21:11:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T21:11:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.7401, val/total_loss: 0.7401, val/hateful_memes/accuracy: 0.6130, val/hateful_memes/binary_f1: 0.4274, val/hateful_memes/roc_auc: 0.6139, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 44s 997ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.613912\n","\u001b[32m2022-04-25T21:12:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.6350, train/hateful_memes/cross_entropy/avg: 0.6247, train/total_loss: 0.6350, train/total_loss/avg: 0.6247, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 670ms, time_since_start: 18m 40s 037ms, eta: 05h 38m 54s 965ms\n","\u001b[32m2022-04-25T21:14:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.6272, train/hateful_memes/cross_entropy/avg: 0.6179, train/total_loss: 0.6272, train/total_loss/avg: 0.6179, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 526ms, time_since_start: 20m 15s 563ms, eta: 05h 36m 47s 316ms\n","\u001b[32m2022-04-25T21:15:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.6272, train/hateful_memes/cross_entropy/avg: 0.5989, train/total_loss: 0.6272, train/total_loss/avg: 0.5989, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 362ms, time_since_start: 21m 50s 926ms, eta: 05h 34m 35s 709ms\n","\u001b[32m2022-04-25T21:17:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.5884, train/hateful_memes/cross_entropy/avg: 0.5815, train/total_loss: 0.5884, train/total_loss/avg: 0.5815, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 989ms, time_since_start: 23m 25s 915ms, eta: 05h 31m 40s 390ms\n","\u001b[32m2022-04-25T21:19:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.5884, train/hateful_memes/cross_entropy/avg: 0.5694, train/total_loss: 0.5884, train/total_loss/avg: 0.5694, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 501ms, time_since_start: 25m 01s 416ms, eta: 05h 31m 50s 585ms\n","\u001b[32m2022-04-25T21:20:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.5865, train/hateful_memes/cross_entropy/avg: 0.5484, train/total_loss: 0.5865, train/total_loss/avg: 0.5484, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 059ms, time_since_start: 26m 36s 476ms, eta: 05h 28m 41s 754ms\n","\u001b[32m2022-04-25T21:22:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.5865, train/hateful_memes/cross_entropy/avg: 0.5369, train/total_loss: 0.5865, train/total_loss/avg: 0.5369, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 431ms, time_since_start: 28m 11s 907ms, eta: 05h 28m 21s 935ms\n","\u001b[32m2022-04-25T21:23:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.5422, train/hateful_memes/cross_entropy/avg: 0.5204, train/total_loss: 0.5422, train/total_loss/avg: 0.5204, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 329ms, time_since_start: 29m 47s 237ms, eta: 05h 26m 23s 998ms\n","\u001b[32m2022-04-25T21:25:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.5422, train/hateful_memes/cross_entropy/avg: 0.5028, train/total_loss: 0.5422, train/total_loss/avg: 0.5028, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 238ms, time_since_start: 31m 22s 476ms, eta: 05h 24m 28s 353ms\n","\u001b[32m2022-04-25T21:27:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-25T21:27:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T21:27:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T21:27:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T21:27:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.4534, train/hateful_memes/cross_entropy/avg: 0.4864, train/total_loss: 0.4534, train/total_loss/avg: 0.4864, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 0.84, time: 01m 59s 416ms, time_since_start: 33m 21s 892ms, eta: 06h 44m 49s 322ms\n","\u001b[32m2022-04-25T21:27:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-25T21:27:28 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-25T21:27:33 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-25T21:27:33 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T21:27:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T21:27:44 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-25T21:27:59 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T21:28:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T21:28:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 0.8931, val/total_loss: 0.8931, val/hateful_memes/accuracy: 0.6667, val/hateful_memes/binary_f1: 0.4079, val/hateful_memes/roc_auc: 0.6737, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 45s 721ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.673662\n","\u001b[32m2022-04-25T21:29:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.4007, train/hateful_memes/cross_entropy/avg: 0.4822, train/total_loss: 0.4007, train/total_loss/avg: 0.4822, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 406ms, time_since_start: 35m 44s 024ms, eta: 05h 25m 11s 090ms\n","\u001b[32m2022-04-25T21:31:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.3980, train/hateful_memes/cross_entropy/avg: 0.4763, train/total_loss: 0.3980, train/total_loss/avg: 0.4763, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 273ms, time_since_start: 37m 19s 298ms, eta: 05h 19m 44s 921ms\n","\u001b[32m2022-04-25T21:33:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.3973, train/hateful_memes/cross_entropy/avg: 0.4682, train/total_loss: 0.3973, train/total_loss/avg: 0.4682, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 814ms, time_since_start: 38m 55s 113ms, eta: 05h 19m 56s 435ms\n","\u001b[32m2022-04-25T21:34:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.3718, train/hateful_memes/cross_entropy/avg: 0.4586, train/total_loss: 0.3718, train/total_loss/avg: 0.4586, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 796ms, time_since_start: 40m 30s 909ms, eta: 05h 18m 15s 375ms\n","\u001b[32m2022-04-25T21:36:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.3545, train/hateful_memes/cross_entropy/avg: 0.4481, train/total_loss: 0.3545, train/total_loss/avg: 0.4481, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 890ms, time_since_start: 42m 06s 800ms, eta: 05h 16m 56s 525ms\n","\u001b[32m2022-04-25T21:37:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.3534, train/hateful_memes/cross_entropy/avg: 0.4365, train/total_loss: 0.3534, train/total_loss/avg: 0.4365, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 016ms, time_since_start: 43m 42s 817ms, eta: 05h 15m 43s 910ms\n","\u001b[32m2022-04-25T21:39:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.3531, train/hateful_memes/cross_entropy/avg: 0.4216, train/total_loss: 0.3531, train/total_loss/avg: 0.4216, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 218ms, time_since_start: 45m 18s 035ms, eta: 05h 11m 29s 630ms\n","\u001b[32m2022-04-25T21:41:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.2887, train/hateful_memes/cross_entropy/avg: 0.4077, train/total_loss: 0.2887, train/total_loss/avg: 0.4077, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 802ms, time_since_start: 46m 53s 838ms, eta: 05h 11m 46s 774ms\n","\u001b[32m2022-04-25T21:42:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.2393, train/hateful_memes/cross_entropy/avg: 0.3946, train/total_loss: 0.2393, train/total_loss/avg: 0.3946, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 736ms, time_since_start: 48m 29s 575ms, eta: 05h 09m 56s 567ms\n","\u001b[32m2022-04-25T21:44:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-25T21:44:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T21:44:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T21:44:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T21:44:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.2390, train/hateful_memes/cross_entropy/avg: 0.3838, train/total_loss: 0.2390, train/total_loss/avg: 0.3838, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 0.85, time: 01m 58s 932ms, time_since_start: 50m 28s 507ms, eta: 06h 23m 01s 247ms\n","\u001b[32m2022-04-25T21:44:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-25T21:44:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-25T21:44:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-25T21:44:40 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T21:44:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T21:44:54 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-25T21:45:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T21:45:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T21:45:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.4827, val/total_loss: 1.4827, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.4852, val/hateful_memes/roc_auc: 0.6775, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 47s 631ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.677478\n","\u001b[32m2022-04-25T21:46:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.2329, train/hateful_memes/cross_entropy/avg: 0.3760, train/total_loss: 0.2329, train/total_loss/avg: 0.3760, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 629ms, time_since_start: 52m 52s 769ms, eta: 05h 09m 33s 461ms\n","\u001b[32m2022-04-25T21:48:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.1955, train/hateful_memes/cross_entropy/avg: 0.3647, train/total_loss: 0.1955, train/total_loss/avg: 0.3647, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 814ms, time_since_start: 54m 28s 584ms, eta: 05h 05m 19s 417ms\n","\u001b[32m2022-04-25T21:50:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.1865, train/hateful_memes/cross_entropy/avg: 0.3561, train/total_loss: 0.1865, train/total_loss/avg: 0.3561, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 786ms, time_since_start: 56m 04s 370ms, eta: 05h 03m 36s 545ms\n","\u001b[32m2022-04-25T21:51:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.1757, train/hateful_memes/cross_entropy/avg: 0.3470, train/total_loss: 0.1757, train/total_loss/avg: 0.3470, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 556ms, time_since_start: 57m 39s 926ms, eta: 05h 01m 15s 586ms\n","\u001b[32m2022-04-25T21:53:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.1467, train/hateful_memes/cross_entropy/avg: 0.3375, train/total_loss: 0.1467, train/total_loss/avg: 0.3375, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 723ms, time_since_start: 59m 15s 650ms, eta: 05h 09s 885ms\n","\u001b[32m2022-04-25T21:54:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.1438, train/hateful_memes/cross_entropy/avg: 0.3283, train/total_loss: 0.1438, train/total_loss/avg: 0.3283, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 891ms, time_since_start: 01h 51s 541ms, eta: 04h 59m 03s 983ms\n","\u001b[32m2022-04-25T21:56:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0822, train/hateful_memes/cross_entropy/avg: 0.3205, train/total_loss: 0.0822, train/total_loss/avg: 0.3205, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 879ms, time_since_start: 01h 02m 27s 421ms, eta: 04h 57m 24s 323ms\n","\u001b[32m2022-04-25T21:58:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0704, train/hateful_memes/cross_entropy/avg: 0.3123, train/total_loss: 0.0704, train/total_loss/avg: 0.3123, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 197ms, time_since_start: 01h 04m 02s 618ms, eta: 04h 53m 40s 404ms\n","\u001b[32m2022-04-25T21:59:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0454, train/hateful_memes/cross_entropy/avg: 0.3053, train/total_loss: 0.0454, train/total_loss/avg: 0.3053, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 716ms, time_since_start: 01h 05m 38s 335ms, eta: 04h 53m 39s 227ms\n","\u001b[32m2022-04-25T22:01:20 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-25T22:01:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T22:01:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T22:01:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T22:01:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0404, train/hateful_memes/cross_entropy/avg: 0.2983, train/total_loss: 0.0404, train/total_loss/avg: 0.2983, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 0.85, time: 01m 58s 554ms, time_since_start: 01h 07m 36s 889ms, eta: 06h 01m 42s 503ms\n","\u001b[32m2022-04-25T22:01:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-25T22:01:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-25T22:01:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-25T22:01:48 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T22:01:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T22:01:58 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-25T22:02:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T22:02:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T22:02:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.5352, val/total_loss: 1.5352, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.4908, val/hateful_memes/roc_auc: 0.6910, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 46s 497ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.691011\n","\u001b[32m2022-04-25T22:04:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0404, train/hateful_memes/cross_entropy/avg: 0.2922, train/total_loss: 0.0404, train/total_loss/avg: 0.2922, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 582ms, time_since_start: 01h 09m 59s 971ms, eta: 04h 53m 02s 240ms\n","\u001b[32m2022-04-25T22:05:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0404, train/hateful_memes/cross_entropy/avg: 0.2880, train/total_loss: 0.0404, train/total_loss/avg: 0.2880, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 824ms, time_since_start: 01h 11m 35s 795ms, eta: 04h 49m 06s 746ms\n","\u001b[32m2022-04-25T22:07:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0379, train/hateful_memes/cross_entropy/avg: 0.2815, train/total_loss: 0.0379, train/total_loss/avg: 0.2815, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 282ms, time_since_start: 01h 13m 11s 078ms, eta: 04h 45m 51s 680ms\n","\u001b[32m2022-04-25T22:08:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0346, train/hateful_memes/cross_entropy/avg: 0.2751, train/total_loss: 0.0346, train/total_loss/avg: 0.2751, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 770ms, time_since_start: 01h 14m 46s 849ms, eta: 04h 45m 42s 245ms\n","\u001b[32m2022-04-25T22:10:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0346, train/hateful_memes/cross_entropy/avg: 0.2710, train/total_loss: 0.0346, train/total_loss/avg: 0.2710, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 704ms, time_since_start: 01h 16m 22s 554ms, eta: 04h 43m 53s 070ms\n","\u001b[32m2022-04-25T22:12:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0303, train/hateful_memes/cross_entropy/avg: 0.2652, train/total_loss: 0.0303, train/total_loss/avg: 0.2652, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 419ms, time_since_start: 01h 17m 57s 974ms, eta: 04h 41m 25s 312ms\n","\u001b[32m2022-04-25T22:13:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0280, train/hateful_memes/cross_entropy/avg: 0.2597, train/total_loss: 0.0280, train/total_loss/avg: 0.2597, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 617ms, time_since_start: 01h 19m 33s 591ms, eta: 04h 40m 23s 034ms\n","\u001b[32m2022-04-25T22:15:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0280, train/hateful_memes/cross_entropy/avg: 0.2562, train/total_loss: 0.0280, train/total_loss/avg: 0.2562, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 094ms, time_since_start: 01h 21m 08s 686ms, eta: 04h 37m 14s 339ms\n","\u001b[32m2022-04-25T22:16:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0266, train/hateful_memes/cross_entropy/avg: 0.2511, train/total_loss: 0.0266, train/total_loss/avg: 0.2511, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 552ms, time_since_start: 01h 22m 44s 238ms, eta: 04h 36m 57s 207ms\n","\u001b[32m2022-04-25T22:18:26 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-25T22:18:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T22:18:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T22:18:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T22:18:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0266, train/hateful_memes/cross_entropy/avg: 0.2491, train/total_loss: 0.0266, train/total_loss/avg: 0.2491, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 535ms, time_since_start: 01h 24m 44s 774ms, eta: 05h 47m 19s 381ms\n","\u001b[32m2022-04-25T22:18:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-25T22:18:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-25T22:18:56 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-25T22:18:56 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T22:18:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T22:19:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T22:19:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T22:19:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 2.1619, val/total_loss: 2.1619, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4014, val/hateful_memes/roc_auc: 0.6640, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 29s 124ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.691011\n","\u001b[32m2022-04-25T22:20:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0141, train/hateful_memes/cross_entropy/avg: 0.2443, train/total_loss: 0.0141, train/total_loss/avg: 0.2443, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 1.02, time: 01m 38s 251ms, time_since_start: 01h 26m 52s 152ms, eta: 04h 41m 26s 863ms\n","\u001b[32m2022-04-25T22:22:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0266, train/hateful_memes/cross_entropy/avg: 0.2408, train/total_loss: 0.0266, train/total_loss/avg: 0.2408, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 823ms, time_since_start: 01h 28m 27s 975ms, eta: 04h 32m 52s 058ms\n","\u001b[32m2022-04-25T22:24:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0266, train/hateful_memes/cross_entropy/avg: 0.2368, train/total_loss: 0.0266, train/total_loss/avg: 0.2368, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 742ms, time_since_start: 01h 30m 03s 718ms, eta: 04h 31m 751ms\n","\u001b[32m2022-04-25T22:25:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0241, train/hateful_memes/cross_entropy/avg: 0.2328, train/total_loss: 0.0241, train/total_loss/avg: 0.2328, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 218ms, time_since_start: 01h 31m 38s 936ms, eta: 04h 27m 55s 011ms\n","\u001b[32m2022-04-25T22:27:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0266, train/hateful_memes/cross_entropy/avg: 0.2310, train/total_loss: 0.0266, train/total_loss/avg: 0.2310, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 827ms, time_since_start: 01h 33m 14s 764ms, eta: 04h 28m 418ms\n","\u001b[32m2022-04-25T22:28:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0266, train/hateful_memes/cross_entropy/avg: 0.2270, train/total_loss: 0.0266, train/total_loss/avg: 0.2270, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 646ms, time_since_start: 01h 34m 50s 411ms, eta: 04h 25m 52s 691ms\n","\u001b[32m2022-04-25T22:30:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0241, train/hateful_memes/cross_entropy/avg: 0.2231, train/total_loss: 0.0241, train/total_loss/avg: 0.2231, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 857ms, time_since_start: 01h 36m 26s 268ms, eta: 04h 24m 50s 352ms\n","\u001b[32m2022-04-25T22:32:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0266, train/hateful_memes/cross_entropy/avg: 0.2199, train/total_loss: 0.0266, train/total_loss/avg: 0.2199, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 802ms, time_since_start: 01h 38m 02s 071ms, eta: 04h 23m 03s 875ms\n","\u001b[32m2022-04-25T22:33:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0241, train/hateful_memes/cross_entropy/avg: 0.2162, train/total_loss: 0.0241, train/total_loss/avg: 0.2162, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 363ms, time_since_start: 01h 39m 37s 435ms, eta: 04h 20m 14s 582ms\n","\u001b[32m2022-04-25T22:35:19 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-25T22:35:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T22:35:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T22:35:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T22:35:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.2127, train/total_loss: 0.0058, train/total_loss/avg: 0.2127, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 0.85, time: 01m 58s 377ms, time_since_start: 01h 41m 35s 812ms, eta: 05h 21m 02s 373ms\n","\u001b[32m2022-04-25T22:35:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-25T22:35:42 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-25T22:35:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-25T22:35:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T22:35:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T22:35:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T22:36:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T22:36:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.1267, val/total_loss: 2.1267, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.4596, val/hateful_memes/roc_auc: 0.6758, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 29s 545ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.691011\n","\u001b[32m2022-04-25T22:37:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.2103, train/total_loss: 0.0058, train/total_loss/avg: 0.2103, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 912ms, time_since_start: 01h 43m 42s 272ms, eta: 04h 21m 11s 108ms\n","\u001b[32m2022-04-25T22:39:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.2075, train/total_loss: 0.0058, train/total_loss/avg: 0.2075, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 712ms, time_since_start: 01h 45m 17s 985ms, eta: 04h 16m 19s 672ms\n","\u001b[32m2022-04-25T22:41:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0241, train/hateful_memes/cross_entropy/avg: 0.2060, train/total_loss: 0.0241, train/total_loss/avg: 0.2060, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 824ms, time_since_start: 01h 46m 53s 809ms, eta: 04h 15m 159ms\n","\u001b[32m2022-04-25T22:42:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0241, train/hateful_memes/cross_entropy/avg: 0.2028, train/total_loss: 0.0241, train/total_loss/avg: 0.2028, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 282ms, time_since_start: 01h 48m 29s 091ms, eta: 04h 11m 56s 741ms\n","\u001b[32m2022-04-25T22:44:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.1998, train/total_loss: 0.0058, train/total_loss/avg: 0.1998, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 704ms, time_since_start: 01h 50m 04s 796ms, eta: 04h 11m 26s 425ms\n","\u001b[32m2022-04-25T22:45:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.1967, train/total_loss: 0.0058, train/total_loss/avg: 0.1967, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 559ms, time_since_start: 01h 51m 40s 356ms, eta: 04h 09m 26s 405ms\n","\u001b[32m2022-04-25T22:47:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.1938, train/total_loss: 0.0058, train/total_loss/avg: 0.1938, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 387ms, time_since_start: 01h 53m 15s 743ms, eta: 04h 07m 22s 353ms\n","\u001b[32m2022-04-25T22:48:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.1914, train/total_loss: 0.0058, train/total_loss/avg: 0.1914, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 669ms, time_since_start: 01h 54m 51s 413ms, eta: 04h 06m 29s 038ms\n","\u001b[32m2022-04-25T22:50:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.1887, train/total_loss: 0.0058, train/total_loss/avg: 0.1887, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 578ms, time_since_start: 01h 56m 26s 992ms, eta: 04h 04m 37s 740ms\n","\u001b[32m2022-04-25T22:52:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-25T22:52:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T22:52:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T22:52:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T22:52:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.1872, train/total_loss: 0.0058, train/total_loss/avg: 0.1872, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 01s 237ms, time_since_start: 01h 58m 28s 229ms, eta: 05h 08m 14s 750ms\n","\u001b[32m2022-04-25T22:52:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-25T22:52:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-25T22:52:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-25T22:52:39 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T22:52:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T22:52:50 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-25T22:53:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T22:53:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T22:53:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.4795, val/total_loss: 2.4795, val/hateful_memes/accuracy: 0.6704, val/hateful_memes/binary_f1: 0.3862, val/hateful_memes/roc_auc: 0.6960, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 49s 141ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-25T22:55:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.1846, train/total_loss: 0.0058, train/total_loss/avg: 0.1846, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 702ms, time_since_start: 02h 54s 075ms, eta: 04h 04m 13s 653ms\n","\u001b[32m2022-04-25T22:56:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1821, train/total_loss: 0.0050, train/total_loss/avg: 0.1821, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 724ms, time_since_start: 02h 02m 29s 799ms, eta: 04h 08s 032ms\n","\u001b[32m2022-04-25T22:58:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0036, train/hateful_memes/cross_entropy/avg: 0.1796, train/total_loss: 0.0036, train/total_loss/avg: 0.1796, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 726ms, time_since_start: 02h 04m 05s 526ms, eta: 03h 58m 31s 033ms\n","\u001b[32m2022-04-25T22:59:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1772, train/total_loss: 0.0035, train/total_loss/avg: 0.1772, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 755ms, time_since_start: 02h 05m 41s 281ms, eta: 03h 56m 57s 901ms\n","\u001b[32m2022-04-25T23:01:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0027, train/hateful_memes/cross_entropy/avg: 0.1749, train/total_loss: 0.0027, train/total_loss/avg: 0.1749, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 259ms, time_since_start: 02h 07m 16s 541ms, eta: 03h 54m 07s 509ms\n","\u001b[32m2022-04-25T23:02:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.1726, train/total_loss: 0.0022, train/total_loss/avg: 0.1726, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 668ms, time_since_start: 02h 08m 52s 209ms, eta: 03h 53m 30s 500ms\n","\u001b[32m2022-04-25T23:04:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.1715, train/total_loss: 0.0022, train/total_loss/avg: 0.1715, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 670ms, time_since_start: 02h 10m 27s 880ms, eta: 03h 51m 53s 515ms\n","\u001b[32m2022-04-25T23:06:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.1694, train/total_loss: 0.0022, train/total_loss/avg: 0.1694, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 455ms, time_since_start: 02h 12m 03s 336ms, eta: 03h 49m 45s 073ms\n","\u001b[32m2022-04-25T23:07:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1673, train/total_loss: 0.0020, train/total_loss/avg: 0.1673, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 582ms, time_since_start: 02h 13m 38s 919ms, eta: 03h 48m 26s 296ms\n","\u001b[32m2022-04-25T23:09:20 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-25T23:09:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T23:09:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T23:09:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T23:09:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1652, train/total_loss: 0.0020, train/total_loss/avg: 0.1652, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 603ms, time_since_start: 02h 15m 35s 522ms, eta: 04h 36m 41s 975ms\n","\u001b[32m2022-04-25T23:09:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-25T23:09:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-25T23:09:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-25T23:09:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T23:09:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T23:09:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T23:10:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T23:10:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.0902, val/total_loss: 2.0902, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.4788, val/hateful_memes/roc_auc: 0.6884, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 29s 117ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-25T23:11:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1634, train/total_loss: 0.0020, train/total_loss/avg: 0.1634, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 596ms, time_since_start: 02h 17m 41s 237ms, eta: 03h 47m 35s 106ms\n","\u001b[32m2022-04-25T23:13:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1626, train/total_loss: 0.0020, train/total_loss/avg: 0.1626, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 903ms, time_since_start: 02h 19m 17s 140ms, eta: 03h 44m 19s 674ms\n","\u001b[32m2022-04-25T23:14:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1607, train/total_loss: 0.0018, train/total_loss/avg: 0.1607, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 394ms, time_since_start: 02h 20m 52s 535ms, eta: 03h 41m 31s 233ms\n","\u001b[32m2022-04-25T23:16:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1587, train/total_loss: 0.0016, train/total_loss/avg: 0.1587, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 796ms, time_since_start: 02h 22m 28s 331ms, eta: 03h 40m 49s 800ms\n","\u001b[32m2022-04-25T23:18:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1574, train/total_loss: 0.0016, train/total_loss/avg: 0.1574, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 884ms, time_since_start: 02h 24m 04s 215ms, eta: 03h 39m 24s 399ms\n","\u001b[32m2022-04-25T23:19:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1556, train/total_loss: 0.0018, train/total_loss/avg: 0.1556, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 401ms, time_since_start: 02h 25m 39s 617ms, eta: 03h 36m 41s 097ms\n","\u001b[32m2022-04-25T23:21:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0027, train/hateful_memes/cross_entropy/avg: 0.1538, train/total_loss: 0.0027, train/total_loss/avg: 0.1538, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 104ms, time_since_start: 02h 27m 15s 722ms, eta: 03h 36m 39s 243ms\n","\u001b[32m2022-04-25T23:22:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0027, train/hateful_memes/cross_entropy/avg: 0.1522, train/total_loss: 0.0027, train/total_loss/avg: 0.1522, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 055ms, time_since_start: 02h 28m 51s 777ms, eta: 03h 34m 54s 885ms\n","\u001b[32m2022-04-25T23:24:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1505, train/total_loss: 0.0018, train/total_loss/avg: 0.1505, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 274ms, time_since_start: 02h 30m 28s 051ms, eta: 03h 33m 46s 303ms\n","\u001b[32m2022-04-25T23:26:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-25T23:26:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T23:26:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T23:26:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T23:26:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1488, train/total_loss: 0.0018, train/total_loss/avg: 0.1488, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00001, ups: 0.85, time: 01m 57s 952ms, time_since_start: 02h 32m 26s 004ms, eta: 04h 19m 54s 549ms\n","\u001b[32m2022-04-25T23:26:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-25T23:26:32 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-25T23:26:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-25T23:26:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T23:26:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T23:26:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T23:26:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T23:26:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.5764, val/total_loss: 2.5764, val/hateful_memes/accuracy: 0.6556, val/hateful_memes/binary_f1: 0.3961, val/hateful_memes/roc_auc: 0.6633, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 22000, val_time: 26s 440ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-25T23:28:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1472, train/total_loss: 0.0018, train/total_loss/avg: 0.1472, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 435ms, time_since_start: 02h 34m 28s 882ms, eta: 03h 30m 51s 668ms\n","\u001b[32m2022-04-25T23:30:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1461, train/total_loss: 0.0018, train/total_loss/avg: 0.1461, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 342ms, time_since_start: 02h 36m 05s 225ms, eta: 03h 29m 01s 542ms\n","\u001b[32m2022-04-25T23:31:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1445, train/total_loss: 0.0018, train/total_loss/avg: 0.1445, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 067ms, time_since_start: 02h 37m 41s 292ms, eta: 03h 26m 47s 938ms\n","\u001b[32m2022-04-25T23:33:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1430, train/total_loss: 0.0018, train/total_loss/avg: 0.1430, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 828ms, time_since_start: 02h 39m 17s 120ms, eta: 03h 24m 39s 607ms\n","\u001b[32m2022-04-25T23:34:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1416, train/total_loss: 0.0021, train/total_loss/avg: 0.1416, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 060ms, time_since_start: 02h 40m 53s 180ms, eta: 03h 23m 31s 663ms\n","\u001b[32m2022-04-25T23:36:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0036, train/hateful_memes/cross_entropy/avg: 0.1406, train/total_loss: 0.0036, train/total_loss/avg: 0.1406, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 505ms, time_since_start: 02h 42m 28s 686ms, eta: 03h 20m 44s 052ms\n","\u001b[32m2022-04-25T23:38:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1391, train/total_loss: 0.0021, train/total_loss/avg: 0.1391, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 929ms, time_since_start: 02h 44m 04s 616ms, eta: 03h 19m 59s 972ms\n","\u001b[32m2022-04-25T23:39:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1377, train/total_loss: 0.0021, train/total_loss/avg: 0.1377, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 812ms, time_since_start: 02h 45m 40s 428ms, eta: 03h 18m 07s 830ms\n","\u001b[32m2022-04-25T23:41:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1364, train/total_loss: 0.0021, train/total_loss/avg: 0.1364, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 489ms, time_since_start: 02h 47m 15s 918ms, eta: 03h 15m 50s 629ms\n","\u001b[32m2022-04-25T23:42:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-25T23:42:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T23:43:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T23:43:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T23:43:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1350, train/total_loss: 0.0021, train/total_loss/avg: 0.1350, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00001, ups: 0.85, time: 01m 57s 892ms, time_since_start: 02h 49m 13s 811ms, eta: 03h 59m 47s 660ms\n","\u001b[32m2022-04-25T23:43:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-25T23:43:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-25T23:43:25 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-25T23:43:25 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T23:43:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T23:43:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-25T23:43:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-25T23:43:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.4036, val/total_loss: 2.4036, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.4466, val/hateful_memes/roc_auc: 0.6917, num_updates: 10000, epoch: 38, iterations: 10000, max_updates: 22000, val_time: 29s 798ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-25T23:45:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1337, train/total_loss: 0.0018, train/total_loss/avg: 0.1337, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 960ms, time_since_start: 02h 51m 20s 572ms, eta: 03h 15m 34s 412ms\n","\u001b[32m2022-04-25T23:47:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1324, train/total_loss: 0.0017, train/total_loss/avg: 0.1324, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 662ms, time_since_start: 02h 52m 56s 234ms, eta: 03h 11m 20s 059ms\n","\u001b[32m2022-04-25T23:48:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1311, train/total_loss: 0.0012, train/total_loss/avg: 0.1311, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 048ms, time_since_start: 02h 54m 32s 283ms, eta: 03h 10m 28s 748ms\n","\u001b[32m2022-04-25T23:50:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1298, train/total_loss: 0.0012, train/total_loss/avg: 0.1298, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 480ms, time_since_start: 02h 56m 07s 764ms, eta: 03h 07m 44s 061ms\n","\u001b[32m2022-04-25T23:51:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1286, train/total_loss: 0.0012, train/total_loss/avg: 0.1286, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 713ms, time_since_start: 02h 57m 43s 477ms, eta: 03h 06m 34s 134ms\n","\u001b[32m2022-04-25T23:53:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1275, train/total_loss: 0.0012, train/total_loss/avg: 0.1275, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 789ms, time_since_start: 02h 59m 19s 266ms, eta: 03h 05m 05s 653ms\n","\u001b[32m2022-04-25T23:55:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1263, train/total_loss: 0.0012, train/total_loss/avg: 0.1263, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 295ms, time_since_start: 03h 54s 562ms, eta: 03h 02m 31s 468ms\n","\u001b[32m2022-04-25T23:56:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1251, train/total_loss: 0.0012, train/total_loss/avg: 0.1251, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 625ms, time_since_start: 03h 02m 30s 187ms, eta: 03h 01m 32s 129ms\n","\u001b[32m2022-04-25T23:58:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1241, train/total_loss: 0.0012, train/total_loss/avg: 0.1241, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 472ms, time_since_start: 03h 04m 05s 660ms, eta: 02h 59m 37s 619ms\n","\u001b[32m2022-04-25T23:59:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-25T23:59:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-25T23:59:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T00:00:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T00:00:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1229, train/total_loss: 0.0010, train/total_loss/avg: 0.1229, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 797ms, time_since_start: 03h 06m 01s 457ms, eta: 03h 35m 54s 230ms\n","\u001b[32m2022-04-26T00:00:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-26T00:00:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-26T00:00:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-26T00:00:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T00:00:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-26T00:00:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T00:00:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T00:00:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.3364, val/total_loss: 2.3364, val/hateful_memes/accuracy: 0.6685, val/hateful_memes/binary_f1: 0.4317, val/hateful_memes/roc_auc: 0.6785, num_updates: 11000, epoch: 42, iterations: 11000, max_updates: 22000, val_time: 29s 875ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-26T00:02:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1218, train/total_loss: 0.0010, train/total_loss/avg: 0.1218, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 472ms, time_since_start: 03h 08m 07s 806ms, eta: 02h 58m 14s 279ms\n","\u001b[32m2022-04-26T00:03:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1208, train/total_loss: 0.0009, train/total_loss/avg: 0.1208, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 998ms, time_since_start: 03h 09m 42s 805ms, eta: 02h 53m 54s 260ms\n","\u001b[32m2022-04-26T00:05:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1197, train/total_loss: 0.0010, train/total_loss/avg: 0.1197, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 518ms, time_since_start: 03h 11m 18s 324ms, eta: 02h 53m 14s 275ms\n","\u001b[32m2022-04-26T00:07:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1187, train/total_loss: 0.0010, train/total_loss/avg: 0.1187, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 509ms, time_since_start: 03h 12m 53s 834ms, eta: 02h 51m 36s 144ms\n","\u001b[32m2022-04-26T00:08:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1176, train/total_loss: 0.0010, train/total_loss/avg: 0.1176, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 233ms, time_since_start: 03h 14m 29s 067ms, eta: 02h 49m 29s 479ms\n","\u001b[32m2022-04-26T00:10:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1166, train/total_loss: 0.0008, train/total_loss/avg: 0.1166, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 544ms, time_since_start: 03h 16m 04s 611ms, eta: 02h 48m 25s 521ms\n","\u001b[32m2022-04-26T00:11:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1156, train/total_loss: 0.0007, train/total_loss/avg: 0.1156, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 472ms, time_since_start: 03h 17m 40s 083ms, eta: 02h 46m 40s 790ms\n","\u001b[32m2022-04-26T00:13:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1147, train/total_loss: 0.0007, train/total_loss/avg: 0.1147, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 071ms, time_since_start: 03h 19m 15s 154ms, eta: 02h 44m 22s 125ms\n","\u001b[32m2022-04-26T00:14:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1137, train/total_loss: 0.0007, train/total_loss/avg: 0.1137, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 474ms, time_since_start: 03h 20m 50s 629ms, eta: 02h 43m 26s 863ms\n","\u001b[32m2022-04-26T00:16:32 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-26T00:16:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-26T00:16:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T00:16:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T00:16:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1128, train/total_loss: 0.0007, train/total_loss/avg: 0.1128, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 788ms, time_since_start: 03h 22m 47s 418ms, eta: 03h 17m 57s 403ms\n","\u001b[32m2022-04-26T00:16:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-26T00:16:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-26T00:16:59 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-26T00:16:59 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T00:16:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.3553, val/total_loss: 2.3553, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.4892, val/hateful_memes/roc_auc: 0.6934, num_updates: 12000, epoch: 46, iterations: 12000, max_updates: 22000, val_time: 06s 154ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-26T00:18:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1123, train/total_loss: 0.0008, train/total_loss/avg: 0.1123, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 338ms, time_since_start: 03h 24m 29s 913ms, eta: 02h 41m 39s 688ms\n","\u001b[32m2022-04-26T00:20:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1113, train/total_loss: 0.0007, train/total_loss/avg: 0.1113, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 594ms, time_since_start: 03h 26m 05s 507ms, eta: 02h 38m 47s 488ms\n","\u001b[32m2022-04-26T00:21:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1104, train/total_loss: 0.0006, train/total_loss/avg: 0.1104, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 997ms, time_since_start: 03h 27m 40s 504ms, eta: 02h 36m 11s 401ms\n","\u001b[32m2022-04-26T00:23:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1095, train/total_loss: 0.0005, train/total_loss/avg: 0.1095, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 515ms, time_since_start: 03h 29m 16s 020ms, eta: 02h 35m 25s 408ms\n","\u001b[32m2022-04-26T00:24:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1087, train/total_loss: 0.0005, train/total_loss/avg: 0.1087, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 405ms, time_since_start: 03h 30m 51s 426ms, eta: 02h 33m 37s 636ms\n","\u001b[32m2022-04-26T00:26:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1079, train/total_loss: 0.0004, train/total_loss/avg: 0.1079, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 426ms, time_since_start: 03h 32m 26s 853ms, eta: 02h 32m 02s 617ms\n","\u001b[32m2022-04-26T00:28:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1070, train/total_loss: 0.0004, train/total_loss/avg: 0.1070, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 538ms, time_since_start: 03h 34m 02s 392ms, eta: 02h 30m 36s 159ms\n","\u001b[32m2022-04-26T00:29:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1062, train/total_loss: 0.0004, train/total_loss/avg: 0.1062, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 930ms, time_since_start: 03h 35m 37s 323ms, eta: 02h 28m 02s 114ms\n","\u001b[32m2022-04-26T00:31:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1054, train/total_loss: 0.0003, train/total_loss/avg: 0.1054, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 416ms, time_since_start: 03h 37m 12s 740ms, eta: 02h 27m 10s 557ms\n","\u001b[32m2022-04-26T00:32:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-26T00:32:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-26T00:33:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T00:33:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T00:33:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1046, train/total_loss: 0.0003, train/total_loss/avg: 0.1046, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 58s 795ms, time_since_start: 03h 39m 11s 536ms, eta: 03h 01m 13s 380ms\n","\u001b[32m2022-04-26T00:33:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-26T00:33:17 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-26T00:33:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-26T00:33:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T00:33:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.7142, val/total_loss: 2.7142, val/hateful_memes/accuracy: 0.6685, val/hateful_memes/binary_f1: 0.3932, val/hateful_memes/roc_auc: 0.6691, num_updates: 13000, epoch: 49, iterations: 13000, max_updates: 22000, val_time: 05s 701ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-26T00:34:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1038, train/total_loss: 0.0003, train/total_loss/avg: 0.1038, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 246ms, time_since_start: 03h 40m 53s 485ms, eta: 02h 25m 11s 518ms\n","\u001b[32m2022-04-26T00:36:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1030, train/total_loss: 0.0003, train/total_loss/avg: 0.1030, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 557ms, time_since_start: 03h 42m 29s 043ms, eta: 02h 22m 32s 028ms\n","\u001b[32m2022-04-26T00:38:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1026, train/total_loss: 0.0003, train/total_loss/avg: 0.1026, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 870ms, time_since_start: 03h 44m 03s 913ms, eta: 02h 19m 54s 014ms\n","\u001b[32m2022-04-26T00:39:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1018, train/total_loss: 0.0004, train/total_loss/avg: 0.1018, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 557ms, time_since_start: 03h 45m 39s 470ms, eta: 02h 19m 17s 621ms\n","\u001b[32m2022-04-26T00:41:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1010, train/total_loss: 0.0004, train/total_loss/avg: 0.1010, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 637ms, time_since_start: 03h 47m 15s 108ms, eta: 02h 17m 47s 416ms\n","\u001b[32m2022-04-26T00:42:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1003, train/total_loss: 0.0003, train/total_loss/avg: 0.1003, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 223ms, time_since_start: 03h 48m 50s 332ms, eta: 02h 15m 34s 781ms\n","\u001b[32m2022-04-26T00:44:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0996, train/total_loss: 0.0004, train/total_loss/avg: 0.0996, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 598ms, time_since_start: 03h 50m 25s 930ms, eta: 02h 14m 29s 549ms\n","\u001b[32m2022-04-26T00:46:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0989, train/total_loss: 0.0003, train/total_loss/avg: 0.0989, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 520ms, time_since_start: 03h 52m 01s 451ms, eta: 02h 12m 45s 833ms\n","\u001b[32m2022-04-26T00:47:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0981, train/total_loss: 0.0004, train/total_loss/avg: 0.0981, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 019ms, time_since_start: 03h 53m 36s 470ms, eta: 02h 10m 27s 429ms\n","\u001b[32m2022-04-26T00:49:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-26T00:49:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-26T00:49:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T00:49:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T00:49:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0974, train/total_loss: 0.0003, train/total_loss/avg: 0.0974, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0., ups: 0.84, time: 01m 59s 365ms, time_since_start: 03h 55m 35s 836ms, eta: 02h 41m 51s 565ms\n","\u001b[32m2022-04-26T00:49:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-26T00:49:42 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-26T00:49:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-26T00:49:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T00:49:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.7443, val/total_loss: 2.7443, val/hateful_memes/accuracy: 0.6704, val/hateful_memes/binary_f1: 0.4367, val/hateful_memes/roc_auc: 0.6755, num_updates: 14000, epoch: 53, iterations: 14000, max_updates: 22000, val_time: 06s 019ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-26T00:51:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0967, train/total_loss: 0.0003, train/total_loss/avg: 0.0967, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 983ms, time_since_start: 03h 57m 17s 840ms, eta: 02h 08m 31s 620ms\n","\u001b[32m2022-04-26T00:52:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0961, train/total_loss: 0.0002, train/total_loss/avg: 0.0961, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 627ms, time_since_start: 03h 58m 53s 468ms, eta: 02h 06m 25s 734ms\n","\u001b[32m2022-04-26T00:54:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0954, train/total_loss: 0.0003, train/total_loss/avg: 0.0954, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 534ms, time_since_start: 04h 29s 002ms, eta: 02h 04m 41s 182ms\n","\u001b[32m2022-04-26T00:56:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0947, train/total_loss: 0.0003, train/total_loss/avg: 0.0947, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 085ms, time_since_start: 04h 02m 04s 087ms, eta: 02h 02m 29s 330ms\n","\u001b[32m2022-04-26T00:57:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0941, train/total_loss: 0.0003, train/total_loss/avg: 0.0941, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 534ms, time_since_start: 04h 03m 39s 622ms, eta: 02h 01m 26s 924ms\n","\u001b[32m2022-04-26T00:59:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0934, train/total_loss: 0.0003, train/total_loss/avg: 0.0934, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 621ms, time_since_start: 04h 05m 15s 243ms, eta: 01h 59m 56s 261ms\n","\u001b[32m2022-04-26T01:00:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0928, train/total_loss: 0.0003, train/total_loss/avg: 0.0928, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 388ms, time_since_start: 04h 06m 50s 631ms, eta: 01h 58m 01s 710ms\n","\u001b[32m2022-04-26T01:02:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0922, train/total_loss: 0.0003, train/total_loss/avg: 0.0922, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 441ms, time_since_start: 04h 08m 26s 073ms, eta: 01h 56m 28s 613ms\n","\u001b[32m2022-04-26T01:04:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0916, train/total_loss: 0.0003, train/total_loss/avg: 0.0916, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 827ms, time_since_start: 04h 10m 900ms, eta: 01h 54m 07s 215ms\n","\u001b[32m2022-04-26T01:05:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-26T01:05:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-26T01:05:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T01:06:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T01:06:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0910, train/total_loss: 0.0003, train/total_loss/avg: 0.0910, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 656ms, time_since_start: 04h 11m 57s 557ms, eta: 02h 18m 24s 750ms\n","\u001b[32m2022-04-26T01:06:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-26T01:06:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-26T01:06:09 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-26T01:06:09 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T01:06:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.6156, val/total_loss: 2.6156, val/hateful_memes/accuracy: 0.6741, val/hateful_memes/binary_f1: 0.4359, val/hateful_memes/roc_auc: 0.6726, num_updates: 15000, epoch: 57, iterations: 15000, max_updates: 22000, val_time: 05s 678ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-26T01:07:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0904, train/total_loss: 0.0003, train/total_loss/avg: 0.0904, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 771ms, time_since_start: 04h 13m 40s 008ms, eta: 01h 53m 10s 761ms\n","\u001b[32m2022-04-26T01:09:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0898, train/total_loss: 0.0002, train/total_loss/avg: 0.0898, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 472ms, time_since_start: 04h 15m 15s 481ms, eta: 01h 50m 02s 475ms\n","\u001b[32m2022-04-26T01:10:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0892, train/total_loss: 0.0002, train/total_loss/avg: 0.0892, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 878ms, time_since_start: 04h 16m 51s 359ms, eta: 01h 48m 53s 083ms\n","\u001b[32m2022-04-26T01:12:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0886, train/total_loss: 0.0002, train/total_loss/avg: 0.0886, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 853ms, time_since_start: 04h 18m 27s 213ms, eta: 01h 47m 13s 891ms\n","\u001b[32m2022-04-26T01:14:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0881, train/total_loss: 0.0001, train/total_loss/avg: 0.0881, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 428ms, time_since_start: 04h 20m 02s 641ms, eta: 01h 45m 08s 281ms\n","\u001b[32m2022-04-26T01:15:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0875, train/total_loss: 0.0001, train/total_loss/avg: 0.0875, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 847ms, time_since_start: 04h 21m 38s 489ms, eta: 01h 43m 58s 543ms\n","\u001b[32m2022-04-26T01:17:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0870, train/total_loss: 0.0001, train/total_loss/avg: 0.0870, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 410ms, time_since_start: 04h 23m 13s 899ms, eta: 01h 41m 53s 022ms\n","\u001b[32m2022-04-26T01:18:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0864, train/total_loss: 0.0001, train/total_loss/avg: 0.0864, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 693ms, time_since_start: 04h 24m 49s 593ms, eta: 01h 40m 33s 872ms\n","\u001b[32m2022-04-26T01:20:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0859, train/total_loss: 0.0001, train/total_loss/avg: 0.0859, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 645ms, time_since_start: 04h 26m 25s 239ms, eta: 01h 38m 53s 563ms\n","\u001b[32m2022-04-26T01:22:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-26T01:22:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-26T01:22:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T01:22:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T01:22:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0853, train/total_loss: 0.0001, train/total_loss/avg: 0.0853, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 58s 222ms, time_since_start: 04h 28m 23s 461ms, eta: 02h 13s 942ms\n","\u001b[32m2022-04-26T01:22:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-26T01:22:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-26T01:22:35 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-26T01:22:35 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T01:22:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 2.8128, val/total_loss: 2.8128, val/hateful_memes/accuracy: 0.6741, val/hateful_memes/binary_f1: 0.4172, val/hateful_memes/roc_auc: 0.6849, num_updates: 16000, epoch: 61, iterations: 16000, max_updates: 22000, val_time: 05s 647ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-26T01:24:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0848, train/total_loss: 0.0001, train/total_loss/avg: 0.0848, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 464ms, time_since_start: 04h 30m 05s 574ms, eta: 01h 36m 28s 148ms\n","\u001b[32m2022-04-26T01:25:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0843, train/total_loss: 0.0002, train/total_loss/avg: 0.0843, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 884ms, time_since_start: 04h 31m 41s 459ms, eta: 01h 34m 15s 848ms\n","\u001b[32m2022-04-26T01:27:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0838, train/total_loss: 0.0001, train/total_loss/avg: 0.0838, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 505ms, time_since_start: 04h 33m 16s 964ms, eta: 01h 32m 16s 330ms\n","\u001b[32m2022-04-26T01:28:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0833, train/total_loss: 0.0001, train/total_loss/avg: 0.0833, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 814ms, time_since_start: 04h 34m 52s 779ms, eta: 01h 30m 56s 852ms\n","\u001b[32m2022-04-26T01:30:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0828, train/total_loss: 0.0001, train/total_loss/avg: 0.0828, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 298ms, time_since_start: 04h 36m 28s 077ms, eta: 01h 28m 50s 506ms\n","\u001b[32m2022-04-26T01:32:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0823, train/total_loss: 0.0001, train/total_loss/avg: 0.0823, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 607ms, time_since_start: 04h 38m 03s 684ms, eta: 01h 27m 30s 563ms\n","\u001b[32m2022-04-26T01:33:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0818, train/total_loss: 0.0001, train/total_loss/avg: 0.0818, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 577ms, time_since_start: 04h 39m 39s 262ms, eta: 01h 25m 51s 739ms\n","\u001b[32m2022-04-26T01:35:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0813, train/total_loss: 0.0001, train/total_loss/avg: 0.0813, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 308ms, time_since_start: 04h 41m 14s 571ms, eta: 01h 24m 312ms\n","\u001b[32m2022-04-26T01:36:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0808, train/total_loss: 0.0001, train/total_loss/avg: 0.0808, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 576ms, time_since_start: 04h 42m 50s 148ms, eta: 01h 22m 37s 283ms\n","\u001b[32m2022-04-26T01:38:32 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-26T01:38:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-26T01:38:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T01:38:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T01:38:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0803, train/total_loss: 0.0001, train/total_loss/avg: 0.0803, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0., ups: 0.84, time: 01m 59s 182ms, time_since_start: 04h 44m 49s 330ms, eta: 01h 41m 414ms\n","\u001b[32m2022-04-26T01:38:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-26T01:38:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-26T01:39:01 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-26T01:39:01 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T01:39:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 2.8654, val/total_loss: 2.8654, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.4601, val/hateful_memes/roc_auc: 0.6849, num_updates: 17000, epoch: 64, iterations: 17000, max_updates: 22000, val_time: 05s 849ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-26T01:40:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0798, train/total_loss: 0.0001, train/total_loss/avg: 0.0798, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 023ms, time_since_start: 04h 46m 31s 204ms, eta: 01h 19m 45s 141ms\n","\u001b[32m2022-04-26T01:42:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0794, train/total_loss: 0.0001, train/total_loss/avg: 0.0794, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 642ms, time_since_start: 04h 48m 06s 846ms, eta: 01h 17m 48s 874ms\n","\u001b[32m2022-04-26T01:43:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0789, train/total_loss: 0.0001, train/total_loss/avg: 0.0789, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 446ms, time_since_start: 04h 49m 42s 293ms, eta: 01h 16m 02s 246ms\n","\u001b[32m2022-04-26T01:45:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0785, train/total_loss: 0.0001, train/total_loss/avg: 0.0785, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 617ms, time_since_start: 04h 51m 17s 910ms, eta: 01h 14m 33s 164ms\n","\u001b[32m2022-04-26T01:46:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0780, train/total_loss: 0.0001, train/total_loss/avg: 0.0780, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 663ms, time_since_start: 04h 52m 53s 574ms, eta: 01h 12m 58s 062ms\n","\u001b[32m2022-04-26T01:48:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0776, train/total_loss: 0.0001, train/total_loss/avg: 0.0776, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 226ms, time_since_start: 04h 54m 28s 800ms, eta: 01h 11m 01s 184ms\n","\u001b[32m2022-04-26T01:50:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0771, train/total_loss: 0.0001, train/total_loss/avg: 0.0771, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 887ms, time_since_start: 04h 56m 04s 688ms, eta: 01h 09m 53s 278ms\n","\u001b[32m2022-04-26T01:51:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0767, train/total_loss: 0.0001, train/total_loss/avg: 0.0767, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 629ms, time_since_start: 04h 57m 40s 317ms, eta: 01h 08m 04s 700ms\n","\u001b[32m2022-04-26T01:53:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0763, train/total_loss: 0.0001, train/total_loss/avg: 0.0763, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 377ms, time_since_start: 04h 59m 15s 695ms, eta: 01h 06m 16s 936ms\n","\u001b[32m2022-04-26T01:54:57 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-26T01:54:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-26T01:55:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T01:55:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T01:55:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0759, train/total_loss: 0.0001, train/total_loss/avg: 0.0759, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 287ms, time_since_start: 05h 01m 11s 982ms, eta: 01h 18m 50s 584ms\n","\u001b[32m2022-04-26T01:55:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-26T01:55:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-26T01:55:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-26T01:55:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T01:55:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 2.8131, val/total_loss: 2.8131, val/hateful_memes/accuracy: 0.6630, val/hateful_memes/binary_f1: 0.4551, val/hateful_memes/roc_auc: 0.6800, num_updates: 18000, epoch: 68, iterations: 18000, max_updates: 22000, val_time: 05s 601ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-26T01:57:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0754, train/total_loss: 0.0001, train/total_loss/avg: 0.0754, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 138ms, time_since_start: 05h 02m 53s 724ms, eta: 01h 03m 33s 144ms\n","\u001b[32m2022-04-26T01:58:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0750, train/total_loss: 0.0001, train/total_loss/avg: 0.0750, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 844ms, time_since_start: 05h 04m 29s 568ms, eta: 01h 01m 43s 990ms\n","\u001b[32m2022-04-26T02:00:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0746, train/total_loss: 0.0001, train/total_loss/avg: 0.0746, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 700ms, time_since_start: 05h 06m 05s 269ms, eta: 01h 01s 132ms\n","\u001b[32m2022-04-26T02:01:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0742, train/total_loss: 0.0001, train/total_loss/avg: 0.0742, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 434ms, time_since_start: 05h 07m 40s 703ms, eta: 58m 14s 044ms\n","\u001b[32m2022-04-26T02:03:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0738, train/total_loss: 0.0001, train/total_loss/avg: 0.0738, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 795ms, time_since_start: 05h 09m 16s 499ms, eta: 56m 49s 838ms\n","\u001b[32m2022-04-26T02:04:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0734, train/total_loss: 0.0001, train/total_loss/avg: 0.0734, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 668ms, time_since_start: 05h 10m 52s 167ms, eta: 55m 08s 027ms\n","\u001b[32m2022-04-26T02:06:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0730, train/total_loss: 0.0001, train/total_loss/avg: 0.0730, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 958ms, time_since_start: 05h 12m 27s 126ms, eta: 53m 06s 898ms\n","\u001b[32m2022-04-26T02:08:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0726, train/total_loss: 0.0001, train/total_loss/avg: 0.0726, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 664ms, time_since_start: 05h 14m 02s 790ms, eta: 51m 53s 319ms\n","\u001b[32m2022-04-26T02:09:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0722, train/total_loss: 0.0001, train/total_loss/avg: 0.0722, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 258ms, time_since_start: 05h 15m 38s 049ms, eta: 50m 03s 221ms\n","\u001b[32m2022-04-26T02:11:19 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-26T02:11:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-26T02:11:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T02:11:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T02:11:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0719, train/total_loss: 0.0001, train/total_loss/avg: 0.0719, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 443ms, time_since_start: 05h 17m 34s 493ms, eta: 59m 12s 706ms\n","\u001b[32m2022-04-26T02:11:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-26T02:11:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-26T02:11:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-26T02:11:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T02:11:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.0095, val/total_loss: 3.0095, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4136, val/hateful_memes/roc_auc: 0.6797, num_updates: 19000, epoch: 72, iterations: 19000, max_updates: 22000, val_time: 05s 664ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-26T02:13:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0715, train/total_loss: 0.0001, train/total_loss/avg: 0.0715, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 649ms, time_since_start: 05h 19m 16s 809ms, eta: 47m 30s 488ms\n","\u001b[32m2022-04-26T02:14:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0711, train/total_loss: 0.0000, train/total_loss/avg: 0.0711, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 409ms, time_since_start: 05h 20m 52s 218ms, eta: 45m 16s 882ms\n","\u001b[32m2022-04-26T02:16:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0707, train/total_loss: 0.0000, train/total_loss/avg: 0.0707, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 930ms, time_since_start: 05h 22m 28s 148ms, eta: 43m 54s 147ms\n","\u001b[32m2022-04-26T02:18:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0704, train/total_loss: 0.0000, train/total_loss/avg: 0.0704, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 865ms, time_since_start: 05h 24m 04s 014ms, eta: 42m 14s 869ms\n","\u001b[32m2022-04-26T02:19:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0700, train/total_loss: 0.0000, train/total_loss/avg: 0.0700, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 621ms, time_since_start: 05h 25m 39s 636ms, eta: 40m 31s 185ms\n","\u001b[32m2022-04-26T02:21:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0697, train/total_loss: 0.0000, train/total_loss/avg: 0.0697, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 944ms, time_since_start: 05h 27m 15s 580ms, eta: 39m 01s 815ms\n","\u001b[32m2022-04-26T02:22:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0693, train/total_loss: 0.0000, train/total_loss/avg: 0.0693, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 428ms, time_since_start: 05h 28m 51s 009ms, eta: 37m 12s 168ms\n","\u001b[32m2022-04-26T02:24:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0690, train/total_loss: 0.0000, train/total_loss/avg: 0.0690, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 834ms, time_since_start: 05h 30m 26s 843ms, eta: 35m 44s 201ms\n","\u001b[32m2022-04-26T02:26:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0686, train/total_loss: 0.0000, train/total_loss/avg: 0.0686, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 817ms, time_since_start: 05h 32m 02s 661ms, eta: 34m 06s 376ms\n","\u001b[32m2022-04-26T02:27:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-26T02:27:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-26T02:27:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T02:28:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T02:28:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0683, train/total_loss: 0.0000, train/total_loss/avg: 0.0683, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 57s 595ms, time_since_start: 05h 34m 256ms, eta: 39m 51s 885ms\n","\u001b[32m2022-04-26T02:28:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-26T02:28:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-26T02:28:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-26T02:28:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T02:28:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 2.9829, val/total_loss: 2.9829, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4337, val/hateful_memes/roc_auc: 0.6823, num_updates: 20000, epoch: 76, iterations: 20000, max_updates: 22000, val_time: 05s 871ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-26T02:29:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0679, train/total_loss: 0.0000, train/total_loss/avg: 0.0679, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 833ms, time_since_start: 05h 35m 42s 962ms, eta: 31m 11s 104ms\n","\u001b[32m2022-04-26T02:31:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0676, train/total_loss: 0.0000, train/total_loss/avg: 0.0676, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 043ms, time_since_start: 05h 37m 19s 005ms, eta: 29m 18s 166ms\n","\u001b[32m2022-04-26T02:33:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0673, train/total_loss: 0.0000, train/total_loss/avg: 0.0673, max mem: 10794.0, experiment: run, epoch: 77, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 352ms, time_since_start: 05h 38m 54s 358ms, eta: 27m 28s 558ms\n","\u001b[32m2022-04-26T02:34:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0669, train/total_loss: 0.0000, train/total_loss/avg: 0.0669, max mem: 10794.0, experiment: run, epoch: 77, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 879ms, time_since_start: 05h 40m 30s 238ms, eta: 26m 154ms\n","\u001b[32m2022-04-26T02:36:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0666, train/total_loss: 0.0000, train/total_loss/avg: 0.0666, max mem: 10794.0, experiment: run, epoch: 78, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 472ms, time_since_start: 05h 42m 05s 710ms, eta: 24m 16s 429ms\n","\u001b[32m2022-04-26T02:37:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0663, train/total_loss: 0.0001, train/total_loss/avg: 0.0663, max mem: 10794.0, experiment: run, epoch: 78, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 583ms, time_since_start: 05h 43m 41s 294ms, eta: 22m 40s 925ms\n","\u001b[32m2022-04-26T02:39:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0660, train/total_loss: 0.0000, train/total_loss/avg: 0.0660, max mem: 10794.0, experiment: run, epoch: 78, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 545ms, time_since_start: 05h 45m 16s 840ms, eta: 21m 03s 209ms\n","\u001b[32m2022-04-26T02:40:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0656, train/total_loss: 0.0001, train/total_loss/avg: 0.0656, max mem: 10794.0, experiment: run, epoch: 79, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 912ms, time_since_start: 05h 46m 51s 752ms, eta: 19m 18s 310ms\n","\u001b[32m2022-04-26T02:42:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0653, train/total_loss: 0.0001, train/total_loss/avg: 0.0653, max mem: 10794.0, experiment: run, epoch: 79, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 510ms, time_since_start: 05h 48m 27s 263ms, eta: 17m 48s 478ms\n","\u001b[32m2022-04-26T02:44:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-26T02:44:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-26T02:44:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T02:44:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T02:44:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0650, train/total_loss: 0.0000, train/total_loss/avg: 0.0650, max mem: 10794.0, experiment: run, epoch: 79, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 0.84, time: 01m 59s 213ms, time_since_start: 05h 50m 26s 477ms, eta: 20m 12s 402ms\n","\u001b[32m2022-04-26T02:44:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-26T02:44:32 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-26T02:44:38 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-26T02:44:38 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T02:44:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 3.0915, val/total_loss: 3.0915, val/hateful_memes/accuracy: 0.6704, val/hateful_memes/binary_f1: 0.4295, val/hateful_memes/roc_auc: 0.6817, num_updates: 21000, epoch: 79, iterations: 21000, max_updates: 22000, val_time: 06s 188ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-26T02:46:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0647, train/total_loss: 0.0000, train/total_loss/avg: 0.0647, max mem: 10794.0, experiment: run, epoch: 80, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 137ms, time_since_start: 05h 52m 08s 803ms, eta: 14m 39s 944ms\n","\u001b[32m2022-04-26T02:47:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0644, train/total_loss: 0.0001, train/total_loss/avg: 0.0644, max mem: 10794.0, experiment: run, epoch: 80, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 542ms, time_since_start: 05h 53m 44s 346ms, eta: 12m 57s 334ms\n","\u001b[32m2022-04-26T02:49:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0641, train/total_loss: 0.0000, train/total_loss/avg: 0.0641, max mem: 10794.0, experiment: run, epoch: 81, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 941ms, time_since_start: 05h 55m 19s 287ms, eta: 11m 15s 888ms\n","\u001b[32m2022-04-26T02:51:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0638, train/total_loss: 0.0001, train/total_loss/avg: 0.0638, max mem: 10794.0, experiment: run, epoch: 81, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 553ms, time_since_start: 05h 56m 54s 841ms, eta: 09m 43s 068ms\n","\u001b[32m2022-04-26T02:52:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0635, train/total_loss: 0.0000, train/total_loss/avg: 0.0635, max mem: 10794.0, experiment: run, epoch: 81, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 674ms, time_since_start: 05h 58m 30s 516ms, eta: 08m 06s 505ms\n","\u001b[32m2022-04-26T02:54:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0632, train/total_loss: 0.0000, train/total_loss/avg: 0.0632, max mem: 10794.0, experiment: run, epoch: 82, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 292ms, time_since_start: 06h 05s 808ms, eta: 06m 27s 648ms\n","\u001b[32m2022-04-26T02:55:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0629, train/total_loss: 0.0000, train/total_loss/avg: 0.0629, max mem: 10794.0, experiment: run, epoch: 82, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 627ms, time_since_start: 06h 01m 41s 435ms, eta: 04m 51s 758ms\n","\u001b[32m2022-04-26T02:57:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0626, train/total_loss: 0.0000, train/total_loss/avg: 0.0626, max mem: 10794.0, experiment: run, epoch: 82, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 712ms, time_since_start: 06h 03m 17s 148ms, eta: 03m 14s 680ms\n","\u001b[32m2022-04-26T02:58:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0624, train/total_loss: 0.0000, train/total_loss/avg: 0.0624, max mem: 10794.0, experiment: run, epoch: 83, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 164ms, time_since_start: 06h 04m 52s 312ms, eta: 01m 36s 781ms\n","\u001b[32m2022-04-26T03:00:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-26T03:00:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-26T03:00:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-26T03:00:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-26T03:00:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0621, train/total_loss: 0.0000, train/total_loss/avg: 0.0621, max mem: 10794.0, experiment: run, epoch: 83, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 0.83, time: 02m 820ms, time_since_start: 06h 06m 53s 133ms, eta: 0ms\n","\u001b[32m2022-04-26T03:00:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-26T03:00:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-26T03:01:05 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-26T03:01:05 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T03:01:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/hateful_memes/cross_entropy: 3.0395, val/total_loss: 3.0395, val/hateful_memes/accuracy: 0.6630, val/hateful_memes/binary_f1: 0.4277, val/hateful_memes/roc_auc: 0.6839, num_updates: 22000, epoch: 83, iterations: 22000, max_updates: 22000, val_time: 05s 835ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.696000\n","\u001b[32m2022-04-26T03:01:05 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2022-04-26T03:01:05 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-04-26T03:01:05 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-04-26T03:01:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-26T03:01:27 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 7000\n","\u001b[32m2022-04-26T03:01:27 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 7000\n","\u001b[32m2022-04-26T03:01:27 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 27\n","\u001b[32m2022-04-26T03:01:29 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-04-26T03:01:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:20<00:00,  3.01it/s]\n","\u001b[32m2022-04-26T03:01:50 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 63\n","\u001b[32m2022-04-26T03:01:50 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-26T03:01:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, test/hateful_memes/cross_entropy: 2.4097, test/total_loss: 2.4097, test/hateful_memes/accuracy: 0.7030, test/hateful_memes/binary_f1: 0.4560, test/hateful_memes/roc_auc: 0.7289\n","\u001b[32m2022-04-26T03:01:51 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 06h 07m 44s 769ms\n"]}],"source":["!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml model=vilbert dataset=hateful_memes"]},{"cell_type":"markdown","metadata":{"id":"T1-cVxkDpDhW"},"source":["\n","\n","## **Vilbert section** "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"MNPI6gj9kwqa","outputId":"c735e376-1566-43d9-e944-92a68b1af9d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-20T02:42:45 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\n","\u001b[32m2022-04-20T02:42:45 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2022-04-20T02:42:45 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-20T02:42:46 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-20T02:42:46 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes'])\n","\u001b[32m2022-04-20T02:42:46 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-20T02:42:46 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-04-20T02:42:46 | mmf_cli.run: \u001b[0mUsing seed 46051916\n","\u001b[32m2022-04-20T02:42:46 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T02:42:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T02:42:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T02:42:49 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T02:42:49 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T02:42:49 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T02:42:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bi_attention_type\": 1,\n","  \"bi_hidden_size\": 1024,\n","  \"bi_intermediate_size\": 1024,\n","  \"bi_num_attention_heads\": 8,\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"cut_first\": \"text\",\n","  \"dynamic_attention\": false,\n","  \"embedding_strategy\": \"plain\",\n","  \"fast_mode\": false,\n","  \"finetune_lr_multiplier\": 1,\n","  \"fixed_t_layer\": 0,\n","  \"fixed_v_layer\": 0,\n","  \"freeze_base\": false,\n","  \"fusion_method\": \"mul\",\n","  \"gradient_checkpointing\": false,\n","  \"hard_cap_seq_len\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"in_batch_pairs\": false,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"vilbert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_negative\": 128,\n","  \"objective\": 0,\n","  \"pad_token_id\": 0,\n","  \"pooling_method\": \"mul\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"t_biattention_id\": [\n","    6,\n","    7,\n","    8,\n","    9,\n","    10,\n","    11\n","  ],\n","  \"task_specific_tokens\": false,\n","  \"text_only\": false,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"v_attention_probs_dropout_prob\": 0.1,\n","  \"v_biattention_id\": [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","  ],\n","  \"v_feature_size\": 2048,\n","  \"v_hidden_act\": \"gelu\",\n","  \"v_hidden_dropout_prob\": 0.1,\n","  \"v_hidden_size\": 1024,\n","  \"v_initializer_range\": 0.02,\n","  \"v_intermediate_size\": 1024,\n","  \"v_num_attention_heads\": 8,\n","  \"v_num_hidden_layers\": 6,\n","  \"v_target_size\": 1601,\n","  \"visual_embedding_dim\": 2048,\n","  \"visual_target\": 0,\n","  \"visualization\": false,\n","  \"vocab_size\": 30522,\n","  \"with_coattention\": true\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.v_pooler.dense.bias', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.v_pooler.dense.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.v_embeddings.image_embeddings.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-20T02:42:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-20T02:42:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-20T02:42:58 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-20T02:42:58 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n","  (model): ViLBERTForClassification(\n","    (bert): ViLBERTBase(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (v_embeddings): BertImageFeatureEmbeddings(\n","        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n","        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (v_layer): ModuleList(\n","          (0): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (c_layer): ModuleList(\n","          (0): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (t_pooler): BertTextPooler(\n","        (dense): Linear(in_features=768, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","      (v_pooler): BertImagePooler(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=1024, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-20T02:42:59 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n","\u001b[32m2022-04-20T02:42:59 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-04-20T02:44:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.7424, train/hateful_memes/cross_entropy/avg: 0.7424, train/total_loss: 0.7424, train/total_loss/avg: 0.7424, max mem: 10793.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.56, time: 01m 04s 416ms, time_since_start: 01m 04s 462ms, eta: 03h 59m 06s 956ms\n","\u001b[32m2022-04-20T02:45:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6010, train/hateful_memes/cross_entropy/avg: 0.6717, train/total_loss: 0.6010, train/total_loss/avg: 0.6717, max mem: 10793.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 204ms, time_since_start: 02m 05s 667ms, eta: 03h 46m 09s 519ms\n","\u001b[32m2022-04-20T02:46:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6010, train/hateful_memes/cross_entropy/avg: 0.6018, train/total_loss: 0.6010, train/total_loss/avg: 0.6018, max mem: 10793.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 991ms, time_since_start: 03m 07s 659ms, eta: 03h 48m 876ms\n","\u001b[32m2022-04-20T02:47:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6010, train/hateful_memes/cross_entropy/avg: 0.6021, train/total_loss: 0.6010, train/total_loss/avg: 0.6021, max mem: 10793.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 423ms, time_since_start: 04m 09s 083ms, eta: 03h 44m 52s 997ms\n","\u001b[32m2022-04-20T02:48:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.6010, train/hateful_memes/cross_entropy/avg: 0.6001, train/total_loss: 0.6010, train/total_loss/avg: 0.6001, max mem: 10793.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 243ms, time_since_start: 05m 10s 326ms, eta: 03h 43m 11s 243ms\n","\u001b[32m2022-04-20T02:49:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.6010, train/hateful_memes/cross_entropy/avg: 0.6038, train/total_loss: 0.6010, train/total_loss/avg: 0.6038, max mem: 10793.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 795ms, time_since_start: 06m 12s 122ms, eta: 03h 44m 09s 069ms\n","\u001b[32m2022-04-20T02:50:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6031, train/hateful_memes/cross_entropy/avg: 0.6120, train/total_loss: 0.6031, train/total_loss/avg: 0.6120, max mem: 10793.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 471ms, time_since_start: 07m 13s 594ms, eta: 03h 41m 56s 116ms\n","\u001b[32m2022-04-20T02:51:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.6010, train/hateful_memes/cross_entropy/avg: 0.6039, train/total_loss: 0.6010, train/total_loss/avg: 0.6039, max mem: 10793.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 254ms, time_since_start: 08m 15s 849ms, eta: 03h 43m 42s 416ms\n","\u001b[32m2022-04-20T02:52:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.6010, train/hateful_memes/cross_entropy/avg: 0.6017, train/total_loss: 0.6010, train/total_loss/avg: 0.6017, max mem: 10793.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 630ms, time_since_start: 09m 17s 479ms, eta: 03h 40m 25s 091ms\n","\u001b[32m2022-04-20T02:53:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T02:53:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T02:53:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T02:53:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T02:53:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.5919, train/hateful_memes/cross_entropy/avg: 0.5845, train/total_loss: 0.5919, train/total_loss/avg: 0.5845, max mem: 10793.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 498ms, time_since_start: 10m 44s 981ms, eta: 05h 11m 27s 686ms\n","\u001b[32m2022-04-20T02:53:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T02:53:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-20T02:53:52 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T02:53:52 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T02:53:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T02:54:07 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-20T02:54:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T02:54:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T02:54:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.7372, val/total_loss: 0.7372, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.3699, val/hateful_memes/roc_auc: 0.6097, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 54s 735ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.609750\n","\u001b[32m2022-04-20T02:55:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.5919, train/hateful_memes/cross_entropy/avg: 0.5688, train/total_loss: 0.5919, train/total_loss/avg: 0.5688, max mem: 10793.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 304ms, time_since_start: 12m 58s 027ms, eta: 04h 37m 23s 857ms\n","\u001b[32m2022-04-20T02:56:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.5842, train/hateful_memes/cross_entropy/avg: 0.5523, train/total_loss: 0.5842, train/total_loss/avg: 0.5523, max mem: 10793.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 479ms, time_since_start: 14m 507ms, eta: 03h 40m 16s 703ms\n","\u001b[32m2022-04-20T02:58:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.5842, train/hateful_memes/cross_entropy/avg: 0.5374, train/total_loss: 0.5842, train/total_loss/avg: 0.5374, max mem: 10793.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 422ms, time_since_start: 15m 01s 929ms, eta: 03h 35m 30s 524ms\n","\u001b[32m2022-04-20T02:59:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.5468, train/hateful_memes/cross_entropy/avg: 0.5284, train/total_loss: 0.5468, train/total_loss/avg: 0.5284, max mem: 10793.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 049ms, time_since_start: 16m 03s 978ms, eta: 03h 36m 39s 441ms\n","\u001b[32m2022-04-20T03:00:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.5842, train/hateful_memes/cross_entropy/avg: 0.5363, train/total_loss: 0.5842, train/total_loss/avg: 0.5363, max mem: 10793.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 584ms, time_since_start: 17m 05s 563ms, eta: 03h 33m 59s 401ms\n","\u001b[32m2022-04-20T03:01:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.5468, train/hateful_memes/cross_entropy/avg: 0.5218, train/total_loss: 0.5468, train/total_loss/avg: 0.5218, max mem: 10793.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 250ms, time_since_start: 18m 07s 813ms, eta: 03h 35m 15s 026ms\n","\u001b[32m2022-04-20T03:02:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.5468, train/hateful_memes/cross_entropy/avg: 0.5128, train/total_loss: 0.5468, train/total_loss/avg: 0.5128, max mem: 10793.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 436ms, time_since_start: 19m 09s 250ms, eta: 03h 31m 23s 556ms\n","\u001b[32m2022-04-20T03:03:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.5468, train/hateful_memes/cross_entropy/avg: 0.5167, train/total_loss: 0.5468, train/total_loss/avg: 0.5167, max mem: 10793.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 425ms, time_since_start: 20m 10s 675ms, eta: 03h 30m 18s 865ms\n","\u001b[32m2022-04-20T03:04:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.5468, train/hateful_memes/cross_entropy/avg: 0.5016, train/total_loss: 0.5468, train/total_loss/avg: 0.5016, max mem: 10793.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 184ms, time_since_start: 21m 12s 860ms, eta: 03h 31m 51s 598ms\n","\u001b[32m2022-04-20T03:05:13 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T03:05:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T03:05:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T03:05:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T03:05:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.4619, train/hateful_memes/cross_entropy/avg: 0.4864, train/total_loss: 0.4619, train/total_loss/avg: 0.4864, max mem: 10793.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 1.16, time: 01m 26s 839ms, time_since_start: 22m 39s 699ms, eta: 04h 54m 23s 122ms\n","\u001b[32m2022-04-20T03:05:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T03:05:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T03:05:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T03:05:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T03:05:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T03:05:48 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T03:05:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T03:06:03 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-20T03:06:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T03:06:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T03:06:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 0.7473, val/total_loss: 0.7473, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.5167, val/hateful_memes/roc_auc: 0.7025, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 54s 847ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T03:07:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.4296, train/hateful_memes/cross_entropy/avg: 0.4706, train/total_loss: 0.4296, train/total_loss/avg: 0.4706, max mem: 10793.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 967ms, time_since_start: 24m 50s 518ms, eta: 04h 16m 14s 607ms\n","\u001b[32m2022-04-20T03:08:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.4125, train/hateful_memes/cross_entropy/avg: 0.4624, train/total_loss: 0.4125, train/total_loss/avg: 0.4624, max mem: 10793.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 1.47, time: 01m 08s 088ms, time_since_start: 25m 58s 607ms, eta: 03h 48m 30s 782ms\n","\u001b[32m2022-04-20T03:09:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.4107, train/hateful_memes/cross_entropy/avg: 0.4582, train/total_loss: 0.4107, train/total_loss/avg: 0.4582, max mem: 10793.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 486ms, time_since_start: 27m 093ms, eta: 03h 25m 18s 824ms\n","\u001b[32m2022-04-20T03:11:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.3710, train/hateful_memes/cross_entropy/avg: 0.4444, train/total_loss: 0.3710, train/total_loss/avg: 0.4444, max mem: 10793.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 499ms, time_since_start: 28m 02s 593ms, eta: 03h 27m 38s 178ms\n","\u001b[32m2022-04-20T03:12:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.3688, train/hateful_memes/cross_entropy/avg: 0.4344, train/total_loss: 0.3688, train/total_loss/avg: 0.4344, max mem: 10793.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 784ms, time_since_start: 29m 04s 377ms, eta: 03h 24m 12s 732ms\n","\u001b[32m2022-04-20T03:13:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.3641, train/hateful_memes/cross_entropy/avg: 0.4202, train/total_loss: 0.3641, train/total_loss/avg: 0.4202, max mem: 10793.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 588ms, time_since_start: 30m 05s 966ms, eta: 03h 22m 31s 381ms\n","\u001b[32m2022-04-20T03:14:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.3581, train/hateful_memes/cross_entropy/avg: 0.4060, train/total_loss: 0.3581, train/total_loss/avg: 0.4060, max mem: 10793.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 035ms, time_since_start: 31m 08s 002ms, eta: 03h 22m 56s 463ms\n","\u001b[32m2022-04-20T03:15:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.3050, train/hateful_memes/cross_entropy/avg: 0.3955, train/total_loss: 0.3050, train/total_loss/avg: 0.3955, max mem: 10793.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 761ms, time_since_start: 32m 09s 764ms, eta: 03h 20m 59s 864ms\n","\u001b[32m2022-04-20T03:16:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.3050, train/hateful_memes/cross_entropy/avg: 0.3924, train/total_loss: 0.3050, train/total_loss/avg: 0.3924, max mem: 10793.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 652ms, time_since_start: 33m 11s 417ms, eta: 03h 19m 35s 830ms\n","\u001b[32m2022-04-20T03:17:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T03:17:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T03:17:24 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T03:17:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T03:17:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.2903, train/hateful_memes/cross_entropy/avg: 0.3803, train/total_loss: 0.2903, train/total_loss/avg: 0.3803, max mem: 10793.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 274ms, time_since_start: 34m 39s 691ms, eta: 04h 44m 17s 279ms\n","\u001b[32m2022-04-20T03:17:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T03:17:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T03:17:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T03:17:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T03:17:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T03:17:48 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T03:17:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T03:18:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T03:18:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T03:18:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.4116, val/total_loss: 1.4116, val/hateful_memes/accuracy: 0.6611, val/hateful_memes/binary_f1: 0.3920, val/hateful_memes/roc_auc: 0.6924, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 37s 200ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T03:19:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.2291, train/hateful_memes/cross_entropy/avg: 0.3695, train/total_loss: 0.2291, train/total_loss/avg: 0.3695, max mem: 10793.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 1.37, time: 01m 13s 299ms, time_since_start: 36m 30s 193ms, eta: 03h 54m 49s 170ms\n","\u001b[32m2022-04-20T03:20:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.1987, train/hateful_memes/cross_entropy/avg: 0.3589, train/total_loss: 0.1987, train/total_loss/avg: 0.3589, max mem: 10793.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 1.47, time: 01m 08s 488ms, time_since_start: 37m 38s 682ms, eta: 03h 38m 14s 801ms\n","\u001b[32m2022-04-20T03:21:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.1936, train/hateful_memes/cross_entropy/avg: 0.3492, train/total_loss: 0.1936, train/total_loss/avg: 0.3492, max mem: 10793.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 840ms, time_since_start: 38m 40s 522ms, eta: 03h 16m 750ms\n","\u001b[32m2022-04-20T03:22:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.1542, train/hateful_memes/cross_entropy/avg: 0.3416, train/total_loss: 0.1542, train/total_loss/avg: 0.3416, max mem: 10793.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 853ms, time_since_start: 39m 42s 376ms, eta: 03h 15m 319ms\n","\u001b[32m2022-04-20T03:23:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.1277, train/hateful_memes/cross_entropy/avg: 0.3324, train/total_loss: 0.1277, train/total_loss/avg: 0.3324, max mem: 10793.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 596ms, time_since_start: 40m 44s 972ms, eta: 03h 16m 17s 204ms\n","\u001b[32m2022-04-20T03:24:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.1127, train/hateful_memes/cross_entropy/avg: 0.3260, train/total_loss: 0.1127, train/total_loss/avg: 0.3260, max mem: 10793.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 096ms, time_since_start: 41m 47s 069ms, eta: 03h 13m 39s 965ms\n","\u001b[32m2022-04-20T03:25:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.1046, train/hateful_memes/cross_entropy/avg: 0.3176, train/total_loss: 0.1046, train/total_loss/avg: 0.3176, max mem: 10793.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 727ms, time_since_start: 42m 48s 797ms, eta: 03h 11m 28s 238ms\n","\u001b[32m2022-04-20T03:26:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0923, train/hateful_memes/cross_entropy/avg: 0.3096, train/total_loss: 0.0923, train/total_loss/avg: 0.3096, max mem: 10793.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 101ms, time_since_start: 43m 50s 898ms, eta: 03h 11m 34s 543ms\n","\u001b[32m2022-04-20T03:27:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0674, train/hateful_memes/cross_entropy/avg: 0.3025, train/total_loss: 0.0674, train/total_loss/avg: 0.3025, max mem: 10793.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 963ms, time_since_start: 44m 52s 862ms, eta: 03h 10m 06s 139ms\n","\u001b[32m2022-04-20T03:28:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T03:28:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T03:29:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T03:29:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T03:29:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0472, train/hateful_memes/cross_entropy/avg: 0.2952, train/total_loss: 0.0472, train/total_loss/avg: 0.2952, max mem: 10793.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 922ms, time_since_start: 46m 21s 785ms, eta: 04h 31m 18s 233ms\n","\u001b[32m2022-04-20T03:29:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T03:29:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T03:29:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T03:29:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T03:29:30 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T03:29:30 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T03:29:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T03:29:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T03:29:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T03:29:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.7405, val/total_loss: 1.7405, val/hateful_memes/accuracy: 0.6481, val/hateful_memes/binary_f1: 0.3709, val/hateful_memes/roc_auc: 0.6775, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 38s 272ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T03:31:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0370, train/hateful_memes/cross_entropy/avg: 0.2883, train/total_loss: 0.0370, train/total_loss/avg: 0.2883, max mem: 10793.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 844ms, time_since_start: 48m 12s 904ms, eta: 03h 41m 902ms\n","\u001b[32m2022-04-20T03:32:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0349, train/hateful_memes/cross_entropy/avg: 0.2819, train/total_loss: 0.0349, train/total_loss/avg: 0.2819, max mem: 10793.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 1.54, time: 01m 05s 479ms, time_since_start: 49m 18s 384ms, eta: 03h 17m 33s 498ms\n","\u001b[32m2022-04-20T03:33:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0317, train/hateful_memes/cross_entropy/avg: 0.2761, train/total_loss: 0.0317, train/total_loss/avg: 0.2761, max mem: 10793.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 669ms, time_since_start: 50m 21s 053ms, eta: 03h 08m 01s 057ms\n","\u001b[32m2022-04-20T03:34:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0311, train/hateful_memes/cross_entropy/avg: 0.2700, train/total_loss: 0.0311, train/total_loss/avg: 0.2700, max mem: 10793.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 615ms, time_since_start: 51m 22s 669ms, eta: 03h 03m 48s 719ms\n","\u001b[32m2022-04-20T03:35:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0311, train/hateful_memes/cross_entropy/avg: 0.2659, train/total_loss: 0.0311, train/total_loss/avg: 0.2659, max mem: 10793.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 730ms, time_since_start: 52m 24s 400ms, eta: 03h 03m 06s 546ms\n","\u001b[32m2022-04-20T03:36:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0299, train/hateful_memes/cross_entropy/avg: 0.2602, train/total_loss: 0.0299, train/total_loss/avg: 0.2602, max mem: 10793.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 519ms, time_since_start: 53m 26s 919ms, eta: 03h 04m 23s 341ms\n","\u001b[32m2022-04-20T03:37:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0281, train/hateful_memes/cross_entropy/avg: 0.2552, train/total_loss: 0.0281, train/total_loss/avg: 0.2552, max mem: 10793.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 828ms, time_since_start: 54m 28s 748ms, eta: 03h 01m 18s 167ms\n","\u001b[32m2022-04-20T03:38:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0244, train/hateful_memes/cross_entropy/avg: 0.2499, train/total_loss: 0.0244, train/total_loss/avg: 0.2499, max mem: 10793.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 180ms, time_since_start: 55m 30s 928ms, eta: 03h 01m 16s 841ms\n","\u001b[32m2022-04-20T03:39:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0244, train/hateful_memes/cross_entropy/avg: 0.2453, train/total_loss: 0.0244, train/total_loss/avg: 0.2453, max mem: 10793.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 838ms, time_since_start: 56m 32s 766ms, eta: 02h 59m 14s 071ms\n","\u001b[32m2022-04-20T03:40:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T03:40:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T03:40:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T03:40:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T03:40:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0199, train/hateful_memes/cross_entropy/avg: 0.2405, train/total_loss: 0.0199, train/total_loss/avg: 0.2405, max mem: 10793.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 467ms, time_since_start: 58m 245ms, eta: 04h 12m 04s 187ms\n","\u001b[32m2022-04-20T03:40:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T03:40:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T03:40:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T03:40:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T03:41:07 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T03:41:07 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T03:41:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T03:41:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T03:41:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T03:41:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 2.0251, val/total_loss: 2.0251, val/hateful_memes/accuracy: 0.6704, val/hateful_memes/binary_f1: 0.4258, val/hateful_memes/roc_auc: 0.6981, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 36s 381ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T03:42:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0177, train/hateful_memes/cross_entropy/avg: 0.2359, train/total_loss: 0.0177, train/total_loss/avg: 0.2359, max mem: 10793.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 1.41, time: 01m 11s 515ms, time_since_start: 59m 48s 144ms, eta: 03h 24m 51s 669ms\n","\u001b[32m2022-04-20T03:43:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0177, train/hateful_memes/cross_entropy/avg: 0.2334, train/total_loss: 0.0177, train/total_loss/avg: 0.2334, max mem: 10793.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 725ms, time_since_start: 01h 51s 870ms, eta: 03h 01m 27s 875ms\n","\u001b[32m2022-04-20T03:44:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0177, train/hateful_memes/cross_entropy/avg: 0.2300, train/total_loss: 0.0177, train/total_loss/avg: 0.2300, max mem: 10793.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 929ms, time_since_start: 01h 01m 53s 799ms, eta: 02h 55m 18s 045ms\n","\u001b[32m2022-04-20T03:45:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0177, train/hateful_memes/cross_entropy/avg: 0.2261, train/total_loss: 0.0177, train/total_loss/avg: 0.2261, max mem: 10793.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 586ms, time_since_start: 01h 02m 56s 386ms, eta: 02h 56m 05s 951ms\n","\u001b[32m2022-04-20T03:46:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0199, train/hateful_memes/cross_entropy/avg: 0.2237, train/total_loss: 0.0199, train/total_loss/avg: 0.2237, max mem: 10793.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 054ms, time_since_start: 01h 03m 58s 441ms, eta: 02h 53m 33s 123ms\n","\u001b[32m2022-04-20T03:48:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0157, train/hateful_memes/cross_entropy/avg: 0.2197, train/total_loss: 0.0157, train/total_loss/avg: 0.2197, max mem: 10793.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 999ms, time_since_start: 01h 05m 01s 440ms, eta: 02h 55m 07s 580ms\n","\u001b[32m2022-04-20T03:49:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0157, train/hateful_memes/cross_entropy/avg: 0.2159, train/total_loss: 0.0157, train/total_loss/avg: 0.2159, max mem: 10793.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 080ms, time_since_start: 01h 06m 03s 521ms, eta: 02h 51m 31s 209ms\n","\u001b[32m2022-04-20T03:50:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0199, train/hateful_memes/cross_entropy/avg: 0.2132, train/total_loss: 0.0199, train/total_loss/avg: 0.2132, max mem: 10793.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 066ms, time_since_start: 01h 07m 05s 587ms, eta: 02h 50m 25s 677ms\n","\u001b[32m2022-04-20T03:51:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2096, train/total_loss: 0.0120, train/total_loss/avg: 0.2096, max mem: 10793.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 744ms, time_since_start: 01h 08m 08s 332ms, eta: 02h 51m 13s 580ms\n","\u001b[32m2022-04-20T03:52:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T03:52:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T03:52:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T03:52:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T03:52:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2061, train/total_loss: 0.0120, train/total_loss/avg: 0.2061, max mem: 10793.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 556ms, time_since_start: 01h 09m 35s 889ms, eta: 03h 57m 27s 219ms\n","\u001b[32m2022-04-20T03:52:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T03:52:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T03:52:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T03:52:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T03:52:43 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T03:52:43 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T03:52:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T03:52:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T03:53:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T03:53:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.0382, val/total_loss: 2.0382, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4290, val/hateful_memes/roc_auc: 0.6900, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 36s 580ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T03:54:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2030, train/total_loss: 0.0120, train/total_loss/avg: 0.2030, max mem: 10793.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 1.41, time: 01m 11s 431ms, time_since_start: 01h 11m 23s 918ms, eta: 03h 12m 30s 649ms\n","\u001b[32m2022-04-20T03:55:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1997, train/total_loss: 0.0073, train/total_loss/avg: 0.1997, max mem: 10793.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 1.56, time: 01m 04s 875ms, time_since_start: 01h 12m 28s 794ms, eta: 02h 53m 44s 658ms\n","\u001b[32m2022-04-20T03:56:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1966, train/total_loss: 0.0065, train/total_loss/avg: 0.1966, max mem: 10793.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 276ms, time_since_start: 01h 13m 31s 070ms, eta: 02h 45m 43s 553ms\n","\u001b[32m2022-04-20T03:57:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1942, train/total_loss: 0.0065, train/total_loss/avg: 0.1942, max mem: 10793.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 327ms, time_since_start: 01h 14m 33s 397ms, eta: 02h 44m 48s 408ms\n","\u001b[32m2022-04-20T03:58:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1916, train/total_loss: 0.0065, train/total_loss/avg: 0.1916, max mem: 10793.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 983ms, time_since_start: 01h 15m 35s 381ms, eta: 02h 42m 50s 796ms\n","\u001b[32m2022-04-20T03:59:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0055, train/hateful_memes/cross_entropy/avg: 0.1888, train/total_loss: 0.0055, train/total_loss/avg: 0.1888, max mem: 10793.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 207ms, time_since_start: 01h 16m 37s 589ms, eta: 02h 42m 22s 853ms\n","\u001b[32m2022-04-20T04:00:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0055, train/hateful_memes/cross_entropy/avg: 0.1860, train/total_loss: 0.0055, train/total_loss/avg: 0.1860, max mem: 10793.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 875ms, time_since_start: 01h 17m 40s 464ms, eta: 02h 43m 03s 430ms\n","\u001b[32m2022-04-20T04:01:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0061, train/hateful_memes/cross_entropy/avg: 0.1851, train/total_loss: 0.0061, train/total_loss/avg: 0.1851, max mem: 10793.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 336ms, time_since_start: 01h 18m 42s 800ms, eta: 02h 40m 36s 178ms\n","\u001b[32m2022-04-20T04:02:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0055, train/hateful_memes/cross_entropy/avg: 0.1824, train/total_loss: 0.0055, train/total_loss/avg: 0.1824, max mem: 10793.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 196ms, time_since_start: 01h 19m 44s 997ms, eta: 02h 39m 11s 340ms\n","\u001b[32m2022-04-20T04:03:46 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T04:03:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T04:03:59 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T04:04:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T04:04:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0061, train/hateful_memes/cross_entropy/avg: 0.1799, train/total_loss: 0.0061, train/total_loss/avg: 0.1799, max mem: 10793.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 872ms, time_since_start: 01h 21m 13s 870ms, eta: 03h 45m 57s 545ms\n","\u001b[32m2022-04-20T04:04:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T04:04:12 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T04:04:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T04:04:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T04:04:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T04:04:20 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T04:04:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.0912, val/total_loss: 2.0912, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.5058, val/hateful_memes/roc_auc: 0.6774, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 08s 111ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T04:05:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0061, train/hateful_memes/cross_entropy/avg: 0.1774, train/total_loss: 0.0061, train/total_loss/avg: 0.1774, max mem: 10793.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 263ms, time_since_start: 01h 22m 32s 251ms, eta: 02h 57m 27s 295ms\n","\u001b[32m2022-04-20T04:06:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0049, train/hateful_memes/cross_entropy/avg: 0.1750, train/total_loss: 0.0049, train/total_loss/avg: 0.1750, max mem: 10793.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 1.56, time: 01m 04s 941ms, time_since_start: 01h 23m 37s 193ms, eta: 02h 42m 54s 710ms\n","\u001b[32m2022-04-20T04:07:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1726, train/total_loss: 0.0032, train/total_loss/avg: 0.1726, max mem: 10793.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 085ms, time_since_start: 01h 24m 39s 279ms, eta: 02h 34m 41s 787ms\n","\u001b[32m2022-04-20T04:08:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1710, train/total_loss: 0.0032, train/total_loss/avg: 0.1710, max mem: 10793.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 979ms, time_since_start: 01h 25m 41s 259ms, eta: 02h 33m 22s 877ms\n","\u001b[32m2022-04-20T04:09:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1688, train/total_loss: 0.0030, train/total_loss/avg: 0.1688, max mem: 10793.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 635ms, time_since_start: 01h 26m 43s 894ms, eta: 02h 33m 56s 607ms\n","\u001b[32m2022-04-20T04:10:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1665, train/total_loss: 0.0030, train/total_loss/avg: 0.1665, max mem: 10793.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 050ms, time_since_start: 01h 27m 45s 945ms, eta: 02h 31m 27s 235ms\n","\u001b[32m2022-04-20T04:11:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1644, train/total_loss: 0.0030, train/total_loss/avg: 0.1644, max mem: 10793.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 334ms, time_since_start: 01h 28m 48s 280ms, eta: 02h 31m 05s 317ms\n","\u001b[32m2022-04-20T04:12:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1623, train/total_loss: 0.0024, train/total_loss/avg: 0.1623, max mem: 10793.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 995ms, time_since_start: 01h 29m 51s 275ms, eta: 02h 31m 37s 445ms\n","\u001b[32m2022-04-20T04:13:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1603, train/total_loss: 0.0028, train/total_loss/avg: 0.1603, max mem: 10793.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 007ms, time_since_start: 01h 30m 53s 282ms, eta: 02h 28m 11s 649ms\n","\u001b[32m2022-04-20T04:14:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T04:14:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T04:15:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T04:15:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T04:15:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1584, train/total_loss: 0.0028, train/total_loss/avg: 0.1584, max mem: 10793.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 809ms, time_since_start: 01h 32m 21s 092ms, eta: 03h 28m 22s 343ms\n","\u001b[32m2022-04-20T04:15:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T04:15:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T04:15:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T04:15:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T04:15:28 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T04:15:28 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T04:15:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.1018, val/total_loss: 2.1018, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.4571, val/hateful_memes/roc_auc: 0.6990, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 08s 757ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T04:16:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1567, train/total_loss: 0.0028, train/total_loss/avg: 0.1567, max mem: 10793.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 993ms, time_since_start: 01h 33m 40s 853ms, eta: 02h 47m 15s 881ms\n","\u001b[32m2022-04-20T04:17:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1550, train/total_loss: 0.0030, train/total_loss/avg: 0.1550, max mem: 10793.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 553ms, time_since_start: 01h 34m 44s 407ms, eta: 02h 28m 39s 478ms\n","\u001b[32m2022-04-20T04:18:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0045, train/hateful_memes/cross_entropy/avg: 0.1531, train/total_loss: 0.0045, train/total_loss/avg: 0.1531, max mem: 10793.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 050ms, time_since_start: 01h 35m 47s 457ms, eta: 02h 26m 24s 702ms\n","\u001b[32m2022-04-20T04:19:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1513, train/total_loss: 0.0030, train/total_loss/avg: 0.1513, max mem: 10793.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 247ms, time_since_start: 01h 36m 49s 704ms, eta: 02h 23m 29s 560ms\n","\u001b[32m2022-04-20T04:20:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1496, train/total_loss: 0.0028, train/total_loss/avg: 0.1496, max mem: 10793.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 378ms, time_since_start: 01h 37m 52s 082ms, eta: 02h 22m 44s 191ms\n","\u001b[32m2022-04-20T04:21:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1479, train/total_loss: 0.0028, train/total_loss/avg: 0.1479, max mem: 10793.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 767ms, time_since_start: 01h 38m 54s 850ms, eta: 02h 22m 33s 810ms\n","\u001b[32m2022-04-20T04:22:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1462, train/total_loss: 0.0028, train/total_loss/avg: 0.1462, max mem: 10793.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 118ms, time_since_start: 01h 39m 56s 968ms, eta: 02h 20m 02s 215ms\n","\u001b[32m2022-04-20T04:23:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1456, train/total_loss: 0.0028, train/total_loss/avg: 0.1456, max mem: 10793.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 225ms, time_since_start: 01h 41m 194ms, eta: 02h 21m 27s 701ms\n","\u001b[32m2022-04-20T04:25:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1440, train/total_loss: 0.0030, train/total_loss/avg: 0.1440, max mem: 10793.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 329ms, time_since_start: 01h 42m 02s 524ms, eta: 02h 18m 24s 024ms\n","\u001b[32m2022-04-20T04:26:03 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T04:26:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T04:26:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T04:26:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T04:26:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1425, train/total_loss: 0.0030, train/total_loss/avg: 0.1425, max mem: 10793.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 741ms, time_since_start: 01h 43m 30s 266ms, eta: 03h 13m 20s 363ms\n","\u001b[32m2022-04-20T04:26:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T04:26:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T04:26:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T04:26:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T04:26:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T04:26:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T04:26:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.2796, val/total_loss: 2.2796, val/hateful_memes/accuracy: 0.6667, val/hateful_memes/binary_f1: 0.4643, val/hateful_memes/roc_auc: 0.6814, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 22000, val_time: 08s 763ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T04:27:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1409, train/total_loss: 0.0028, train/total_loss/avg: 0.1409, max mem: 10793.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 762ms, time_since_start: 01h 44m 51s 794ms, eta: 02h 39m 05s 929ms\n","\u001b[32m2022-04-20T04:28:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1394, train/total_loss: 0.0030, train/total_loss/avg: 0.1394, max mem: 10793.0, experiment: run, epoch: 35, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 817ms, time_since_start: 01h 45m 55s 612ms, eta: 02h 18m 27s 525ms\n","\u001b[32m2022-04-20T04:29:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1379, train/total_loss: 0.0030, train/total_loss/avg: 0.1379, max mem: 10793.0, experiment: run, epoch: 35, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 682ms, time_since_start: 01h 46m 58s 294ms, eta: 02h 14m 56s 010ms\n","\u001b[32m2022-04-20T04:31:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1366, train/total_loss: 0.0030, train/total_loss/avg: 0.1366, max mem: 10793.0, experiment: run, epoch: 36, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 984ms, time_since_start: 01h 48m 02s 279ms, eta: 02h 16m 39s 073ms\n","\u001b[32m2022-04-20T04:32:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1352, train/total_loss: 0.0030, train/total_loss/avg: 0.1352, max mem: 10793.0, experiment: run, epoch: 36, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 791ms, time_since_start: 01h 49m 05s 070ms, eta: 02h 13m 02s 326ms\n","\u001b[32m2022-04-20T04:33:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1347, train/total_loss: 0.0032, train/total_loss/avg: 0.1347, max mem: 10793.0, experiment: run, epoch: 37, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 369ms, time_since_start: 01h 50m 08s 440ms, eta: 02h 13m 11s 438ms\n","\u001b[32m2022-04-20T04:34:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1333, train/total_loss: 0.0032, train/total_loss/avg: 0.1333, max mem: 10793.0, experiment: run, epoch: 37, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 489ms, time_since_start: 01h 51m 10s 929ms, eta: 02h 10m 16s 877ms\n","\u001b[32m2022-04-20T04:35:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1320, train/total_loss: 0.0032, train/total_loss/avg: 0.1320, max mem: 10793.0, experiment: run, epoch: 37, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 546ms, time_since_start: 01h 52m 13s 476ms, eta: 02h 09m 20s 395ms\n","\u001b[32m2022-04-20T04:36:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1306, train/total_loss: 0.0032, train/total_loss/avg: 0.1306, max mem: 10793.0, experiment: run, epoch: 38, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 710ms, time_since_start: 01h 53m 17s 186ms, eta: 02h 10m 40s 052ms\n","\u001b[32m2022-04-20T04:37:19 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T04:37:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T04:37:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T04:37:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T04:37:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1293, train/total_loss: 0.0030, train/total_loss/avg: 0.1293, max mem: 10793.0, experiment: run, epoch: 38, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 105ms, time_since_start: 01h 54m 46s 292ms, eta: 03h 01m 14s 393ms\n","\u001b[32m2022-04-20T04:37:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T04:37:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T04:37:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T04:37:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T04:37:53 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T04:37:53 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T04:37:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.4778, val/total_loss: 2.4778, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4742, val/hateful_memes/roc_auc: 0.6885, num_updates: 10000, epoch: 38, iterations: 10000, max_updates: 22000, val_time: 08s 467ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T04:39:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1281, train/total_loss: 0.0017, train/total_loss/avg: 0.1281, max mem: 10793.0, experiment: run, epoch: 38, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 472ms, time_since_start: 01h 56m 05s 234ms, eta: 02h 22m 08s 794ms\n","\u001b[32m2022-04-20T04:40:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1268, train/total_loss: 0.0009, train/total_loss/avg: 0.1268, max mem: 10793.0, experiment: run, epoch: 39, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00001, ups: 1.56, time: 01m 04s 413ms, time_since_start: 01h 57m 09s 648ms, eta: 02h 08m 50s 059ms\n","\u001b[32m2022-04-20T04:41:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1256, train/total_loss: 0.0009, train/total_loss/avg: 0.1256, max mem: 10793.0, experiment: run, epoch: 39, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 118ms, time_since_start: 01h 58m 11s 767ms, eta: 02h 03m 11s 470ms\n","\u001b[32m2022-04-20T04:42:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1244, train/total_loss: 0.0009, train/total_loss/avg: 0.1244, max mem: 10793.0, experiment: run, epoch: 40, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 709ms, time_since_start: 01h 59m 14s 477ms, eta: 02h 03m 18s 006ms\n","\u001b[32m2022-04-20T04:43:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1232, train/total_loss: 0.0009, train/total_loss/avg: 0.1232, max mem: 10793.0, experiment: run, epoch: 40, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 955ms, time_since_start: 02h 16s 433ms, eta: 02h 46s 053ms\n","\u001b[32m2022-04-20T04:44:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1220, train/total_loss: 0.0008, train/total_loss/avg: 0.1220, max mem: 10793.0, experiment: run, epoch: 40, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 153ms, time_since_start: 02h 01m 18s 586ms, eta: 02h 05s 988ms\n","\u001b[32m2022-04-20T04:45:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1210, train/total_loss: 0.0008, train/total_loss/avg: 0.1210, max mem: 10793.0, experiment: run, epoch: 41, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 481ms, time_since_start: 02h 02m 21s 068ms, eta: 01h 59m 40s 422ms\n","\u001b[32m2022-04-20T04:46:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1199, train/total_loss: 0.0006, train/total_loss/avg: 0.1199, max mem: 10793.0, experiment: run, epoch: 41, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 838ms, time_since_start: 02h 03m 22s 906ms, eta: 01h 57m 23s 646ms\n","\u001b[32m2022-04-20T04:47:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1192, train/total_loss: 0.0006, train/total_loss/avg: 0.1192, max mem: 10793.0, experiment: run, epoch: 41, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 874ms, time_since_start: 02h 04m 24s 781ms, eta: 01h 56m 24s 815ms\n","\u001b[32m2022-04-20T04:48:26 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T04:48:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T04:48:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T04:48:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T04:48:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1182, train/total_loss: 0.0005, train/total_loss/avg: 0.1182, max mem: 10793.0, experiment: run, epoch: 42, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 204ms, time_since_start: 02h 05m 52s 985ms, eta: 02h 44m 27s 391ms\n","\u001b[32m2022-04-20T04:48:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T04:48:52 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T04:48:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T04:48:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T04:49:00 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T04:49:00 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T04:49:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.4699, val/total_loss: 2.4699, val/hateful_memes/accuracy: 0.6574, val/hateful_memes/binary_f1: 0.4127, val/hateful_memes/roc_auc: 0.6840, num_updates: 11000, epoch: 42, iterations: 11000, max_updates: 22000, val_time: 08s 722ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T04:50:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1172, train/total_loss: 0.0006, train/total_loss/avg: 0.1172, max mem: 10793.0, experiment: run, epoch: 42, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 864ms, time_since_start: 02h 07m 12s 574ms, eta: 02h 10m 55s 598ms\n","\u001b[32m2022-04-20T04:51:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1161, train/total_loss: 0.0006, train/total_loss/avg: 0.1161, max mem: 10793.0, experiment: run, epoch: 43, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 366ms, time_since_start: 02h 08m 15s 941ms, eta: 01h 55m 59s 959ms\n","\u001b[32m2022-04-20T04:52:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1151, train/total_loss: 0.0006, train/total_loss/avg: 0.1151, max mem: 10793.0, experiment: run, epoch: 43, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 759ms, time_since_start: 02h 09m 17s 700ms, eta: 01h 52m 605ms\n","\u001b[32m2022-04-20T04:53:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1141, train/total_loss: 0.0006, train/total_loss/avg: 0.1141, max mem: 10793.0, experiment: run, epoch: 43, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 792ms, time_since_start: 02h 10m 19s 493ms, eta: 01h 51m 01s 334ms\n","\u001b[32m2022-04-20T04:54:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1131, train/total_loss: 0.0005, train/total_loss/avg: 0.1131, max mem: 10793.0, experiment: run, epoch: 44, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 265ms, time_since_start: 02h 11m 21s 759ms, eta: 01h 50m 49s 070ms\n","\u001b[32m2022-04-20T04:55:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1122, train/total_loss: 0.0004, train/total_loss/avg: 0.1122, max mem: 10793.0, experiment: run, epoch: 44, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 930ms, time_since_start: 02h 12m 23s 689ms, eta: 01h 49m 10s 237ms\n","\u001b[32m2022-04-20T04:56:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1113, train/total_loss: 0.0005, train/total_loss/avg: 0.1113, max mem: 10793.0, experiment: run, epoch: 44, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 650ms, time_since_start: 02h 13m 25s 339ms, eta: 01h 47m 37s 916ms\n","\u001b[32m2022-04-20T04:57:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1104, train/total_loss: 0.0005, train/total_loss/avg: 0.1104, max mem: 10793.0, experiment: run, epoch: 45, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 343ms, time_since_start: 02h 14m 27s 682ms, eta: 01h 47m 47s 109ms\n","\u001b[32m2022-04-20T04:58:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1095, train/total_loss: 0.0005, train/total_loss/avg: 0.1095, max mem: 10793.0, experiment: run, epoch: 45, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 870ms, time_since_start: 02h 15m 29s 553ms, eta: 01h 45m 55s 131ms\n","\u001b[32m2022-04-20T04:59:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T04:59:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T04:59:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T04:59:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T04:59:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1086, train/total_loss: 0.0005, train/total_loss/avg: 0.1086, max mem: 10793.0, experiment: run, epoch: 46, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 877ms, time_since_start: 02h 16m 57s 430ms, eta: 02h 28m 57s 141ms\n","\u001b[32m2022-04-20T04:59:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T04:59:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T04:59:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T04:59:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T05:00:06 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T05:00:06 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T05:00:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.5960, val/total_loss: 2.5960, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.4669, val/hateful_memes/roc_auc: 0.6837, num_updates: 12000, epoch: 46, iterations: 12000, max_updates: 22000, val_time: 10s 141ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T05:01:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1077, train/total_loss: 0.0005, train/total_loss/avg: 0.1077, max mem: 10793.0, experiment: run, epoch: 46, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0., ups: 1.47, time: 01m 08s 850ms, time_since_start: 02h 18m 16s 424ms, eta: 01h 55m 32s 104ms\n","\u001b[32m2022-04-20T05:02:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1069, train/total_loss: 0.0006, train/total_loss/avg: 0.1069, max mem: 10793.0, experiment: run, epoch: 46, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0., ups: 1.59, time: 01m 03s 889ms, time_since_start: 02h 19m 20s 314ms, eta: 01h 46m 07s 566ms\n","\u001b[32m2022-04-20T05:03:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1060, train/total_loss: 0.0006, train/total_loss/avg: 0.1060, max mem: 10793.0, experiment: run, epoch: 47, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 008ms, time_since_start: 02h 20m 22s 322ms, eta: 01h 41m 57s 104ms\n","\u001b[32m2022-04-20T05:04:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1054, train/total_loss: 0.0006, train/total_loss/avg: 0.1054, max mem: 10793.0, experiment: run, epoch: 47, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 754ms, time_since_start: 02h 21m 24s 077ms, eta: 01h 40m 29s 221ms\n","\u001b[32m2022-04-20T05:05:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1048, train/total_loss: 0.0006, train/total_loss/avg: 0.1048, max mem: 10793.0, experiment: run, epoch: 47, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 509ms, time_since_start: 02h 22m 25s 587ms, eta: 01h 39m 02s 773ms\n","\u001b[32m2022-04-20T05:06:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1040, train/total_loss: 0.0006, train/total_loss/avg: 0.1040, max mem: 10793.0, experiment: run, epoch: 48, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 753ms, time_since_start: 02h 23m 28s 340ms, eta: 01h 39m 59s 104ms\n","\u001b[32m2022-04-20T05:07:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1033, train/total_loss: 0.0006, train/total_loss/avg: 0.1033, max mem: 10793.0, experiment: run, epoch: 48, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 803ms, time_since_start: 02h 24m 30s 144ms, eta: 01h 37m 25s 481ms\n","\u001b[32m2022-04-20T05:08:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1025, train/total_loss: 0.0006, train/total_loss/avg: 0.1025, max mem: 10793.0, experiment: run, epoch: 49, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 071ms, time_since_start: 02h 25m 32s 216ms, eta: 01h 36m 47s 699ms\n","\u001b[32m2022-04-20T05:09:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1018, train/total_loss: 0.0006, train/total_loss/avg: 0.1018, max mem: 10793.0, experiment: run, epoch: 49, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 954ms, time_since_start: 02h 26m 34s 171ms, eta: 01h 35m 33s 706ms\n","\u001b[32m2022-04-20T05:10:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T05:10:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T05:10:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T05:11:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T05:11:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1010, train/total_loss: 0.0006, train/total_loss/avg: 0.1010, max mem: 10793.0, experiment: run, epoch: 49, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0., ups: 1.15, time: 01m 27s 786ms, time_since_start: 02h 28m 01s 957ms, eta: 02h 13m 55s 101ms\n","\u001b[32m2022-04-20T05:11:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T05:11:00 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T05:11:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T05:11:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T05:11:07 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T05:11:07 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T05:11:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.7384, val/total_loss: 2.7384, val/hateful_memes/accuracy: 0.6722, val/hateful_memes/binary_f1: 0.4197, val/hateful_memes/roc_auc: 0.6749, num_updates: 13000, epoch: 49, iterations: 13000, max_updates: 22000, val_time: 07s 006ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T05:12:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1002, train/total_loss: 0.0006, train/total_loss/avg: 0.1002, max mem: 10793.0, experiment: run, epoch: 50, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 071ms, time_since_start: 02h 29m 18s 039ms, eta: 01h 44m 11s 899ms\n","\u001b[32m2022-04-20T05:13:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0995, train/total_loss: 0.0005, train/total_loss/avg: 0.0995, max mem: 10793.0, experiment: run, epoch: 50, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0., ups: 1.54, time: 01m 05s 372ms, time_since_start: 02h 30m 23s 411ms, eta: 01h 37m 30s 552ms\n","\u001b[32m2022-04-20T05:14:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0990, train/total_loss: 0.0005, train/total_loss/avg: 0.0990, max mem: 10793.0, experiment: run, epoch: 50, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 222ms, time_since_start: 02h 31m 24s 634ms, eta: 01h 30m 16s 934ms\n","\u001b[32m2022-04-20T05:15:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0983, train/total_loss: 0.0004, train/total_loss/avg: 0.0983, max mem: 10793.0, experiment: run, epoch: 51, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0., ups: 1.59, time: 01m 03s 090ms, time_since_start: 02h 32m 27s 725ms, eta: 01h 31m 58s 048ms\n","\u001b[32m2022-04-20T05:16:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0975, train/total_loss: 0.0004, train/total_loss/avg: 0.0975, max mem: 10793.0, experiment: run, epoch: 51, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 095ms, time_since_start: 02h 33m 29s 820ms, eta: 01h 29m 27s 829ms\n","\u001b[32m2022-04-20T05:17:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0968, train/total_loss: 0.0004, train/total_loss/avg: 0.0968, max mem: 10793.0, experiment: run, epoch: 52, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 420ms, time_since_start: 02h 34m 32s 241ms, eta: 01h 28m 52s 466ms\n","\u001b[32m2022-04-20T05:18:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0961, train/total_loss: 0.0003, train/total_loss/avg: 0.0961, max mem: 10793.0, experiment: run, epoch: 52, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 842ms, time_since_start: 02h 35m 34s 084ms, eta: 01h 27m 188ms\n","\u001b[32m2022-04-20T05:19:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0955, train/total_loss: 0.0003, train/total_loss/avg: 0.0955, max mem: 10793.0, experiment: run, epoch: 52, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 935ms, time_since_start: 02h 36m 36s 019ms, eta: 01h 26m 05s 043ms\n","\u001b[32m2022-04-20T05:20:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0948, train/total_loss: 0.0003, train/total_loss/avg: 0.0948, max mem: 10793.0, experiment: run, epoch: 53, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 265ms, time_since_start: 02h 37m 38s 285ms, eta: 01h 25m 29s 247ms\n","\u001b[32m2022-04-20T05:21:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T05:21:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T05:21:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T05:22:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T05:22:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0941, train/total_loss: 0.0003, train/total_loss/avg: 0.0941, max mem: 10793.0, experiment: run, epoch: 53, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0., ups: 1.15, time: 01m 27s 843ms, time_since_start: 02h 39m 06s 128ms, eta: 01h 59m 06s 962ms\n","\u001b[32m2022-04-20T05:22:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T05:22:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T05:22:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T05:22:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T05:22:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T05:22:15 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T05:22:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 3.0149, val/total_loss: 3.0149, val/hateful_memes/accuracy: 0.6556, val/hateful_memes/binary_f1: 0.3961, val/hateful_memes/roc_auc: 0.6851, num_updates: 14000, epoch: 53, iterations: 14000, max_updates: 22000, val_time: 11s 114ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T05:23:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0934, train/total_loss: 0.0003, train/total_loss/avg: 0.0934, max mem: 10793.0, experiment: run, epoch: 54, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0., ups: 1.39, time: 01m 12s 796ms, time_since_start: 02h 40m 30s 047ms, eta: 01h 37m 28s 685ms\n","\u001b[32m2022-04-20T05:24:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0928, train/total_loss: 0.0003, train/total_loss/avg: 0.0928, max mem: 10793.0, experiment: run, epoch: 54, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0., ups: 1.59, time: 01m 03s 097ms, time_since_start: 02h 41m 33s 145ms, eta: 01h 23m 25s 253ms\n","\u001b[32m2022-04-20T05:25:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0921, train/total_loss: 0.0003, train/total_loss/avg: 0.0921, max mem: 10793.0, experiment: run, epoch: 54, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 806ms, time_since_start: 02h 42m 34s 951ms, eta: 01h 20m 40s 028ms\n","\u001b[32m2022-04-20T05:26:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0915, train/total_loss: 0.0003, train/total_loss/avg: 0.0915, max mem: 10793.0, experiment: run, epoch: 55, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 151ms, time_since_start: 02h 43m 37s 103ms, eta: 01h 20m 03s 842ms\n","\u001b[32m2022-04-20T05:27:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0911, train/total_loss: 0.0003, train/total_loss/avg: 0.0911, max mem: 10793.0, experiment: run, epoch: 55, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 797ms, time_since_start: 02h 44m 38s 900ms, eta: 01h 18m 33s 576ms\n","\u001b[32m2022-04-20T05:28:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0905, train/total_loss: 0.0003, train/total_loss/avg: 0.0905, max mem: 10793.0, experiment: run, epoch: 55, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 962ms, time_since_start: 02h 45m 40s 863ms, eta: 01h 17m 43s 175ms\n","\u001b[32m2022-04-20T05:29:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0899, train/total_loss: 0.0001, train/total_loss/avg: 0.0899, max mem: 10793.0, experiment: run, epoch: 56, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 907ms, time_since_start: 02h 46m 43s 771ms, eta: 01h 17m 50s 336ms\n","\u001b[32m2022-04-20T05:30:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0893, train/total_loss: 0.0001, train/total_loss/avg: 0.0893, max mem: 10793.0, experiment: run, epoch: 56, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 908ms, time_since_start: 02h 47m 45s 679ms, eta: 01h 15m 33s 181ms\n","\u001b[32m2022-04-20T05:31:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0887, train/total_loss: 0.0001, train/total_loss/avg: 0.0887, max mem: 10793.0, experiment: run, epoch: 57, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 498ms, time_since_start: 02h 48m 48s 178ms, eta: 01h 15m 12s 836ms\n","\u001b[32m2022-04-20T05:32:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T05:32:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T05:33:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T05:33:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T05:33:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0881, train/total_loss: 0.0001, train/total_loss/avg: 0.0881, max mem: 10793.0, experiment: run, epoch: 57, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0., ups: 1.15, time: 01m 27s 355ms, time_since_start: 02h 50m 15s 544ms, eta: 01h 43m 39s 633ms\n","\u001b[32m2022-04-20T05:33:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T05:33:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T05:33:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T05:33:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T05:33:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T05:33:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T05:33:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.6133, val/total_loss: 2.6133, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4940, val/hateful_memes/roc_auc: 0.6918, num_updates: 15000, epoch: 57, iterations: 15000, max_updates: 22000, val_time: 09s 760ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T05:34:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0875, train/total_loss: 0.0001, train/total_loss/avg: 0.0875, max mem: 10793.0, experiment: run, epoch: 57, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0., ups: 1.43, time: 01m 10s 515ms, time_since_start: 02h 51m 35s 849ms, eta: 01h 22m 28s 292ms\n","\u001b[32m2022-04-20T05:35:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0869, train/total_loss: 0.0001, train/total_loss/avg: 0.0869, max mem: 10793.0, experiment: run, epoch: 58, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0., ups: 1.54, time: 01m 05s 577ms, time_since_start: 02h 52m 41s 427ms, eta: 01h 15m 35s 105ms\n","\u001b[32m2022-04-20T05:36:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0864, train/total_loss: 0.0001, train/total_loss/avg: 0.0864, max mem: 10793.0, experiment: run, epoch: 58, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 946ms, time_since_start: 02h 53m 43s 373ms, eta: 01h 10m 20s 959ms\n","\u001b[32m2022-04-20T05:37:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0858, train/total_loss: 0.0001, train/total_loss/avg: 0.0858, max mem: 10793.0, experiment: run, epoch: 58, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 610ms, time_since_start: 02h 54m 44s 984ms, eta: 01h 08m 55s 433ms\n","\u001b[32m2022-04-20T05:38:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0852, train/total_loss: 0.0001, train/total_loss/avg: 0.0852, max mem: 10793.0, experiment: run, epoch: 59, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 590ms, time_since_start: 02h 55m 47s 578ms, eta: 01h 08m 57s 816ms\n","\u001b[32m2022-04-20T05:39:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0847, train/total_loss: 0.0001, train/total_loss/avg: 0.0847, max mem: 10793.0, experiment: run, epoch: 59, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 020ms, time_since_start: 02h 56m 49s 599ms, eta: 01h 07m 16s 798ms\n","\u001b[32m2022-04-20T05:40:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0842, train/total_loss: 0.0001, train/total_loss/avg: 0.0842, max mem: 10793.0, experiment: run, epoch: 60, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 658ms, time_since_start: 02h 57m 52s 257ms, eta: 01h 06m 54s 577ms\n","\u001b[32m2022-04-20T05:41:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0837, train/total_loss: 0.0001, train/total_loss/avg: 0.0837, max mem: 10793.0, experiment: run, epoch: 60, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 054ms, time_since_start: 02h 58m 54s 316ms, eta: 01h 05m 13s 026ms\n","\u001b[32m2022-04-20T05:42:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0831, train/total_loss: 0.0001, train/total_loss/avg: 0.0831, max mem: 10793.0, experiment: run, epoch: 60, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 885ms, time_since_start: 02h 59m 56s 201ms, eta: 01h 03m 59s 206ms\n","\u001b[32m2022-04-20T05:43:57 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T05:43:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T05:44:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T05:44:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T05:44:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0826, train/total_loss: 0.0001, train/total_loss/avg: 0.0826, max mem: 10793.0, experiment: run, epoch: 61, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 181ms, time_since_start: 03h 01m 24s 383ms, eta: 01h 29m 40s 856ms\n","\u001b[32m2022-04-20T05:44:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T05:44:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T05:44:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T05:44:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T05:44:33 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T05:44:33 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T05:44:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 2.6963, val/total_loss: 2.6963, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4373, val/hateful_memes/roc_auc: 0.6959, num_updates: 16000, epoch: 61, iterations: 16000, max_updates: 22000, val_time: 10s 633ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T05:45:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0821, train/total_loss: 0.0001, train/total_loss/avg: 0.0821, max mem: 10793.0, experiment: run, epoch: 61, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0., ups: 1.39, time: 01m 12s 673ms, time_since_start: 03h 02m 47s 695ms, eta: 01h 12m 40s 653ms\n","\u001b[32m2022-04-20T05:46:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0816, train/total_loss: 0.0001, train/total_loss/avg: 0.0816, max mem: 10793.0, experiment: run, epoch: 61, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0., ups: 1.59, time: 01m 03s 504ms, time_since_start: 03h 03m 51s 200ms, eta: 01h 02m 25s 896ms\n","\u001b[32m2022-04-20T05:47:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0811, train/total_loss: 0.0001, train/total_loss/avg: 0.0811, max mem: 10793.0, experiment: run, epoch: 62, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0., ups: 1.59, time: 01m 03s 590ms, time_since_start: 03h 04m 54s 790ms, eta: 01h 01m 26s 297ms\n","\u001b[32m2022-04-20T05:48:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0806, train/total_loss: 0.0001, train/total_loss/avg: 0.0806, max mem: 10793.0, experiment: run, epoch: 62, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 477ms, time_since_start: 03h 05m 57s 268ms, eta: 59m 18s 234ms\n","\u001b[32m2022-04-20T05:49:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0801, train/total_loss: 0.0001, train/total_loss/avg: 0.0801, max mem: 10793.0, experiment: run, epoch: 63, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 704ms, time_since_start: 03h 06m 59s 973ms, eta: 58m 27s 402ms\n","\u001b[32m2022-04-20T05:51:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0796, train/total_loss: 0.0001, train/total_loss/avg: 0.0796, max mem: 10793.0, experiment: run, epoch: 63, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 656ms, time_since_start: 03h 08m 02s 630ms, eta: 57m 20s 988ms\n","\u001b[32m2022-04-20T05:52:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0791, train/total_loss: 0.0001, train/total_loss/avg: 0.0791, max mem: 10793.0, experiment: run, epoch: 63, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 281ms, time_since_start: 03h 09m 04s 912ms, eta: 55m 57s 032ms\n","\u001b[32m2022-04-20T05:53:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0787, train/total_loss: 0.0001, train/total_loss/avg: 0.0787, max mem: 10793.0, experiment: run, epoch: 64, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0., ups: 1.59, time: 01m 03s 361ms, time_since_start: 03h 10m 08s 273ms, eta: 55m 50s 794ms\n","\u001b[32m2022-04-20T05:54:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0782, train/total_loss: 0.0001, train/total_loss/avg: 0.0782, max mem: 10793.0, experiment: run, epoch: 64, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 839ms, time_since_start: 03h 11m 11s 112ms, eta: 54m 19s 289ms\n","\u001b[32m2022-04-20T05:55:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T05:55:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T05:55:24 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T05:55:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T05:55:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0777, train/total_loss: 0.0001, train/total_loss/avg: 0.0777, max mem: 10793.0, experiment: run, epoch: 64, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0., ups: 1.15, time: 01m 27s 783ms, time_since_start: 03h 12m 38s 896ms, eta: 01h 14m 23s 791ms\n","\u001b[32m2022-04-20T05:55:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T05:55:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T05:55:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T05:55:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T05:55:45 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T05:55:45 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T05:55:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 2.9484, val/total_loss: 2.9484, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4252, val/hateful_memes/roc_auc: 0.6911, num_updates: 17000, epoch: 64, iterations: 17000, max_updates: 22000, val_time: 08s 284ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T05:56:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0773, train/total_loss: 0.0001, train/total_loss/avg: 0.0773, max mem: 10793.0, experiment: run, epoch: 65, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 827ms, time_since_start: 03h 13m 57s 017ms, eta: 57m 59s 700ms\n","\u001b[32m2022-04-20T05:58:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0768, train/total_loss: 0.0001, train/total_loss/avg: 0.0768, max mem: 10793.0, experiment: run, epoch: 65, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0., ups: 1.54, time: 01m 05s 673ms, time_since_start: 03h 15m 02s 691ms, eta: 53m 25s 919ms\n","\u001b[32m2022-04-20T05:59:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0764, train/total_loss: 0.0001, train/total_loss/avg: 0.0764, max mem: 10793.0, experiment: run, epoch: 66, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 884ms, time_since_start: 03h 16m 05s 575ms, eta: 50m 05s 809ms\n","\u001b[32m2022-04-20T06:00:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0760, train/total_loss: 0.0001, train/total_loss/avg: 0.0760, max mem: 10793.0, experiment: run, epoch: 66, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 076ms, time_since_start: 03h 17m 07s 652ms, eta: 48m 24s 067ms\n","\u001b[32m2022-04-20T06:01:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0755, train/total_loss: 0.0001, train/total_loss/avg: 0.0755, max mem: 10793.0, experiment: run, epoch: 66, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 874ms, time_since_start: 03h 18m 09s 526ms, eta: 47m 11s 689ms\n","\u001b[32m2022-04-20T06:02:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0751, train/total_loss: 0.0001, train/total_loss/avg: 0.0751, max mem: 10793.0, experiment: run, epoch: 67, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 543ms, time_since_start: 03h 19m 12s 070ms, eta: 46m 38s 696ms\n","\u001b[32m2022-04-20T06:03:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0747, train/total_loss: 0.0001, train/total_loss/avg: 0.0747, max mem: 10793.0, experiment: run, epoch: 67, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 829ms, time_since_start: 03h 20m 13s 899ms, eta: 45m 03s 854ms\n","\u001b[32m2022-04-20T06:04:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0743, train/total_loss: 0.0001, train/total_loss/avg: 0.0743, max mem: 10793.0, experiment: run, epoch: 67, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 910ms, time_since_start: 03h 21m 15s 810ms, eta: 44m 04s 461ms\n","\u001b[32m2022-04-20T06:05:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0739, train/total_loss: 0.0001, train/total_loss/avg: 0.0739, max mem: 10793.0, experiment: run, epoch: 68, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 490ms, time_since_start: 03h 22m 18s 300ms, eta: 43m 25s 651ms\n","\u001b[32m2022-04-20T06:06:19 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T06:06:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T06:06:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T06:06:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T06:06:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0734, train/total_loss: 0.0001, train/total_loss/avg: 0.0734, max mem: 10793.0, experiment: run, epoch: 68, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0., ups: 1.15, time: 01m 27s 483ms, time_since_start: 03h 23m 45s 784ms, eta: 59m 18s 849ms\n","\u001b[32m2022-04-20T06:06:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T06:06:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:06:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:06:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T06:06:52 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T06:06:52 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T06:06:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 2.8156, val/total_loss: 2.8156, val/hateful_memes/accuracy: 0.6704, val/hateful_memes/binary_f1: 0.4145, val/hateful_memes/roc_auc: 0.6915, num_updates: 18000, epoch: 68, iterations: 18000, max_updates: 22000, val_time: 08s 600ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T06:08:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0730, train/total_loss: 0.0001, train/total_loss/avg: 0.0730, max mem: 10793.0, experiment: run, epoch: 69, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 971ms, time_since_start: 03h 25m 04s 359ms, eta: 46m 15s 282ms\n","\u001b[32m2022-04-20T06:09:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0726, train/total_loss: 0.0001, train/total_loss/avg: 0.0726, max mem: 10793.0, experiment: run, epoch: 69, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0., ups: 1.52, time: 01m 06s 773ms, time_since_start: 03h 26m 11s 132ms, eta: 43m 517ms\n","\u001b[32m2022-04-20T06:10:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0722, train/total_loss: 0.0001, train/total_loss/avg: 0.0722, max mem: 10793.0, experiment: run, epoch: 69, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 841ms, time_since_start: 03h 27m 12s 973ms, eta: 38m 47s 025ms\n","\u001b[32m2022-04-20T06:11:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0718, train/total_loss: 0.0001, train/total_loss/avg: 0.0718, max mem: 10793.0, experiment: run, epoch: 70, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 944ms, time_since_start: 03h 28m 15s 918ms, eta: 38m 24s 529ms\n","\u001b[32m2022-04-20T06:12:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0715, train/total_loss: 0.0001, train/total_loss/avg: 0.0715, max mem: 10793.0, experiment: run, epoch: 70, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 463ms, time_since_start: 03h 29m 18s 385ms, eta: 37m 03s 509ms\n","\u001b[32m2022-04-20T06:13:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0711, train/total_loss: 0.0001, train/total_loss/avg: 0.0711, max mem: 10793.0, experiment: run, epoch: 70, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 888ms, time_since_start: 03h 30m 20s 274ms, eta: 35m 39s 994ms\n","\u001b[32m2022-04-20T06:14:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0707, train/total_loss: 0.0001, train/total_loss/avg: 0.0707, max mem: 10793.0, experiment: run, epoch: 71, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 851ms, time_since_start: 03h 31m 23s 128ms, eta: 35m 09s 467ms\n","\u001b[32m2022-04-20T06:15:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0703, train/total_loss: 0.0001, train/total_loss/avg: 0.0703, max mem: 10793.0, experiment: run, epoch: 71, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 952ms, time_since_start: 03h 32m 25s 081ms, eta: 33m 36s 186ms\n","\u001b[32m2022-04-20T06:16:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0699, train/total_loss: 0.0000, train/total_loss/avg: 0.0699, max mem: 10793.0, experiment: run, epoch: 72, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0., ups: 1.59, time: 01m 03s 123ms, time_since_start: 03h 33m 28s 204ms, eta: 33m 10s 086ms\n","\u001b[32m2022-04-20T06:17:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T06:17:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T06:17:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T06:17:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T06:17:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0696, train/total_loss: 0.0000, train/total_loss/avg: 0.0696, max mem: 10793.0, experiment: run, epoch: 72, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0., ups: 1.15, time: 01m 27s 216ms, time_since_start: 03h 34m 55s 421ms, eta: 44m 20s 990ms\n","\u001b[32m2022-04-20T06:17:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T06:17:54 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:17:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:17:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T06:18:02 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T06:18:02 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T06:18:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.0393, val/total_loss: 3.0393, val/hateful_memes/accuracy: 0.6667, val/hateful_memes/binary_f1: 0.4231, val/hateful_memes/roc_auc: 0.6935, num_updates: 19000, epoch: 72, iterations: 19000, max_updates: 22000, val_time: 08s 759ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T06:19:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0692, train/total_loss: 0.0000, train/total_loss/avg: 0.0692, max mem: 10793.0, experiment: run, epoch: 72, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0., ups: 1.43, time: 01m 10s 653ms, time_since_start: 03h 36m 14s 850ms, eta: 34m 43s 790ms\n","\u001b[32m2022-04-20T06:20:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0688, train/total_loss: 0.0000, train/total_loss/avg: 0.0688, max mem: 10793.0, experiment: run, epoch: 73, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0., ups: 1.56, time: 01m 04s 531ms, time_since_start: 03h 37m 19s 382ms, eta: 30m 37s 605ms\n","\u001b[32m2022-04-20T06:21:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0685, train/total_loss: 0.0000, train/total_loss/avg: 0.0685, max mem: 10793.0, experiment: run, epoch: 73, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 016ms, time_since_start: 03h 38m 21s 399ms, eta: 28m 22s 922ms\n","\u001b[32m2022-04-20T06:22:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0681, train/total_loss: 0.0000, train/total_loss/avg: 0.0681, max mem: 10793.0, experiment: run, epoch: 73, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 821ms, time_since_start: 03h 39m 23s 221ms, eta: 27m 14s 687ms\n","\u001b[32m2022-04-20T06:23:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0678, train/total_loss: 0.0000, train/total_loss/avg: 0.0678, max mem: 10793.0, experiment: run, epoch: 74, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0., ups: 1.59, time: 01m 03s 035ms, time_since_start: 03h 40m 26s 257ms, eta: 26m 42s 688ms\n","\u001b[32m2022-04-20T06:24:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0674, train/total_loss: 0.0000, train/total_loss/avg: 0.0674, max mem: 10793.0, experiment: run, epoch: 74, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 952ms, time_since_start: 03h 41m 28s 209ms, eta: 25m 12s 130ms\n","\u001b[32m2022-04-20T06:25:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0671, train/total_loss: 0.0000, train/total_loss/avg: 0.0671, max mem: 10793.0, experiment: run, epoch: 75, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 314ms, time_since_start: 03h 42m 30s 523ms, eta: 24m 17s 588ms\n","\u001b[32m2022-04-20T06:26:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0668, train/total_loss: 0.0000, train/total_loss/avg: 0.0668, max mem: 10793.0, experiment: run, epoch: 75, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 947ms, time_since_start: 03h 43m 32s 471ms, eta: 23m 06s 015ms\n","\u001b[32m2022-04-20T06:27:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0664, train/total_loss: 0.0000, train/total_loss/avg: 0.0664, max mem: 10793.0, experiment: run, epoch: 75, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 992ms, time_since_start: 03h 44m 34s 464ms, eta: 22m 03s 984ms\n","\u001b[32m2022-04-20T06:28:36 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T06:28:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T06:28:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T06:29:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T06:29:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0661, train/total_loss: 0.0000, train/total_loss/avg: 0.0661, max mem: 10793.0, experiment: run, epoch: 76, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 543ms, time_since_start: 03h 46m 04s 007ms, eta: 30m 21s 311ms\n","\u001b[32m2022-04-20T06:29:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T06:29:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:29:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:29:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T06:29:11 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T06:29:11 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T06:29:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 3.0417, val/total_loss: 3.0417, val/hateful_memes/accuracy: 0.6611, val/hateful_memes/binary_f1: 0.4153, val/hateful_memes/roc_auc: 0.6921, num_updates: 20000, epoch: 76, iterations: 20000, max_updates: 22000, val_time: 08s 514ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T06:30:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0658, train/total_loss: 0.0000, train/total_loss/avg: 0.0658, max mem: 10793.0, experiment: run, epoch: 76, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.43, time: 01m 10s 526ms, time_since_start: 03h 47m 23s 050ms, eta: 22m 42s 779ms\n","\u001b[32m2022-04-20T06:31:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0654, train/total_loss: 0.0000, train/total_loss/avg: 0.0654, max mem: 10793.0, experiment: run, epoch: 76, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.56, time: 01m 04s 445ms, time_since_start: 03h 48m 27s 496ms, eta: 19m 39s 748ms\n","\u001b[32m2022-04-20T06:32:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0651, train/total_loss: 0.0000, train/total_loss/avg: 0.0651, max mem: 10793.0, experiment: run, epoch: 77, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 741ms, time_since_start: 03h 49m 30s 237ms, eta: 18m 04s 730ms\n","\u001b[32m2022-04-20T06:33:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0648, train/total_loss: 0.0000, train/total_loss/avg: 0.0648, max mem: 10793.0, experiment: run, epoch: 77, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 020ms, time_since_start: 03h 50m 32s 258ms, eta: 16m 49s 199ms\n","\u001b[32m2022-04-20T06:34:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0645, train/total_loss: 0.0000, train/total_loss/avg: 0.0645, max mem: 10793.0, experiment: run, epoch: 78, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 961ms, time_since_start: 03h 51m 35s 220ms, eta: 16m 483ms\n","\u001b[32m2022-04-20T06:35:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0642, train/total_loss: 0.0000, train/total_loss/avg: 0.0642, max mem: 10793.0, experiment: run, epoch: 78, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 956ms, time_since_start: 03h 52m 37s 177ms, eta: 14m 42s 143ms\n","\u001b[32m2022-04-20T06:36:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0639, train/total_loss: 0.0000, train/total_loss/avg: 0.0639, max mem: 10793.0, experiment: run, epoch: 78, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 971ms, time_since_start: 03h 53m 39s 148ms, eta: 13m 39s 320ms\n","\u001b[32m2022-04-20T06:37:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0635, train/total_loss: 0.0000, train/total_loss/avg: 0.0635, max mem: 10793.0, experiment: run, epoch: 79, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 419ms, time_since_start: 03h 54m 41s 568ms, eta: 12m 41s 770ms\n","\u001b[32m2022-04-20T06:38:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0632, train/total_loss: 0.0000, train/total_loss/avg: 0.0632, max mem: 10793.0, experiment: run, epoch: 79, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 781ms, time_since_start: 03h 55m 43s 350ms, eta: 11m 31s 154ms\n","\u001b[32m2022-04-20T06:39:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T06:39:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T06:39:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T06:40:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T06:40:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0629, train/total_loss: 0.0000, train/total_loss/avg: 0.0629, max mem: 10793.0, experiment: run, epoch: 79, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 1.15, time: 01m 27s 654ms, time_since_start: 03h 57m 11s 005ms, eta: 14m 51s 448ms\n","\u001b[32m2022-04-20T06:40:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T06:40:10 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:40:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:40:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T06:40:18 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T06:40:18 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T06:40:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 2.9483, val/total_loss: 2.9483, val/hateful_memes/accuracy: 0.6667, val/hateful_memes/binary_f1: 0.4479, val/hateful_memes/roc_auc: 0.6986, num_updates: 21000, epoch: 79, iterations: 21000, max_updates: 22000, val_time: 08s 895ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T06:41:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0626, train/total_loss: 0.0000, train/total_loss/avg: 0.0626, max mem: 10793.0, experiment: run, epoch: 80, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 204ms, time_since_start: 03h 58m 29s 139ms, eta: 10m 33s 431ms\n","\u001b[32m2022-04-20T06:42:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0623, train/total_loss: 0.0000, train/total_loss/avg: 0.0623, max mem: 10793.0, experiment: run, epoch: 80, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.56, time: 01m 04s 345ms, time_since_start: 03h 59m 33s 485ms, eta: 08m 43s 517ms\n","\u001b[32m2022-04-20T06:43:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0621, train/total_loss: 0.0000, train/total_loss/avg: 0.0621, max mem: 10793.0, experiment: run, epoch: 81, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 228ms, time_since_start: 04h 35s 714ms, eta: 07m 23s 005ms\n","\u001b[32m2022-04-20T06:44:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0618, train/total_loss: 0.0000, train/total_loss/avg: 0.0618, max mem: 10793.0, experiment: run, epoch: 81, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 866ms, time_since_start: 04h 01m 37s 580ms, eta: 06m 17s 507ms\n","\u001b[32m2022-04-20T06:45:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0615, train/total_loss: 0.0000, train/total_loss/avg: 0.0615, max mem: 10793.0, experiment: run, epoch: 81, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 610ms, time_since_start: 04h 02m 39s 190ms, eta: 05m 13s 287ms\n","\u001b[32m2022-04-20T06:46:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0612, train/total_loss: 0.0000, train/total_loss/avg: 0.0612, max mem: 10793.0, experiment: run, epoch: 82, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 376ms, time_since_start: 04h 03m 41s 567ms, eta: 04m 13s 749ms\n","\u001b[32m2022-04-20T06:47:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0609, train/total_loss: 0.0000, train/total_loss/avg: 0.0609, max mem: 10793.0, experiment: run, epoch: 82, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 772ms, time_since_start: 04h 04m 43s 340ms, eta: 03m 08s 468ms\n","\u001b[32m2022-04-20T06:48:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0606, train/total_loss: 0.0000, train/total_loss/avg: 0.0606, max mem: 10793.0, experiment: run, epoch: 82, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 796ms, time_since_start: 04h 05m 45s 136ms, eta: 02m 05s 694ms\n","\u001b[32m2022-04-20T06:49:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0604, train/total_loss: 0.0000, train/total_loss/avg: 0.0604, max mem: 10793.0, experiment: run, epoch: 83, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 117ms, time_since_start: 04h 06m 47s 254ms, eta: 01m 03s 173ms\n","\u001b[32m2022-04-20T06:50:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T06:50:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T06:51:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T06:51:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T06:51:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0601, train/total_loss: 0.0000, train/total_loss/avg: 0.0601, max mem: 10793.0, experiment: run, epoch: 83, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 241ms, time_since_start: 04h 08m 15s 495ms, eta: 0ms\n","\u001b[32m2022-04-20T06:51:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T06:51:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:51:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:51:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T06:51:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T06:51:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T06:51:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/hateful_memes/cross_entropy: 2.9632, val/total_loss: 2.9632, val/hateful_memes/accuracy: 0.6611, val/hateful_memes/binary_f1: 0.4299, val/hateful_memes/roc_auc: 0.6959, num_updates: 22000, epoch: 83, iterations: 22000, max_updates: 22000, val_time: 09s 051ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702544\n","\u001b[32m2022-04-20T06:51:24 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2022-04-20T06:51:24 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-04-20T06:51:24 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-04-20T06:52:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-20T06:52:10 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n","\u001b[32m2022-04-20T06:52:10 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n","\u001b[32m2022-04-20T06:52:10 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 8\n","\u001b[32m2022-04-20T06:52:15 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-04-20T06:52:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:52:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:52:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","100% 63/63 [00:30<00:00,  2.08it/s]\n","\u001b[32m2022-04-20T06:52:45 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 63\n","\u001b[32m2022-04-20T06:52:45 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T06:52:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, test/hateful_memes/cross_entropy: 0.7706, test/total_loss: 0.7706, test/hateful_memes/accuracy: 0.6895, test/hateful_memes/binary_f1: 0.5278, test/hateful_memes/roc_auc: 0.7166\n","\u001b[32m2022-04-20T06:52:46 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 04h 09m 47s 209ms\n"]}],"source":["!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml model=vilbert dataset=hateful_memes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Vpb05LA7OQjI","outputId":"f2748567-5e14-4eb1-bbeb-e0dcf840da69"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZQ3lmwL85WfL","outputId":"b1db68a9-a500-4fed-b789-9a9a3bef11a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-20T06:53:08 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\n","\u001b[32m2022-04-20T06:53:08 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2022-04-20T06:53:08 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-20T06:53:08 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n","\u001b[32m2022-04-20T06:53:08 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-04-20T06:53:08 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-20T06:53:08 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-20T06:53:08 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False'])\n","\u001b[32m2022-04-20T06:53:08 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-20T06:53:08 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-04-20T06:53:08 | mmf_cli.run: \u001b[0mUsing seed 8889394\n","\u001b[32m2022-04-20T06:53:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:53:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:53:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T06:53:12 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T06:53:12 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T06:53:12 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T06:53:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bi_attention_type\": 1,\n","  \"bi_hidden_size\": 1024,\n","  \"bi_intermediate_size\": 1024,\n","  \"bi_num_attention_heads\": 8,\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"cut_first\": \"text\",\n","  \"dynamic_attention\": false,\n","  \"embedding_strategy\": \"plain\",\n","  \"fast_mode\": false,\n","  \"finetune_lr_multiplier\": 1,\n","  \"fixed_t_layer\": 0,\n","  \"fixed_v_layer\": 0,\n","  \"freeze_base\": false,\n","  \"fusion_method\": \"mul\",\n","  \"gradient_checkpointing\": false,\n","  \"hard_cap_seq_len\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"in_batch_pairs\": false,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"vilbert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_negative\": 128,\n","  \"objective\": 0,\n","  \"pad_token_id\": 0,\n","  \"pooling_method\": \"mul\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"t_biattention_id\": [\n","    6,\n","    7,\n","    8,\n","    9,\n","    10,\n","    11\n","  ],\n","  \"task_specific_tokens\": false,\n","  \"text_only\": false,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"v_attention_probs_dropout_prob\": 0.1,\n","  \"v_biattention_id\": [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","  ],\n","  \"v_feature_size\": 2048,\n","  \"v_hidden_act\": \"gelu\",\n","  \"v_hidden_dropout_prob\": 0.1,\n","  \"v_hidden_size\": 1024,\n","  \"v_initializer_range\": 0.02,\n","  \"v_intermediate_size\": 1024,\n","  \"v_num_attention_heads\": 8,\n","  \"v_num_hidden_layers\": 6,\n","  \"v_target_size\": 1601,\n","  \"visual_embedding_dim\": 2048,\n","  \"visual_target\": 0,\n","  \"visualization\": false,\n","  \"vocab_size\": 30522,\n","  \"with_coattention\": true\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.v_pooler.dense.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.v_pooler.dense.weight', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.t_pooler.dense.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.v_layer.5.output.LayerNorm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-20T06:53:24 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-20T06:53:24 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-20T06:53:25 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:53:40 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:53:40 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-20T06:53:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-20T06:53:40 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n","\u001b[32m2022-04-20T06:53:40 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n","\u001b[32m2022-04-20T06:53:40 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 8\n","\u001b[32m2022-04-20T06:53:40 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-20T06:53:40 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n","  (model): ViLBERTForClassification(\n","    (bert): ViLBERTBase(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (v_embeddings): BertImageFeatureEmbeddings(\n","        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n","        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (v_layer): ModuleList(\n","          (0): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (c_layer): ModuleList(\n","          (0): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (t_pooler): BertTextPooler(\n","        (dense): Linear(in_features=768, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","      (v_pooler): BertImagePooler(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=1024, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-20T06:53:41 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n","\u001b[32m2022-04-20T06:53:41 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2022-04-20T06:53:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 17/17 [00:06<00:00,  2.58it/s]\n","\u001b[32m2022-04-20T06:53:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T06:53:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T06:53:48 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 0.7473, val/total_loss: 0.7473, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.5167, val/hateful_memes/roc_auc: 0.7025\n","\u001b[32m2022-04-20T06:53:48 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 22s 711ms\n"]}],"source":["!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\" \\\n","model=vilbert \\\n","dataset=hateful_memes \\\n","run_type=val \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114978,"status":"ok","timestamp":1650905022453,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"kopZ1ckriisP","outputId":"7798ffbe-c522-49cb-dabf-f4ad27f1ce97"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-25T16:41:53 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\n","\u001b[32m2022-04-25T16:41:53 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2022-04-25T16:41:53 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-25T16:41:53 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n","\u001b[32m2022-04-25T16:41:53 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save_vilbert_default/best.ckpt\n","\u001b[32m2022-04-25T16:41:53 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-25T16:41:53 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-25T16:41:53 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save_vilbert_default/best.ckpt', 'checkpoint.resume_pretrained=False'])\n","\u001b[32m2022-04-25T16:41:53 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-25T16:41:53 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n","\u001b[32m2022-04-25T16:41:53 | mmf_cli.run: \u001b[0mUsing seed 53326153\n","\u001b[32m2022-04-25T16:41:53 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-25T16:42:01 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T16:42:01 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T16:42:01 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T16:42:01 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bi_attention_type\": 1,\n","  \"bi_hidden_size\": 1024,\n","  \"bi_intermediate_size\": 1024,\n","  \"bi_num_attention_heads\": 8,\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"cut_first\": \"text\",\n","  \"dynamic_attention\": false,\n","  \"embedding_strategy\": \"plain\",\n","  \"fast_mode\": false,\n","  \"finetune_lr_multiplier\": 1,\n","  \"fixed_t_layer\": 0,\n","  \"fixed_v_layer\": 0,\n","  \"freeze_base\": false,\n","  \"fusion_method\": \"mul\",\n","  \"gradient_checkpointing\": false,\n","  \"hard_cap_seq_len\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"in_batch_pairs\": false,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"vilbert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_negative\": 128,\n","  \"objective\": 0,\n","  \"pad_token_id\": 0,\n","  \"pooling_method\": \"mul\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"t_biattention_id\": [\n","    6,\n","    7,\n","    8,\n","    9,\n","    10,\n","    11\n","  ],\n","  \"task_specific_tokens\": false,\n","  \"text_only\": false,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"v_attention_probs_dropout_prob\": 0.1,\n","  \"v_biattention_id\": [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","  ],\n","  \"v_feature_size\": 2048,\n","  \"v_hidden_act\": \"gelu\",\n","  \"v_hidden_dropout_prob\": 0.1,\n","  \"v_hidden_size\": 1024,\n","  \"v_initializer_range\": 0.02,\n","  \"v_intermediate_size\": 1024,\n","  \"v_num_attention_heads\": 8,\n","  \"v_num_hidden_layers\": 6,\n","  \"v_target_size\": 1601,\n","  \"visual_embedding_dim\": 2048,\n","  \"visual_target\": 0,\n","  \"visualization\": false,\n","  \"vocab_size\": 30522,\n","  \"with_coattention\": true\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.t_pooler.dense.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.v_pooler.dense.bias', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.t_pooler.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.v_pooler.dense.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-25T16:42:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-25T16:42:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-25T16:42:08 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-25T16:43:07 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-25T16:43:07 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-25T16:43:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-25T16:43:07 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n","\u001b[32m2022-04-25T16:43:07 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n","\u001b[32m2022-04-25T16:43:07 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 8\n","\u001b[32m2022-04-25T16:43:07 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-25T16:43:07 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n","  (model): ViLBERTForClassification(\n","    (bert): ViLBERTBase(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (v_embeddings): BertImageFeatureEmbeddings(\n","        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n","        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (v_layer): ModuleList(\n","          (0): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (c_layer): ModuleList(\n","          (0): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (t_pooler): BertTextPooler(\n","        (dense): Linear(in_features=768, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","      (v_pooler): BertImagePooler(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=1024, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-25T16:43:07 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n","\u001b[32m2022-04-25T16:43:07 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-04-25T16:43:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:32<00:00,  1.93it/s]\n","\u001b[32m2022-04-25T16:43:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 63\n","\u001b[32m2022-04-25T16:43:40 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T16:43:40 | mmf.trainers.callbacks.logistics: \u001b[0mtest/hateful_memes/cross_entropy: 0.7706, test/total_loss: 0.7706, test/hateful_memes/accuracy: 0.6895, test/hateful_memes/binary_f1: 0.5278, test/hateful_memes/roc_auc: 0.7166\n","\u001b[32m2022-04-25T16:43:40 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01m 32s 669ms\n"]}],"source":["!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\" \\\n","model=vilbert \\\n","dataset=hateful_memes \\\n","run_type=test \\\n","checkpoint.resume_file=save_vilbert_default/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pde3jQE06IkW","outputId":"c5cf1fed-408a-46ca-dd0e-994270dd033d"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2okQRfxiyWg2","outputId":"47aed2a4-a1a2-4584-fcf2-4060dc5e4801"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-20T06:53:56 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\n","\u001b[32m2022-04-20T06:53:56 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2022-04-20T06:53:56 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-20T06:53:56 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n","\u001b[32m2022-04-20T06:53:56 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-04-20T06:53:56 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","\u001b[32m2022-04-20T06:53:56 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-20T06:53:56 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-20T06:53:56 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n","\u001b[32m2022-04-20T06:53:56 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-20T06:53:56 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-04-20T06:53:56 | mmf_cli.run: \u001b[0mUsing seed 57016427\n","\u001b[32m2022-04-20T06:53:57 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:54:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:54:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T06:54:00 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T06:54:00 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T06:54:00 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T06:54:00 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bi_attention_type\": 1,\n","  \"bi_hidden_size\": 1024,\n","  \"bi_intermediate_size\": 1024,\n","  \"bi_num_attention_heads\": 8,\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"cut_first\": \"text\",\n","  \"dynamic_attention\": false,\n","  \"embedding_strategy\": \"plain\",\n","  \"fast_mode\": false,\n","  \"finetune_lr_multiplier\": 1,\n","  \"fixed_t_layer\": 0,\n","  \"fixed_v_layer\": 0,\n","  \"freeze_base\": false,\n","  \"fusion_method\": \"mul\",\n","  \"gradient_checkpointing\": false,\n","  \"hard_cap_seq_len\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"in_batch_pairs\": false,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"vilbert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_negative\": 128,\n","  \"objective\": 0,\n","  \"pad_token_id\": 0,\n","  \"pooling_method\": \"mul\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"t_biattention_id\": [\n","    6,\n","    7,\n","    8,\n","    9,\n","    10,\n","    11\n","  ],\n","  \"task_specific_tokens\": false,\n","  \"text_only\": false,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"v_attention_probs_dropout_prob\": 0.1,\n","  \"v_biattention_id\": [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","  ],\n","  \"v_feature_size\": 2048,\n","  \"v_hidden_act\": \"gelu\",\n","  \"v_hidden_dropout_prob\": 0.1,\n","  \"v_hidden_size\": 1024,\n","  \"v_initializer_range\": 0.02,\n","  \"v_intermediate_size\": 1024,\n","  \"v_num_attention_heads\": 8,\n","  \"v_num_hidden_layers\": 6,\n","  \"v_target_size\": 1601,\n","  \"visual_embedding_dim\": 2048,\n","  \"visual_target\": 0,\n","  \"visualization\": false,\n","  \"vocab_size\": 30522,\n","  \"with_coattention\": true\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.v_pooler.dense.bias', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.t_pooler.dense.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.v_pooler.dense.weight', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-20T06:54:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-20T06:54:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-20T06:54:08 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:54:23 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:54:23 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-20T06:54:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-20T06:54:23 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n","\u001b[32m2022-04-20T06:54:23 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n","\u001b[32m2022-04-20T06:54:23 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 8\n","\u001b[32m2022-04-20T06:54:23 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n","\u001b[32m2022-04-20T06:54:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:22<00:00,  2.84it/s]\n","\u001b[32m2022-04-20T06:54:46 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes/save/hateful_memes_vilbert_57016427/reports/hateful_memes_run_test_2022-04-20T06:54:45.csv\n","\u001b[32m2022-04-20T06:54:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 63\n","\u001b[32m2022-04-20T06:54:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"]}],"source":["!mmf_predict config=\"mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\" \\\n","model=vilbert \\\n","dataset=hateful_memes \\\n","run_type=test \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QrGKwIy7qSye","outputId":"1f5403d9-679c-45c8-92f9-a9287dbcb61a"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WYcrwAfSDbHe","outputId":"966993c3-c787-4c5d-90d5-6666920c1d6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-20T06:54:55 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\n","\u001b[32m2022-04-20T06:54:55 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2022-04-20T06:54:55 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-20T06:54:55 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n","\u001b[32m2022-04-20T06:54:55 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-04-20T06:54:55 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","\u001b[32m2022-04-20T06:54:55 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-20T06:54:55 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-20T06:54:55 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n","\u001b[32m2022-04-20T06:54:55 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-20T06:54:55 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-04-20T06:54:55 | mmf_cli.run: \u001b[0mUsing seed 55470448\n","\u001b[32m2022-04-20T06:54:55 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:54:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:54:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\u001b[32m2022-04-20T06:54:59 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T06:54:59 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T06:54:59 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T06:54:59 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bi_attention_type\": 1,\n","  \"bi_hidden_size\": 1024,\n","  \"bi_intermediate_size\": 1024,\n","  \"bi_num_attention_heads\": 8,\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"cut_first\": \"text\",\n","  \"dynamic_attention\": false,\n","  \"embedding_strategy\": \"plain\",\n","  \"fast_mode\": false,\n","  \"finetune_lr_multiplier\": 1,\n","  \"fixed_t_layer\": 0,\n","  \"fixed_v_layer\": 0,\n","  \"freeze_base\": false,\n","  \"fusion_method\": \"mul\",\n","  \"gradient_checkpointing\": false,\n","  \"hard_cap_seq_len\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"in_batch_pairs\": false,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"vilbert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_negative\": 128,\n","  \"objective\": 0,\n","  \"pad_token_id\": 0,\n","  \"pooling_method\": \"mul\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"t_biattention_id\": [\n","    6,\n","    7,\n","    8,\n","    9,\n","    10,\n","    11\n","  ],\n","  \"task_specific_tokens\": false,\n","  \"text_only\": false,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"v_attention_probs_dropout_prob\": 0.1,\n","  \"v_biattention_id\": [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","  ],\n","  \"v_feature_size\": 2048,\n","  \"v_hidden_act\": \"gelu\",\n","  \"v_hidden_dropout_prob\": 0.1,\n","  \"v_hidden_size\": 1024,\n","  \"v_initializer_range\": 0.02,\n","  \"v_intermediate_size\": 1024,\n","  \"v_num_attention_heads\": 8,\n","  \"v_num_hidden_layers\": 6,\n","  \"v_target_size\": 1601,\n","  \"visual_embedding_dim\": 2048,\n","  \"visual_target\": 0,\n","  \"visualization\": false,\n","  \"vocab_size\": 30522,\n","  \"with_coattention\": true\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.v_pooler.dense.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.t_pooler.dense.weight', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.v_pooler.dense.weight', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-20T06:55:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-20T06:55:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-20T06:55:07 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:55:20 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-20T06:55:20 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-20T06:55:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-20T06:55:21 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n","\u001b[32m2022-04-20T06:55:21 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n","\u001b[32m2022-04-20T06:55:21 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 8\n","\u001b[32m2022-04-20T06:55:21 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n","\u001b[32m2022-04-20T06:55:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 17/17 [00:06<00:00,  2.51it/s]\n","\u001b[32m2022-04-20T06:55:27 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes/save/hateful_memes_vilbert_55470448/reports/hateful_memes_run_val_2022-04-20T06:55:27.csv\n","\u001b[32m2022-04-20T06:55:27 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 17\n","\u001b[32m2022-04-20T06:55:27 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"]}],"source":["!mmf_predict config=\"mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\" \\\n","model=vilbert \\\n","dataset=hateful_memes \\\n","run_type=val \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pnQU7yMqEJun"},"outputs":[],"source":["# 1. roberta\n","# 2. data aug\n"]},{"cell_type":"markdown","metadata":{"id":"QnWs5l0XeFKk"},"source":["# **Vilbert CC**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3254537,"status":"ok","timestamp":1650573968874,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"Y7o1_nWAeMcd","outputId":"56c5a9e9-ed7a-4337-c398-209ce8d06f02"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-21T14:26:52 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml\n","\u001b[32m2022-04-21T14:26:52 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2022-04-21T14:26:52 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-21T14:26:52 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-21T14:26:52 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml', 'model=vilbert', 'dataset=hateful_memes'])\n","\u001b[32m2022-04-21T14:26:52 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-21T14:26:52 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-04-21T14:26:52 | mmf_cli.run: \u001b[0mUsing seed 52146428\n","\u001b[32m2022-04-21T14:26:52 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [05:55<00:00, 28.9MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:00<00:00, 425kB/s]\n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkt68y7hl\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 31.8kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfjwurioo\n","Downloading: 100% 570/570 [00:00<00:00, 578kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4xtgzx91\n","Downloading: 100% 232k/232k [00:00<00:00, 3.12MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp92sh2evi\n","Downloading: 100% 466k/466k [00:00<00:00, 4.71MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-21T14:35:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T14:35:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T14:35:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T14:35:32 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bi_attention_type\": 1,\n","  \"bi_hidden_size\": 1024,\n","  \"bi_intermediate_size\": 1024,\n","  \"bi_num_attention_heads\": 8,\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"cut_first\": \"text\",\n","  \"dynamic_attention\": false,\n","  \"embedding_strategy\": \"plain\",\n","  \"fast_mode\": false,\n","  \"finetune_lr_multiplier\": 1,\n","  \"fixed_t_layer\": 0,\n","  \"fixed_v_layer\": 0,\n","  \"freeze_base\": false,\n","  \"fusion_method\": \"mul\",\n","  \"gradient_checkpointing\": false,\n","  \"hard_cap_seq_len\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"in_batch_pairs\": false,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"vilbert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_negative\": 128,\n","  \"objective\": 0,\n","  \"pad_token_id\": 0,\n","  \"pooling_method\": \"mul\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"t_biattention_id\": [\n","    6,\n","    7,\n","    8,\n","    9,\n","    10,\n","    11\n","  ],\n","  \"task_specific_tokens\": false,\n","  \"text_only\": false,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"v_attention_probs_dropout_prob\": 0.1,\n","  \"v_biattention_id\": [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","  ],\n","  \"v_feature_size\": 2048,\n","  \"v_hidden_act\": \"gelu\",\n","  \"v_hidden_dropout_prob\": 0.1,\n","  \"v_hidden_size\": 1024,\n","  \"v_initializer_range\": 0.02,\n","  \"v_intermediate_size\": 1024,\n","  \"v_num_attention_heads\": 8,\n","  \"v_num_hidden_layers\": 6,\n","  \"v_target_size\": 1601,\n","  \"visual_embedding_dim\": 2048,\n","  \"visual_target\": 0,\n","  \"visualization\": false,\n","  \"vocab_size\": 30522,\n","  \"with_coattention\": true\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmpi35z5qxc\n","Downloading: 100% 440M/440M [00:06<00:00, 71.3MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.v_pooler.dense.bias', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.t_pooler.dense.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.t_pooler.dense.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.v_pooler.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.c_layer.3.biattention.key2.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-21T14:35:46 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-21T14:35:46 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-21T14:35:46 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.pretrained.cc_original.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.pretrained.cc.original/vilbert.pretrained.cc_original.tar.gz ]\n","Downloading vilbert.pretrained.cc_original.tar.gz: 100% 918M/918M [00:19<00:00, 46.4MB/s]\n","[ Starting checksum for vilbert.pretrained.cc_original.tar.gz]\n","[ Checksum successful for vilbert.pretrained.cc_original.tar.gz]\n","Unpacking vilbert.pretrained.cc_original.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T14:36:17 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T14:36:17 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T14:36:17 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T14:36:17 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2022-04-21T14:36:18 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-21T14:36:18 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n","  (model): ViLBERTForClassification(\n","    (bert): ViLBERTBase(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (v_embeddings): BertImageFeatureEmbeddings(\n","        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n","        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (v_layer): ModuleList(\n","          (0): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (c_layer): ModuleList(\n","          (0): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (t_pooler): BertTextPooler(\n","        (dense): Linear(in_features=768, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","      (v_pooler): BertImagePooler(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=1024, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-21T14:36:18 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n","\u001b[32m2022-04-21T14:36:18 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-04-21T14:37:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.5797, train/hateful_memes/cross_entropy/avg: 0.5797, train/total_loss: 0.5797, train/total_loss/avg: 0.5797, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 628ms, time_since_start: 02m 08s 437ms, eta: 05h 58m 41s 480ms\n","\u001b[32m2022-04-21T14:39:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.5797, train/hateful_memes/cross_entropy/avg: 0.6082, train/total_loss: 0.5797, train/total_loss/avg: 0.6082, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 312ms, time_since_start: 03m 43s 749ms, eta: 05h 52m 11s 402ms\n","\u001b[32m2022-04-21T14:41:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6337, train/hateful_memes/cross_entropy/avg: 0.6167, train/total_loss: 0.6337, train/total_loss/avg: 0.6167, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 107ms, time_since_start: 05m 18s 857ms, eta: 05h 49m 49s 153ms\n","\u001b[32m2022-04-21T14:42:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.5797, train/hateful_memes/cross_entropy/avg: 0.6056, train/total_loss: 0.5797, train/total_loss/avg: 0.6056, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 291ms, time_since_start: 06m 54s 148ms, eta: 05h 48m 52s 822ms\n","\u001b[32m2022-04-21T14:44:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.5797, train/hateful_memes/cross_entropy/avg: 0.5959, train/total_loss: 0.5797, train/total_loss/avg: 0.5959, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 279ms, time_since_start: 08m 29s 427ms, eta: 05h 47m 13s 246ms\n","\u001b[32m2022-04-21T14:45:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.5797, train/hateful_memes/cross_entropy/avg: 0.5953, train/total_loss: 0.5797, train/total_loss/avg: 0.5953, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 752ms, time_since_start: 10m 04s 180ms, eta: 05h 43m 41s 710ms\n","\u001b[32m2022-04-21T14:47:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.5797, train/hateful_memes/cross_entropy/avg: 0.5871, train/total_loss: 0.5797, train/total_loss/avg: 0.5871, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 319ms, time_since_start: 11m 39s 499ms, eta: 05h 44m 08s 102ms\n","\u001b[32m2022-04-21T14:49:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.5797, train/hateful_memes/cross_entropy/avg: 0.5911, train/total_loss: 0.5797, train/total_loss/avg: 0.5911, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 948ms, time_since_start: 13m 14s 447ms, eta: 05h 41m 11s 220ms\n","\u001b[32m2022-04-21T14:50:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.5797, train/hateful_memes/cross_entropy/avg: 0.5658, train/total_loss: 0.5797, train/total_loss/avg: 0.5658, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 178ms, time_since_start: 14m 49s 626ms, eta: 05h 40m 24s 156ms\n","\u001b[32m2022-04-21T14:52:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T14:52:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T14:52:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T14:52:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T14:52:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.5724, train/hateful_memes/cross_entropy/avg: 0.5449, train/total_loss: 0.5724, train/total_loss/avg: 0.5449, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 545ms, time_since_start: 16m 45s 171ms, eta: 06h 51m 17s 008ms\n","\u001b[32m2022-04-21T14:52:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T14:52:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T14:52:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T14:52:40 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T14:52:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T14:52:51 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-21T14:53:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T14:53:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T14:53:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.7676, val/total_loss: 0.7676, val/hateful_memes/accuracy: 0.6407, val/hateful_memes/binary_f1: 0.3310, val/hateful_memes/roc_auc: 0.6465, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 43s 114ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.646471\n","\u001b[32m2022-04-21T14:54:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.5724, train/hateful_memes/cross_entropy/avg: 0.5183, train/total_loss: 0.5724, train/total_loss/avg: 0.5183, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 1.00, time: 01m 40s 044ms, time_since_start: 19m 08s 331ms, eta: 05h 54m 24s 680ms\n","\u001b[32m2022-04-21T14:56:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.5572, train/hateful_memes/cross_entropy/avg: 0.4998, train/total_loss: 0.5572, train/total_loss/avg: 0.4998, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 541ms, time_since_start: 20m 43s 873ms, eta: 05h 36m 50s 497ms\n","\u001b[32m2022-04-21T14:58:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.5572, train/hateful_memes/cross_entropy/avg: 0.4816, train/total_loss: 0.5572, train/total_loss/avg: 0.4816, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 395ms, time_since_start: 22m 19s 268ms, eta: 05h 34m 42s 510ms\n","\u001b[32m2022-04-21T14:59:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.5382, train/hateful_memes/cross_entropy/avg: 0.4642, train/total_loss: 0.5382, train/total_loss/avg: 0.4642, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 183ms, time_since_start: 23m 54s 452ms, eta: 05h 32m 21s 129ms\n","\u001b[32m2022-04-21T15:01:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.5382, train/hateful_memes/cross_entropy/avg: 0.4666, train/total_loss: 0.5382, train/total_loss/avg: 0.4666, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 433ms, time_since_start: 25m 29s 885ms, eta: 05h 31m 36s 506ms\n","\u001b[32m2022-04-21T15:02:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.5002, train/hateful_memes/cross_entropy/avg: 0.4530, train/total_loss: 0.5002, train/total_loss/avg: 0.4530, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 887ms, time_since_start: 27m 04s 773ms, eta: 05h 28m 06s 199ms\n","\u001b[32m2022-04-21T15:04:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.5002, train/hateful_memes/cross_entropy/avg: 0.4411, train/total_loss: 0.5002, train/total_loss/avg: 0.4411, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 518ms, time_since_start: 28m 40s 292ms, eta: 05h 28m 39s 916ms\n","\u001b[32m2022-04-21T15:06:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.3637, train/hateful_memes/cross_entropy/avg: 0.4224, train/total_loss: 0.3637, train/total_loss/avg: 0.4224, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 356ms, time_since_start: 30m 15s 648ms, eta: 05h 26m 29s 415ms\n","\u001b[32m2022-04-21T15:07:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.3637, train/hateful_memes/cross_entropy/avg: 0.4044, train/total_loss: 0.3637, train/total_loss/avg: 0.4044, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 987ms, time_since_start: 31m 50s 635ms, eta: 05h 23m 36s 985ms\n","\u001b[32m2022-04-21T15:09:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T15:09:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T15:09:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T15:09:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T15:09:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.3563, train/hateful_memes/cross_entropy/avg: 0.3884, train/total_loss: 0.3563, train/total_loss/avg: 0.3884, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 296ms, time_since_start: 33m 46s 932ms, eta: 06h 34m 14s 692ms\n","\u001b[32m2022-04-21T15:09:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T15:09:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T15:09:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T15:09:39 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T15:09:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T15:09:48 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-21T15:10:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T15:10:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T15:10:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 0.9934, val/total_loss: 0.9934, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.4971, val/hateful_memes/roc_auc: 0.6865, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 44s 084ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.686471\n","\u001b[32m2022-04-21T15:11:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.2961, train/hateful_memes/cross_entropy/avg: 0.3767, train/total_loss: 0.2961, train/total_loss/avg: 0.3767, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 748ms, time_since_start: 36m 07s 766ms, eta: 05h 26m 20s 169ms\n","\u001b[32m2022-04-21T15:13:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.2633, train/hateful_memes/cross_entropy/avg: 0.3611, train/total_loss: 0.2633, train/total_loss/avg: 0.3611, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 319ms, time_since_start: 37m 43s 085ms, eta: 05h 19m 54s 085ms\n","\u001b[32m2022-04-21T15:15:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.2529, train/hateful_memes/cross_entropy/avg: 0.3469, train/total_loss: 0.2529, train/total_loss/avg: 0.3469, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 489ms, time_since_start: 39m 18s 575ms, eta: 05h 18m 51s 197ms\n","\u001b[32m2022-04-21T15:16:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.2511, train/hateful_memes/cross_entropy/avg: 0.3343, train/total_loss: 0.2511, train/total_loss/avg: 0.3343, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 256ms, time_since_start: 40m 53s 831ms, eta: 05h 16m 27s 592ms\n","\u001b[32m2022-04-21T15:18:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.2490, train/hateful_memes/cross_entropy/avg: 0.3227, train/total_loss: 0.2490, train/total_loss/avg: 0.3227, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 573ms, time_since_start: 42m 29s 404ms, eta: 05h 15m 53s 635ms\n","\u001b[32m2022-04-21T15:19:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.2379, train/hateful_memes/cross_entropy/avg: 0.3113, train/total_loss: 0.2379, train/total_loss/avg: 0.3113, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 493ms, time_since_start: 44m 04s 898ms, eta: 05h 14m 733ms\n","\u001b[32m2022-04-21T15:21:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.1432, train/hateful_memes/cross_entropy/avg: 0.3019, train/total_loss: 0.1432, train/total_loss/avg: 0.3019, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 083ms, time_since_start: 45m 39s 981ms, eta: 05h 11m 03s 009ms\n","\u001b[32m2022-04-21T15:23:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.1043, train/hateful_memes/cross_entropy/avg: 0.2921, train/total_loss: 0.1043, train/total_loss/avg: 0.2921, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 545ms, time_since_start: 47m 15s 526ms, eta: 05h 10m 56s 507ms\n","\u001b[32m2022-04-21T15:24:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.0838, train/hateful_memes/cross_entropy/avg: 0.2828, train/total_loss: 0.0838, train/total_loss/avg: 0.2828, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 643ms, time_since_start: 48m 51s 170ms, eta: 05h 09m 38s 519ms\n","\u001b[32m2022-04-21T15:26:13 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T15:26:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T15:26:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T15:26:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T15:26:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.0811, train/hateful_memes/cross_entropy/avg: 0.2739, train/total_loss: 0.0811, train/total_loss/avg: 0.2739, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 0.85, time: 01m 58s 280ms, time_since_start: 50m 49s 451ms, eta: 06h 20m 55s 389ms\n","\u001b[32m2022-04-21T15:26:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T15:26:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T15:26:41 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T15:26:41 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T15:26:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T15:26:51 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-21T15:27:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T15:27:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T15:27:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.2964, val/total_loss: 1.2964, val/hateful_memes/accuracy: 0.6981, val/hateful_memes/binary_f1: 0.4890, val/hateful_memes/roc_auc: 0.7039, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 41s 944ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.703941\n","\u001b[32m2022-04-21T15:28:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.0558, train/hateful_memes/cross_entropy/avg: 0.2654, train/total_loss: 0.0558, train/total_loss/avg: 0.2654, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 648ms, time_since_start: 53m 08s 046ms, eta: 05h 09m 37s 137ms\n","\u001b[32m2022-04-21T15:30:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.0449, train/hateful_memes/cross_entropy/avg: 0.2580, train/total_loss: 0.0449, train/total_loss/avg: 0.2580, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 091ms, time_since_start: 54m 43s 138ms, eta: 05h 03m 01s 146ms\n","\u001b[32m2022-04-21T15:32:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0429, train/hateful_memes/cross_entropy/avg: 0.2505, train/total_loss: 0.0429, train/total_loss/avg: 0.2505, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 869ms, time_since_start: 56m 19s 007ms, eta: 05h 03m 52s 347ms\n","\u001b[32m2022-04-21T15:33:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0356, train/hateful_memes/cross_entropy/avg: 0.2434, train/total_loss: 0.0356, train/total_loss/avg: 0.2434, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 871ms, time_since_start: 57m 54s 878ms, eta: 05h 02m 15s 180ms\n","\u001b[32m2022-04-21T15:35:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0324, train/hateful_memes/cross_entropy/avg: 0.2366, train/total_loss: 0.0324, train/total_loss/avg: 0.2366, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 677ms, time_since_start: 59m 30s 556ms, eta: 05h 01s 333ms\n","\u001b[32m2022-04-21T15:36:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0324, train/hateful_memes/cross_entropy/avg: 0.2312, train/total_loss: 0.0324, train/total_loss/avg: 0.2312, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 777ms, time_since_start: 01h 01m 06s 334ms, eta: 04h 58m 42s 664ms\n","\u001b[32m2022-04-21T15:38:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0302, train/hateful_memes/cross_entropy/avg: 0.2255, train/total_loss: 0.0302, train/total_loss/avg: 0.2255, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 787ms, time_since_start: 01h 02m 42s 121ms, eta: 04h 57m 07s 034ms\n","\u001b[32m2022-04-21T15:40:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0287, train/hateful_memes/cross_entropy/avg: 0.2197, train/total_loss: 0.0287, train/total_loss/avg: 0.2197, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 339ms, time_since_start: 01h 04m 17s 460ms, eta: 04h 54m 06s 731ms\n","\u001b[32m2022-04-21T15:41:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0272, train/hateful_memes/cross_entropy/avg: 0.2141, train/total_loss: 0.0272, train/total_loss/avg: 0.2141, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 888ms, time_since_start: 01h 05m 53s 349ms, eta: 04h 54m 10s 877ms\n","\u001b[32m2022-04-21T15:43:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T15:43:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T15:43:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T15:43:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T15:43:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0216, train/hateful_memes/cross_entropy/avg: 0.2089, train/total_loss: 0.0216, train/total_loss/avg: 0.2089, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 0.85, time: 01m 58s 575ms, time_since_start: 01h 07m 51s 924ms, eta: 06h 01m 46s 350ms\n","\u001b[32m2022-04-21T15:43:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T15:43:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T15:43:44 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T15:43:44 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T15:43:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T15:43:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T15:44:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T15:44:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.5579, val/total_loss: 1.5579, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.4828, val/hateful_memes/roc_auc: 0.6971, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 31s 451ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.703941\n","\u001b[32m2022-04-21T15:45:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0195, train/hateful_memes/cross_entropy/avg: 0.2038, train/total_loss: 0.0195, train/total_loss/avg: 0.2038, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 708ms, time_since_start: 01h 10m 085ms, eta: 04h 53m 25s 113ms\n","\u001b[32m2022-04-21T15:47:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0176, train/hateful_memes/cross_entropy/avg: 0.1990, train/total_loss: 0.0176, train/total_loss/avg: 0.1990, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 730ms, time_since_start: 01h 11m 35s 816ms, eta: 04h 48m 49s 664ms\n","\u001b[32m2022-04-21T15:48:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0097, train/hateful_memes/cross_entropy/avg: 0.1945, train/total_loss: 0.0097, train/total_loss/avg: 0.1945, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 050ms, time_since_start: 01h 13m 10s 866ms, eta: 04h 45m 09s 913ms\n","\u001b[32m2022-04-21T15:50:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0091, train/hateful_memes/cross_entropy/avg: 0.1901, train/total_loss: 0.0091, train/total_loss/avg: 0.1901, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 527ms, time_since_start: 01h 14m 46s 393ms, eta: 04h 44m 58s 624ms\n","\u001b[32m2022-04-21T15:52:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0087, train/hateful_memes/cross_entropy/avg: 0.1859, train/total_loss: 0.0087, train/total_loss/avg: 0.1859, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 582ms, time_since_start: 01h 16m 21s 976ms, eta: 04h 43m 31s 334ms\n","\u001b[32m2022-04-21T15:53:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1820, train/total_loss: 0.0065, train/total_loss/avg: 0.1820, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 247ms, time_since_start: 01h 17m 57s 224ms, eta: 04h 40m 54s 884ms\n","\u001b[32m2022-04-21T15:55:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1785, train/total_loss: 0.0065, train/total_loss/avg: 0.1785, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 598ms, time_since_start: 01h 19m 32s 822ms, eta: 04h 40m 19s 643ms\n","\u001b[32m2022-04-21T15:56:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1754, train/total_loss: 0.0065, train/total_loss/avg: 0.1754, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 946ms, time_since_start: 01h 21m 07s 769ms, eta: 04h 36m 48s 491ms\n","\u001b[32m2022-04-21T15:58:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.1720, train/total_loss: 0.0056, train/total_loss/avg: 0.1720, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 432ms, time_since_start: 01h 22m 43s 202ms, eta: 04h 36m 36s 367ms\n","\u001b[32m2022-04-21T16:00:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T16:00:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T16:00:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T16:00:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T16:00:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0047, train/hateful_memes/cross_entropy/avg: 0.1686, train/total_loss: 0.0047, train/total_loss/avg: 0.1686, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 01s 675ms, time_since_start: 01h 24m 44s 877ms, eta: 05h 50m 36s 502ms\n","\u001b[32m2022-04-21T16:00:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T16:00:31 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T16:00:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T16:00:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T16:00:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T16:00:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T16:01:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T16:01:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.7670, val/total_loss: 1.7670, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.4724, val/hateful_memes/roc_auc: 0.6952, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 30s 414ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.703941\n","\u001b[32m2022-04-21T16:02:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0047, train/hateful_memes/cross_entropy/avg: 0.1659, train/total_loss: 0.0047, train/total_loss/avg: 0.1659, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 456ms, time_since_start: 01h 26m 51s 750ms, eta: 04h 36m 18s 214ms\n","\u001b[32m2022-04-21T16:04:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0044, train/hateful_memes/cross_entropy/avg: 0.1627, train/total_loss: 0.0044, train/total_loss/avg: 0.1627, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 044ms, time_since_start: 01h 28m 27s 795ms, eta: 04h 33m 29s 833ms\n","\u001b[32m2022-04-21T16:05:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0044, train/hateful_memes/cross_entropy/avg: 0.1609, train/total_loss: 0.0044, train/total_loss/avg: 0.1609, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 925ms, time_since_start: 01h 30m 03s 720ms, eta: 04h 31m 31s 877ms\n","\u001b[32m2022-04-21T16:07:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0036, train/hateful_memes/cross_entropy/avg: 0.1580, train/total_loss: 0.0036, train/total_loss/avg: 0.1580, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 229ms, time_since_start: 01h 31m 38s 949ms, eta: 04h 27m 56s 782ms\n","\u001b[32m2022-04-21T16:09:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0036, train/hateful_memes/cross_entropy/avg: 0.1554, train/total_loss: 0.0036, train/total_loss/avg: 0.1554, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 823ms, time_since_start: 01h 33m 14s 773ms, eta: 04h 27m 59s 714ms\n","\u001b[32m2022-04-21T16:10:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0036, train/hateful_memes/cross_entropy/avg: 0.1527, train/total_loss: 0.0036, train/total_loss/avg: 0.1527, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 454ms, time_since_start: 01h 34m 50s 228ms, eta: 04h 25m 20s 702ms\n","\u001b[32m2022-04-21T16:12:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0036, train/hateful_memes/cross_entropy/avg: 0.1504, train/total_loss: 0.0036, train/total_loss/avg: 0.1504, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 644ms, time_since_start: 01h 36m 25s 873ms, eta: 04h 24m 15s 127ms\n","\u001b[32m2022-04-21T16:13:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1478, train/total_loss: 0.0035, train/total_loss/avg: 0.1478, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 648ms, time_since_start: 01h 38m 01s 522ms, eta: 04h 22m 38s 551ms\n","\u001b[32m2022-04-21T16:15:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1453, train/total_loss: 0.0032, train/total_loss/avg: 0.1453, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 074ms, time_since_start: 01h 39m 36s 596ms, eta: 04h 19m 27s 149ms\n","\u001b[32m2022-04-21T16:16:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T16:16:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T16:17:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T16:17:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T16:17:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.1429, train/total_loss: 0.0031, train/total_loss/avg: 0.1429, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 0.85, time: 01m 58s 530ms, time_since_start: 01h 41m 35s 127ms, eta: 05h 21m 27s 351ms\n","\u001b[32m2022-04-21T16:17:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T16:17:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T16:17:27 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T16:17:27 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T16:17:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T16:17:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T16:17:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T16:17:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 1.9175, val/total_loss: 1.9175, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4582, val/hateful_memes/roc_auc: 0.6988, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 28s 012ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.703941\n","\u001b[32m2022-04-21T16:19:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.1406, train/total_loss: 0.0031, train/total_loss/avg: 0.1406, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 415ms, time_since_start: 01h 43m 39s 556ms, eta: 04h 19m 50s 732ms\n","\u001b[32m2022-04-21T16:21:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.1384, train/total_loss: 0.0031, train/total_loss/avg: 0.1384, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 317ms, time_since_start: 01h 45m 14s 874ms, eta: 04h 15m 16s 243ms\n","\u001b[32m2022-04-21T16:22:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1362, train/total_loss: 0.0030, train/total_loss/avg: 0.1362, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 493ms, time_since_start: 01h 46m 50s 368ms, eta: 04h 14m 07s 334ms\n","\u001b[32m2022-04-21T16:24:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1341, train/total_loss: 0.0030, train/total_loss/avg: 0.1341, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 806ms, time_since_start: 01h 48m 25s 174ms, eta: 04h 10m 41s 264ms\n","\u001b[32m2022-04-21T16:25:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.1321, train/total_loss: 0.0031, train/total_loss/avg: 0.1321, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 290ms, time_since_start: 01h 50m 465ms, eta: 04h 10m 21s 122ms\n","\u001b[32m2022-04-21T16:27:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.1301, train/total_loss: 0.0031, train/total_loss/avg: 0.1301, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 314ms, time_since_start: 01h 51m 35s 779ms, eta: 04h 08m 47s 935ms\n","\u001b[32m2022-04-21T16:28:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.1283, train/total_loss: 0.0031, train/total_loss/avg: 0.1283, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 025ms, time_since_start: 01h 53m 10s 804ms, eta: 04h 06m 26s 029ms\n","\u001b[32m2022-04-21T16:30:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1264, train/total_loss: 0.0025, train/total_loss/avg: 0.1264, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 429ms, time_since_start: 01h 54m 46s 234ms, eta: 04h 05m 51s 931ms\n","\u001b[32m2022-04-21T16:32:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1247, train/total_loss: 0.0025, train/total_loss/avg: 0.1247, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 316ms, time_since_start: 01h 56m 21s 550ms, eta: 04h 03m 57s 417ms\n","\u001b[32m2022-04-21T16:33:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T16:33:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T16:33:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T16:34:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T16:34:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1229, train/total_loss: 0.0025, train/total_loss/avg: 0.1229, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 01s 865ms, time_since_start: 01h 58m 23s 416ms, eta: 05h 09m 50s 652ms\n","\u001b[32m2022-04-21T16:34:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T16:34:10 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T16:34:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T16:34:15 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T16:34:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T16:34:28 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-21T16:34:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T16:35:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T16:35:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.1071, val/total_loss: 2.1071, val/hateful_memes/accuracy: 0.7000, val/hateful_memes/binary_f1: 0.4873, val/hateful_memes/roc_auc: 0.7070, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 50s 101ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.707044\n","\u001b[32m2022-04-21T16:36:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.1212, train/total_loss: 0.0022, train/total_loss/avg: 0.1212, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 764ms, time_since_start: 02h 50s 284ms, eta: 04h 04m 23s 007ms\n","\u001b[32m2022-04-21T16:38:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.1196, train/total_loss: 0.0022, train/total_loss/avg: 0.1196, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 453ms, time_since_start: 02h 02m 25s 738ms, eta: 03h 59m 27s 275ms\n","\u001b[32m2022-04-21T16:39:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.1180, train/total_loss: 0.0022, train/total_loss/avg: 0.1180, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 557ms, time_since_start: 02h 04m 01s 295ms, eta: 03h 58m 05s 685ms\n","\u001b[32m2022-04-21T16:41:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1164, train/total_loss: 0.0020, train/total_loss/avg: 0.1164, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 589ms, time_since_start: 02h 05m 36s 885ms, eta: 03h 56m 33s 397ms\n","\u001b[32m2022-04-21T16:42:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1149, train/total_loss: 0.0017, train/total_loss/avg: 0.1149, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 007ms, time_since_start: 02h 07m 11s 892ms, eta: 03h 53m 30s 266ms\n","\u001b[32m2022-04-21T16:44:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1134, train/total_loss: 0.0017, train/total_loss/avg: 0.1134, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 612ms, time_since_start: 02h 08m 47s 505ms, eta: 03h 53m 22s 274ms\n","\u001b[32m2022-04-21T16:46:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1121, train/total_loss: 0.0017, train/total_loss/avg: 0.1121, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 594ms, time_since_start: 02h 10m 23s 099ms, eta: 03h 51m 42s 367ms\n","\u001b[32m2022-04-21T16:47:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1107, train/total_loss: 0.0017, train/total_loss/avg: 0.1107, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 087ms, time_since_start: 02h 11m 58s 186ms, eta: 03h 48m 51s 902ms\n","\u001b[32m2022-04-21T16:49:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1099, train/total_loss: 0.0020, train/total_loss/avg: 0.1099, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 350ms, time_since_start: 02h 13m 33s 537ms, eta: 03h 47m 53s 027ms\n","\u001b[32m2022-04-21T16:50:55 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T16:50:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T16:51:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T16:51:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T16:51:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1086, train/total_loss: 0.0020, train/total_loss/avg: 0.1086, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 0.85, time: 01m 57s 843ms, time_since_start: 02h 15m 31s 381ms, eta: 04h 39m 38s 608ms\n","\u001b[32m2022-04-21T16:51:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T16:51:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T16:51:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T16:51:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T16:51:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T16:51:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T16:51:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T16:51:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.2494, val/total_loss: 2.2494, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4136, val/hateful_memes/roc_auc: 0.6893, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 32s 220ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.707044\n","\u001b[32m2022-04-21T16:53:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.1073, train/total_loss: 0.0022, train/total_loss/avg: 0.1073, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 381ms, time_since_start: 02h 17m 39s 985ms, eta: 03h 47m 04s 752ms\n","\u001b[32m2022-04-21T16:55:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1060, train/total_loss: 0.0020, train/total_loss/avg: 0.1060, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 498ms, time_since_start: 02h 19m 15s 483ms, eta: 03h 43m 22s 813ms\n","\u001b[32m2022-04-21T16:56:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1049, train/total_loss: 0.0025, train/total_loss/avg: 0.1049, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 185ms, time_since_start: 02h 20m 50s 669ms, eta: 03h 41m 02s 131ms\n","\u001b[32m2022-04-21T16:58:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1036, train/total_loss: 0.0025, train/total_loss/avg: 0.1036, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 517ms, time_since_start: 02h 22m 26s 186ms, eta: 03h 40m 11s 194ms\n","\u001b[32m2022-04-21T16:59:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1024, train/total_loss: 0.0020, train/total_loss/avg: 0.1024, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 477ms, time_since_start: 02h 24m 01s 663ms, eta: 03h 38m 28s 566ms\n","\u001b[32m2022-04-21T17:01:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1017, train/total_loss: 0.0020, train/total_loss/avg: 0.1017, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 832ms, time_since_start: 02h 25m 36s 496ms, eta: 03h 35m 23s 613ms\n","\u001b[32m2022-04-21T17:02:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1006, train/total_loss: 0.0017, train/total_loss/avg: 0.1006, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 309ms, time_since_start: 02h 27m 11s 805ms, eta: 03h 34m 51s 639ms\n","\u001b[32m2022-04-21T17:04:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.0994, train/total_loss: 0.0014, train/total_loss/avg: 0.0994, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 936ms, time_since_start: 02h 28m 46s 742ms, eta: 03h 32m 24s 673ms\n","\u001b[32m2022-04-21T17:06:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0983, train/total_loss: 0.0010, train/total_loss/avg: 0.0983, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 282ms, time_since_start: 02h 30m 22s 024ms, eta: 03h 31m 34s 152ms\n","\u001b[32m2022-04-21T17:07:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T17:07:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T17:07:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T17:08:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T17:08:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0973, train/total_loss: 0.0010, train/total_loss/avg: 0.0973, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 01s 186ms, time_since_start: 02h 32m 23s 210ms, eta: 04h 27m 02s 016ms\n","\u001b[32m2022-04-21T17:08:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T17:08:09 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T17:08:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T17:08:15 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T17:08:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T17:08:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T17:08:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T17:08:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.1649, val/total_loss: 2.1649, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.4377, val/hateful_memes/roc_auc: 0.6928, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 22000, val_time: 30s 286ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.707044\n","\u001b[32m2022-04-21T17:10:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.0963, train/total_loss: 0.0014, train/total_loss/avg: 0.0963, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 092ms, time_since_start: 02h 34m 29s 591ms, eta: 03h 30m 06s 620ms\n","\u001b[32m2022-04-21T17:11:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0953, train/total_loss: 0.0010, train/total_loss/avg: 0.0953, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 553ms, time_since_start: 02h 36m 05s 144ms, eta: 03h 27m 18s 731ms\n","\u001b[32m2022-04-21T17:13:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0943, train/total_loss: 0.0010, train/total_loss/avg: 0.0943, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 597ms, time_since_start: 02h 37m 40s 742ms, eta: 03h 25m 47s 330ms\n","\u001b[32m2022-04-21T17:15:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0933, train/total_loss: 0.0009, train/total_loss/avg: 0.0933, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 322ms, time_since_start: 02h 39m 16s 064ms, eta: 03h 23m 34s 803ms\n","\u001b[32m2022-04-21T17:16:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0923, train/total_loss: 0.0009, train/total_loss/avg: 0.0923, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 526ms, time_since_start: 02h 40m 51s 591ms, eta: 03h 22m 23s 866ms\n","\u001b[32m2022-04-21T17:18:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0916, train/total_loss: 0.0009, train/total_loss/avg: 0.0916, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 975ms, time_since_start: 02h 42m 26s 567ms, eta: 03h 19m 37s 205ms\n","\u001b[32m2022-04-21T17:19:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0907, train/total_loss: 0.0009, train/total_loss/avg: 0.0907, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 408ms, time_since_start: 02h 44m 01s 976ms, eta: 03h 18m 54s 756ms\n","\u001b[32m2022-04-21T17:21:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0900, train/total_loss: 0.0010, train/total_loss/avg: 0.0900, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 436ms, time_since_start: 02h 45m 37s 412ms, eta: 03h 17m 21s 161ms\n","\u001b[32m2022-04-21T17:22:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0891, train/total_loss: 0.0009, train/total_loss/avg: 0.0891, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 252ms, time_since_start: 02h 47m 12s 664ms, eta: 03h 15m 21s 463ms\n","\u001b[32m2022-04-21T17:24:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T17:24:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T17:24:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T17:25:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T17:25:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0882, train/total_loss: 0.0009, train/total_loss/avg: 0.0882, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00001, ups: 0.82, time: 02m 02s 311ms, time_since_start: 02h 49m 14s 976ms, eta: 04h 08m 46s 900ms\n","\u001b[32m2022-04-21T17:25:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T17:25:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T17:25:07 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T17:25:07 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T17:25:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T17:25:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T17:25:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T17:25:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.3594, val/total_loss: 2.3594, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.4615, val/hateful_memes/roc_auc: 0.6864, num_updates: 10000, epoch: 38, iterations: 10000, max_updates: 22000, val_time: 30s 205ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.707044\n","\u001b[32m2022-04-21T17:27:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0873, train/total_loss: 0.0009, train/total_loss/avg: 0.0873, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 516ms, time_since_start: 02h 51m 21s 700ms, eta: 03h 14m 40s 738ms\n","\u001b[32m2022-04-21T17:28:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0865, train/total_loss: 0.0009, train/total_loss/avg: 0.0865, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 930ms, time_since_start: 02h 52m 56s 631ms, eta: 03h 09m 52s 284ms\n","\u001b[32m2022-04-21T17:30:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0857, train/total_loss: 0.0009, train/total_loss/avg: 0.0857, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 358ms, time_since_start: 02h 54m 31s 990ms, eta: 03h 09m 06s 669ms\n","\u001b[32m2022-04-21T17:31:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0857, train/total_loss: 0.0009, train/total_loss/avg: 0.0857, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 166ms, time_since_start: 02h 56m 07s 156ms, eta: 03h 07m 06s 946ms\n","\u001b[32m2022-04-21T17:33:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0849, train/total_loss: 0.0010, train/total_loss/avg: 0.0849, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 402ms, time_since_start: 02h 57m 42s 558ms, eta: 03h 05m 57s 774ms\n","\u001b[32m2022-04-21T17:35:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0841, train/total_loss: 0.0009, train/total_loss/avg: 0.0841, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 382ms, time_since_start: 02h 59m 17s 941ms, eta: 03h 04m 18s 483ms\n","\u001b[32m2022-04-21T17:36:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0833, train/total_loss: 0.0006, train/total_loss/avg: 0.0833, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 871ms, time_since_start: 03h 52s 812ms, eta: 03h 01m 42s 711ms\n","\u001b[32m2022-04-21T17:38:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0826, train/total_loss: 0.0006, train/total_loss/avg: 0.0826, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 391ms, time_since_start: 03h 02m 28s 204ms, eta: 03h 01m 05s 456ms\n","\u001b[32m2022-04-21T17:39:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0819, train/total_loss: 0.0009, train/total_loss/avg: 0.0819, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 431ms, time_since_start: 03h 04m 03s 636ms, eta: 02h 59m 33s 029ms\n","\u001b[32m2022-04-21T17:41:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T17:41:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T17:41:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T17:41:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T17:41:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0811, train/total_loss: 0.0006, train/total_loss/avg: 0.0811, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00001, ups: 0.85, time: 01m 58s 880ms, time_since_start: 03h 06m 02s 516ms, eta: 03h 41m 39s 150ms\n","\u001b[32m2022-04-21T17:41:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T17:41:49 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T17:41:54 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T17:41:54 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T17:41:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T17:42:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T17:42:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T17:42:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.3472, val/total_loss: 2.3472, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.4281, val/hateful_memes/roc_auc: 0.6947, num_updates: 11000, epoch: 42, iterations: 11000, max_updates: 22000, val_time: 30s 982ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.707044\n","\u001b[32m2022-04-21T17:43:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0805, train/total_loss: 0.0006, train/total_loss/avg: 0.0805, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 383ms, time_since_start: 03h 08m 09s 884ms, eta: 02h 58m 04s 456ms\n","\u001b[32m2022-04-21T17:45:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0798, train/total_loss: 0.0004, train/total_loss/avg: 0.0798, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 941ms, time_since_start: 03h 09m 44s 826ms, eta: 02h 53m 48s 004ms\n","\u001b[32m2022-04-21T17:47:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0791, train/total_loss: 0.0004, train/total_loss/avg: 0.0791, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 337ms, time_since_start: 03h 11m 20s 164ms, eta: 02h 52m 54s 545ms\n","\u001b[32m2022-04-21T17:48:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0784, train/total_loss: 0.0004, train/total_loss/avg: 0.0784, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 271ms, time_since_start: 03h 12m 55s 435ms, eta: 02h 51m 10s 418ms\n","\u001b[32m2022-04-21T17:50:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0781, train/total_loss: 0.0009, train/total_loss/avg: 0.0781, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 979ms, time_since_start: 03h 14m 30s 414ms, eta: 02h 49m 02s 380ms\n","\u001b[32m2022-04-21T17:51:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0774, train/total_loss: 0.0005, train/total_loss/avg: 0.0774, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 204ms, time_since_start: 03h 16m 05s 619ms, eta: 02h 47m 49s 626ms\n","\u001b[32m2022-04-21T17:53:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0768, train/total_loss: 0.0004, train/total_loss/avg: 0.0768, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 077ms, time_since_start: 03h 17m 40s 697ms, eta: 02h 45m 59s 486ms\n","\u001b[32m2022-04-21T17:55:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0761, train/total_loss: 0.0004, train/total_loss/avg: 0.0761, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 703ms, time_since_start: 03h 19m 15s 400ms, eta: 02h 43m 43s 930ms\n","\u001b[32m2022-04-21T17:56:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0755, train/total_loss: 0.0004, train/total_loss/avg: 0.0755, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 147ms, time_since_start: 03h 20m 50s 547ms, eta: 02h 42m 53s 264ms\n","\u001b[32m2022-04-21T17:58:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T17:58:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T17:58:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T17:58:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T17:58:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0749, train/total_loss: 0.0004, train/total_loss/avg: 0.0749, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00001, ups: 0.84, time: 01m 59s 616ms, time_since_start: 03h 22m 50s 164ms, eta: 03h 22m 45s 050ms\n","\u001b[32m2022-04-21T17:58:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T17:58:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T17:58:42 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T17:58:42 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T17:58:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.5599, val/total_loss: 2.5599, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.4392, val/hateful_memes/roc_auc: 0.6960, num_updates: 12000, epoch: 46, iterations: 12000, max_updates: 22000, val_time: 05s 784ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.707044\n","\u001b[32m2022-04-21T18:00:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0742, train/total_loss: 0.0004, train/total_loss/avg: 0.0742, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 408ms, time_since_start: 03h 24m 32s 359ms, eta: 02h 41m 46s 711ms\n","\u001b[32m2022-04-21T18:01:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0736, train/total_loss: 0.0004, train/total_loss/avg: 0.0736, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 494ms, time_since_start: 03h 26m 07s 855ms, eta: 02h 38m 37s 606ms\n","\u001b[32m2022-04-21T18:03:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0731, train/total_loss: 0.0004, train/total_loss/avg: 0.0731, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 995ms, time_since_start: 03h 27m 42s 850ms, eta: 02h 36m 11s 221ms\n","\u001b[32m2022-04-21T18:05:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0725, train/total_loss: 0.0002, train/total_loss/avg: 0.0725, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 555ms, time_since_start: 03h 29m 18s 405ms, eta: 02h 35m 29s 242ms\n","\u001b[32m2022-04-21T18:06:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0719, train/total_loss: 0.0002, train/total_loss/avg: 0.0719, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 495ms, time_since_start: 03h 30m 53s 901ms, eta: 02h 33m 46s 291ms\n","\u001b[32m2022-04-21T18:08:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0713, train/total_loss: 0.0003, train/total_loss/avg: 0.0713, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 437ms, time_since_start: 03h 32m 29s 338ms, eta: 02h 32m 03s 619ms\n","\u001b[32m2022-04-21T18:09:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0708, train/total_loss: 0.0003, train/total_loss/avg: 0.0708, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 675ms, time_since_start: 03h 34m 05s 014ms, eta: 02h 30m 49s 073ms\n","\u001b[32m2022-04-21T18:11:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0702, train/total_loss: 0.0003, train/total_loss/avg: 0.0702, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 334ms, time_since_start: 03h 35m 40s 348ms, eta: 02h 28m 39s 847ms\n","\u001b[32m2022-04-21T18:13:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0698, train/total_loss: 0.0003, train/total_loss/avg: 0.0698, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 694ms, time_since_start: 03h 37m 16s 042ms, eta: 02h 27m 36s 225ms\n","\u001b[32m2022-04-21T18:14:38 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T18:14:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T18:14:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T18:15:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T18:15:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0692, train/total_loss: 0.0003, train/total_loss/avg: 0.0692, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0., ups: 0.84, time: 01m 59s 937ms, time_since_start: 03h 39m 15s 979ms, eta: 03h 02m 57s 856ms\n","\u001b[32m2022-04-21T18:15:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T18:15:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T18:15:08 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T18:15:08 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T18:15:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.2074, val/total_loss: 2.2074, val/hateful_memes/accuracy: 0.6981, val/hateful_memes/binary_f1: 0.4985, val/hateful_memes/roc_auc: 0.7029, num_updates: 13000, epoch: 49, iterations: 13000, max_updates: 22000, val_time: 05s 667ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.707044\n","\u001b[32m2022-04-21T18:16:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0687, train/total_loss: 0.0002, train/total_loss/avg: 0.0687, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 368ms, time_since_start: 03h 40m 58s 017ms, eta: 02h 25m 22s 641ms\n","\u001b[32m2022-04-21T18:18:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0682, train/total_loss: 0.0002, train/total_loss/avg: 0.0682, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 933ms, time_since_start: 03h 42m 33s 951ms, eta: 02h 23m 05s 682ms\n","\u001b[32m2022-04-21T18:19:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0679, train/total_loss: 0.0002, train/total_loss/avg: 0.0679, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 379ms, time_since_start: 03h 44m 09s 330ms, eta: 02h 20m 39s 044ms\n","\u001b[32m2022-04-21T18:21:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0674, train/total_loss: 0.0002, train/total_loss/avg: 0.0674, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 811ms, time_since_start: 03h 45m 45s 142ms, eta: 02h 19m 39s 909ms\n","\u001b[32m2022-04-21T18:23:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0669, train/total_loss: 0.0001, train/total_loss/avg: 0.0669, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 840ms, time_since_start: 03h 47m 20s 983ms, eta: 02h 18m 04s 930ms\n","\u001b[32m2022-04-21T18:24:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0664, train/total_loss: 0.0001, train/total_loss/avg: 0.0664, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 447ms, time_since_start: 03h 48m 56s 431ms, eta: 02h 15m 53s 921ms\n","\u001b[32m2022-04-21T18:26:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0659, train/total_loss: 0.0001, train/total_loss/avg: 0.0659, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 807ms, time_since_start: 03h 50m 32s 238ms, eta: 02h 14m 47s 203ms\n","\u001b[32m2022-04-21T18:27:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0654, train/total_loss: 0.0001, train/total_loss/avg: 0.0654, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 802ms, time_since_start: 03h 52m 08s 041ms, eta: 02h 13m 09s 361ms\n","\u001b[32m2022-04-21T18:29:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0649, train/total_loss: 0.0001, train/total_loss/avg: 0.0649, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 347ms, time_since_start: 03h 53m 43s 389ms, eta: 02h 10m 54s 478ms\n","\u001b[32m2022-04-21T18:31:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T18:31:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T18:31:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T18:31:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T18:31:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0645, train/total_loss: 0.0001, train/total_loss/avg: 0.0645, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0., ups: 0.83, time: 02m 565ms, time_since_start: 03h 55m 43s 954ms, eta: 02h 43m 29s 207ms\n","\u001b[32m2022-04-21T18:31:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T18:31:30 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T18:31:36 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T18:31:36 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T18:31:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T18:31:48 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-21T18:32:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T18:32:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T18:32:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.2292, val/total_loss: 2.2292, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.5165, val/hateful_memes/roc_auc: 0.7135, num_updates: 14000, epoch: 53, iterations: 14000, max_updates: 22000, val_time: 01m 10s 123ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.713515\n","\u001b[32m2022-04-21T18:34:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0640, train/total_loss: 0.0001, train/total_loss/avg: 0.0640, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 551ms, time_since_start: 03h 58m 30s 631ms, eta: 02h 09m 17s 238ms\n","\u001b[32m2022-04-21T18:35:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0636, train/total_loss: 0.0001, train/total_loss/avg: 0.0636, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 603ms, time_since_start: 04h 06s 235ms, eta: 02h 06m 23s 881ms\n","\u001b[32m2022-04-21T18:37:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0631, train/total_loss: 0.0001, train/total_loss/avg: 0.0631, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 677ms, time_since_start: 04h 01m 41s 912ms, eta: 02h 04m 52s 379ms\n","\u001b[32m2022-04-21T18:39:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0627, train/total_loss: 0.0001, train/total_loss/avg: 0.0627, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 054ms, time_since_start: 04h 03m 16s 967ms, eta: 02h 02m 26s 934ms\n","\u001b[32m2022-04-21T18:40:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0622, train/total_loss: 0.0001, train/total_loss/avg: 0.0622, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 682ms, time_since_start: 04h 04m 52s 650ms, eta: 02h 01m 38s 218ms\n","\u001b[32m2022-04-21T18:42:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0618, train/total_loss: 0.0001, train/total_loss/avg: 0.0618, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 600ms, time_since_start: 04h 06m 28s 250ms, eta: 01h 59m 54s 703ms\n","\u001b[32m2022-04-21T18:43:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0614, train/total_loss: 0.0001, train/total_loss/avg: 0.0614, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 233ms, time_since_start: 04h 08m 03s 483ms, eta: 01h 57m 50s 210ms\n","\u001b[32m2022-04-21T18:45:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0610, train/total_loss: 0.0001, train/total_loss/avg: 0.0610, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 642ms, time_since_start: 04h 09m 39s 126ms, eta: 01h 56m 43s 319ms\n","\u001b[32m2022-04-21T18:47:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0606, train/total_loss: 0.0001, train/total_loss/avg: 0.0606, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 190ms, time_since_start: 04h 11m 14s 316ms, eta: 01h 54m 33s 419ms\n","\u001b[32m2022-04-21T18:48:36 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T18:48:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T18:48:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T18:48:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T18:48:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0602, train/total_loss: 0.0001, train/total_loss/avg: 0.0602, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 58s 311ms, time_since_start: 04h 13m 12s 628ms, eta: 02h 20m 22s 596ms\n","\u001b[32m2022-04-21T18:48:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T18:48:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T18:49:05 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T18:49:05 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T18:49:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T18:49:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T18:49:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T18:49:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.4510, val/total_loss: 2.4510, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4815, val/hateful_memes/roc_auc: 0.7038, num_updates: 15000, epoch: 57, iterations: 15000, max_updates: 22000, val_time: 28s 853ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.713515\n","\u001b[32m2022-04-21T18:51:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0598, train/total_loss: 0.0001, train/total_loss/avg: 0.0598, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 714ms, time_since_start: 04h 15m 18s 197ms, eta: 01h 53m 06s 715ms\n","\u001b[32m2022-04-21T18:52:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0594, train/total_loss: 0.0001, train/total_loss/avg: 0.0594, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 529ms, time_since_start: 04h 16m 53s 726ms, eta: 01h 50m 06s 426ms\n","\u001b[32m2022-04-21T18:54:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0590, train/total_loss: 0.0001, train/total_loss/avg: 0.0590, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 691ms, time_since_start: 04h 18m 29s 418ms, eta: 01h 48m 40s 338ms\n","\u001b[32m2022-04-21T18:55:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0586, train/total_loss: 0.0001, train/total_loss/avg: 0.0586, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 639ms, time_since_start: 04h 20m 05s 058ms, eta: 01h 46m 59s 508ms\n","\u001b[32m2022-04-21T18:57:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0583, train/total_loss: 0.0001, train/total_loss/avg: 0.0583, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 133ms, time_since_start: 04h 21m 40s 191ms, eta: 01h 44m 48s 822ms\n","\u001b[32m2022-04-21T18:59:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0579, train/total_loss: 0.0001, train/total_loss/avg: 0.0579, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 552ms, time_since_start: 04h 23m 15s 744ms, eta: 01h 43m 39s 339ms\n","\u001b[32m2022-04-21T19:00:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0575, train/total_loss: 0.0001, train/total_loss/avg: 0.0575, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 270ms, time_since_start: 04h 24m 51s 014ms, eta: 01h 41m 44s 063ms\n","\u001b[32m2022-04-21T19:02:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0572, train/total_loss: 0.0001, train/total_loss/avg: 0.0572, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 603ms, time_since_start: 04h 26m 26s 618ms, eta: 01h 40m 28s 210ms\n","\u001b[32m2022-04-21T19:03:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0568, train/total_loss: 0.0001, train/total_loss/avg: 0.0568, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 558ms, time_since_start: 04h 28m 02s 177ms, eta: 01h 38m 48s 192ms\n","\u001b[32m2022-04-21T19:05:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T19:05:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T19:05:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T19:05:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T19:05:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0566, train/total_loss: 0.0001, train/total_loss/avg: 0.0566, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0., ups: 0.83, time: 02m 272ms, time_since_start: 04h 30m 02s 450ms, eta: 02h 02m 19s 011ms\n","\u001b[32m2022-04-21T19:05:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T19:05:49 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T19:05:54 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T19:05:54 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T19:05:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T19:06:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T19:06:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T19:06:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 2.2674, val/total_loss: 2.2674, val/hateful_memes/accuracy: 0.6981, val/hateful_memes/binary_f1: 0.5356, val/hateful_memes/roc_auc: 0.7089, num_updates: 16000, epoch: 61, iterations: 16000, max_updates: 22000, val_time: 32s 319ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.713515\n","\u001b[32m2022-04-21T19:07:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0562, train/total_loss: 0.0001, train/total_loss/avg: 0.0562, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 531ms, time_since_start: 04h 32m 11s 303ms, eta: 01h 36m 32s 190ms\n","\u001b[32m2022-04-21T19:09:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0559, train/total_loss: 0.0001, train/total_loss/avg: 0.0559, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 470ms, time_since_start: 04h 33m 46s 773ms, eta: 01h 33m 51s 418ms\n","\u001b[32m2022-04-21T19:11:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0555, train/total_loss: 0.0001, train/total_loss/avg: 0.0555, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 144ms, time_since_start: 04h 35m 21s 918ms, eta: 01h 31m 55s 414ms\n","\u001b[32m2022-04-21T19:12:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0552, train/total_loss: 0.0001, train/total_loss/avg: 0.0552, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 484ms, time_since_start: 04h 36m 57s 402ms, eta: 01h 30m 38s 041ms\n","\u001b[32m2022-04-21T19:14:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0549, train/total_loss: 0.0001, train/total_loss/avg: 0.0549, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 088ms, time_since_start: 04h 38m 32s 490ms, eta: 01h 28m 38s 757ms\n","\u001b[32m2022-04-21T19:15:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0545, train/total_loss: 0.0001, train/total_loss/avg: 0.0545, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 560ms, time_since_start: 04h 40m 08s 051ms, eta: 01h 27m 28s 008ms\n","\u001b[32m2022-04-21T19:17:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0542, train/total_loss: 0.0001, train/total_loss/avg: 0.0542, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 322ms, time_since_start: 04h 41m 43s 374ms, eta: 01h 25m 37s 988ms\n","\u001b[32m2022-04-21T19:19:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0539, train/total_loss: 0.0001, train/total_loss/avg: 0.0539, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 028ms, time_since_start: 04h 43m 18s 402ms, eta: 01h 23m 45s 471ms\n","\u001b[32m2022-04-21T19:20:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0536, train/total_loss: 0.0001, train/total_loss/avg: 0.0536, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 271ms, time_since_start: 04h 44m 53s 674ms, eta: 01h 22m 21s 450ms\n","\u001b[32m2022-04-21T19:22:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T19:22:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T19:22:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T19:22:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T19:22:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0532, train/total_loss: 0.0001, train/total_loss/avg: 0.0532, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 57s 727ms, time_since_start: 04h 46m 51s 401ms, eta: 01h 39m 46s 425ms\n","\u001b[32m2022-04-21T19:22:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T19:22:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T19:22:43 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T19:22:43 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T19:22:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T19:22:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T19:23:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T19:23:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 2.6155, val/total_loss: 2.6155, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.4777, val/hateful_memes/roc_auc: 0.7072, num_updates: 17000, epoch: 64, iterations: 17000, max_updates: 22000, val_time: 30s 165ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.713515\n","\u001b[32m2022-04-21T19:24:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0529, train/total_loss: 0.0001, train/total_loss/avg: 0.0529, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 027ms, time_since_start: 04h 48m 57s 595ms, eta: 01h 19m 45s 316ms\n","\u001b[32m2022-04-21T19:26:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0526, train/total_loss: 0.0001, train/total_loss/avg: 0.0526, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 771ms, time_since_start: 04h 50m 33s 367ms, eta: 01h 17m 55s 199ms\n","\u001b[32m2022-04-21T19:27:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0523, train/total_loss: 0.0001, train/total_loss/avg: 0.0523, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 493ms, time_since_start: 04h 52m 08s 861ms, eta: 01h 16m 04s 494ms\n","\u001b[32m2022-04-21T19:29:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0520, train/total_loss: 0.0001, train/total_loss/avg: 0.0520, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 700ms, time_since_start: 04h 53m 44s 561ms, eta: 01h 14m 37s 041ms\n","\u001b[32m2022-04-21T19:31:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0517, train/total_loss: 0.0001, train/total_loss/avg: 0.0517, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 815ms, time_since_start: 04h 55m 20s 377ms, eta: 01h 13m 05s 007ms\n","\u001b[32m2022-04-21T19:32:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0514, train/total_loss: 0.0001, train/total_loss/avg: 0.0514, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 355ms, time_since_start: 04h 56m 55s 732ms, eta: 01h 11m 06s 968ms\n","\u001b[32m2022-04-21T19:34:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0511, train/total_loss: 0.0001, train/total_loss/avg: 0.0511, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 710ms, time_since_start: 04h 58m 31s 443ms, eta: 01h 09m 45s 521ms\n","\u001b[32m2022-04-21T19:35:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0509, train/total_loss: 0.0001, train/total_loss/avg: 0.0509, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 798ms, time_since_start: 05h 07s 242ms, eta: 01h 08m 11s 944ms\n","\u001b[32m2022-04-21T19:37:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0506, train/total_loss: 0.0001, train/total_loss/avg: 0.0506, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 370ms, time_since_start: 05h 01m 42s 612ms, eta: 01h 06m 16s 656ms\n","\u001b[32m2022-04-21T19:39:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T19:39:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T19:39:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T19:39:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T19:39:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0503, train/total_loss: 0.0001, train/total_loss/avg: 0.0503, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 57s 400ms, time_since_start: 05h 03m 40s 013ms, eta: 01h 19m 35s 859ms\n","\u001b[32m2022-04-21T19:39:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T19:39:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T19:39:31 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T19:39:31 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T19:39:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T19:39:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T19:39:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T19:39:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 2.5684, val/total_loss: 2.5684, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.4710, val/hateful_memes/roc_auc: 0.7127, num_updates: 18000, epoch: 68, iterations: 18000, max_updates: 22000, val_time: 29s 115ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.713515\n","\u001b[32m2022-04-21T19:41:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0500, train/total_loss: 0.0001, train/total_loss/avg: 0.0500, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 453ms, time_since_start: 05h 05m 45s 583ms, eta: 01h 03m 45s 623ms\n","\u001b[32m2022-04-21T19:43:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0497, train/total_loss: 0.0001, train/total_loss/avg: 0.0497, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 088ms, time_since_start: 05h 07m 21s 672ms, eta: 01h 01m 53s 451ms\n","\u001b[32m2022-04-21T19:44:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0495, train/total_loss: 0.0001, train/total_loss/avg: 0.0495, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 035ms, time_since_start: 05h 08m 57s 708ms, eta: 01h 13s 735ms\n","\u001b[32m2022-04-21T19:46:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0492, train/total_loss: 0.0001, train/total_loss/avg: 0.0492, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 657ms, time_since_start: 05h 10m 33s 366ms, eta: 58m 22s 225ms\n","\u001b[32m2022-04-21T19:47:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0489, train/total_loss: 0.0001, train/total_loss/avg: 0.0489, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 807ms, time_since_start: 05h 12m 09s 173ms, eta: 56m 50s 269ms\n","\u001b[32m2022-04-21T19:49:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0487, train/total_loss: 0.0000, train/total_loss/avg: 0.0487, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 595ms, time_since_start: 05h 13m 44s 769ms, eta: 55m 05s 508ms\n","\u001b[32m2022-04-21T19:51:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0484, train/total_loss: 0.0000, train/total_loss/avg: 0.0484, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 232ms, time_since_start: 05h 15m 20s 002ms, eta: 53m 16s 096ms\n","\u001b[32m2022-04-21T19:52:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0481, train/total_loss: 0.0000, train/total_loss/avg: 0.0481, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 630ms, time_since_start: 05h 16m 55s 632ms, eta: 51m 52s 205ms\n","\u001b[32m2022-04-21T19:54:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0479, train/total_loss: 0.0000, train/total_loss/avg: 0.0479, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 164ms, time_since_start: 05h 18m 30s 797ms, eta: 50m 250ms\n","\u001b[32m2022-04-21T19:55:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T19:55:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T19:56:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T19:56:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T19:56:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0476, train/total_loss: 0.0000, train/total_loss/avg: 0.0476, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 932ms, time_since_start: 05h 20m 26s 730ms, eta: 58m 57s 112ms\n","\u001b[32m2022-04-21T19:56:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T19:56:13 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T19:56:19 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T19:56:19 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T19:56:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 2.7689, val/total_loss: 2.7689, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4186, val/hateful_memes/roc_auc: 0.7118, num_updates: 19000, epoch: 72, iterations: 19000, max_updates: 22000, val_time: 06s 093ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.713515\n","\u001b[32m2022-04-21T19:57:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0474, train/total_loss: 0.0000, train/total_loss/avg: 0.0474, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 443ms, time_since_start: 05h 22m 09s 268ms, eta: 47m 24s 405ms\n","\u001b[32m2022-04-21T19:59:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0471, train/total_loss: 0.0000, train/total_loss/avg: 0.0471, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 232ms, time_since_start: 05h 23m 44s 500ms, eta: 45m 11s 833ms\n","\u001b[32m2022-04-21T20:01:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0469, train/total_loss: 0.0000, train/total_loss/avg: 0.0469, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 283ms, time_since_start: 05h 25m 19s 783ms, eta: 43m 36s 381ms\n","\u001b[32m2022-04-21T20:02:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0467, train/total_loss: 0.0000, train/total_loss/avg: 0.0467, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 291ms, time_since_start: 05h 26m 55s 075ms, eta: 41m 59s 696ms\n","\u001b[32m2022-04-21T20:04:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0464, train/total_loss: 0.0000, train/total_loss/avg: 0.0464, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 956ms, time_since_start: 05h 28m 30s 031ms, eta: 40m 14s 258ms\n","\u001b[32m2022-04-21T20:05:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0462, train/total_loss: 0.0000, train/total_loss/avg: 0.0462, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 268ms, time_since_start: 05h 30m 05s 299ms, eta: 38m 45s 306ms\n","\u001b[32m2022-04-21T20:07:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0459, train/total_loss: 0.0000, train/total_loss/avg: 0.0459, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 898ms, time_since_start: 05h 31m 40s 198ms, eta: 36m 59s 778ms\n","\u001b[32m2022-04-21T20:09:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0457, train/total_loss: 0.0000, train/total_loss/avg: 0.0457, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 266ms, time_since_start: 05h 33m 15s 465ms, eta: 35m 31s 502ms\n","\u001b[32m2022-04-21T20:10:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0455, train/total_loss: 0.0000, train/total_loss/avg: 0.0455, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 313ms, time_since_start: 05h 34m 50s 778ms, eta: 33m 55s 600ms\n","\u001b[32m2022-04-21T20:12:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T20:12:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T20:12:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T20:12:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T20:12:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0453, train/total_loss: 0.0000, train/total_loss/avg: 0.0453, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0., ups: 0.87, time: 01m 55s 993ms, time_since_start: 05h 36m 46s 772ms, eta: 39m 19s 315ms\n","\u001b[32m2022-04-21T20:12:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T20:12:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T20:12:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T20:12:39 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T20:12:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 2.7243, val/total_loss: 2.7243, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.4601, val/hateful_memes/roc_auc: 0.7101, num_updates: 20000, epoch: 76, iterations: 20000, max_updates: 22000, val_time: 05s 958ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.713515\n","\u001b[32m2022-04-21T20:14:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0450, train/total_loss: 0.0000, train/total_loss/avg: 0.0450, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 364ms, time_since_start: 05h 38m 29s 096ms, eta: 31m 02s 049ms\n","\u001b[32m2022-04-21T20:15:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0448, train/total_loss: 0.0000, train/total_loss/avg: 0.0448, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 356ms, time_since_start: 05h 40m 04s 452ms, eta: 29m 05s 593ms\n","\u001b[32m2022-04-21T20:17:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0446, train/total_loss: 0.0000, train/total_loss/avg: 0.0446, max mem: 10794.0, experiment: run, epoch: 77, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 787ms, time_since_start: 05h 41m 39s 240ms, eta: 27m 18s 783ms\n","\u001b[32m2022-04-21T20:19:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0444, train/total_loss: 0.0000, train/total_loss/avg: 0.0444, max mem: 10794.0, experiment: run, epoch: 77, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 325ms, time_since_start: 05h 43m 14s 566ms, eta: 25m 51s 142ms\n","\u001b[32m2022-04-21T20:20:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0441, train/total_loss: 0.0000, train/total_loss/avg: 0.0441, max mem: 10794.0, experiment: run, epoch: 78, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 995ms, time_since_start: 05h 44m 49s 561ms, eta: 24m 09s 156ms\n","\u001b[32m2022-04-21T20:22:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0439, train/total_loss: 0.0000, train/total_loss/avg: 0.0439, max mem: 10794.0, experiment: run, epoch: 78, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 284ms, time_since_start: 05h 46m 24s 846ms, eta: 22m 36s 660ms\n","\u001b[32m2022-04-21T20:23:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0437, train/total_loss: 0.0000, train/total_loss/avg: 0.0437, max mem: 10794.0, experiment: run, epoch: 78, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 328ms, time_since_start: 05h 48m 175ms, eta: 21m 344ms\n","\u001b[32m2022-04-21T20:25:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0435, train/total_loss: 0.0000, train/total_loss/avg: 0.0435, max mem: 10794.0, experiment: run, epoch: 79, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 857ms, time_since_start: 05h 49m 35s 032ms, eta: 19m 17s 635ms\n","\u001b[32m2022-04-21T20:26:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0433, train/total_loss: 0.0000, train/total_loss/avg: 0.0433, max mem: 10794.0, experiment: run, epoch: 79, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 363ms, time_since_start: 05h 51m 10s 396ms, eta: 17m 46s 837ms\n","\u001b[32m2022-04-21T20:28:32 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T20:28:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T20:28:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T20:28:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T20:28:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0431, train/total_loss: 0.0000, train/total_loss/avg: 0.0431, max mem: 10794.0, experiment: run, epoch: 79, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 58s 442ms, time_since_start: 05h 53m 08s 839ms, eta: 20m 04s 565ms\n","\u001b[32m2022-04-21T20:28:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T20:28:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T20:29:00 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T20:29:00 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T20:29:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 2.7642, val/total_loss: 2.7642, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.4430, val/hateful_memes/roc_auc: 0.7112, num_updates: 21000, epoch: 79, iterations: 21000, max_updates: 22000, val_time: 05s 680ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.713515\n","\u001b[32m2022-04-21T20:30:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0429, train/total_loss: 0.0000, train/total_loss/avg: 0.0429, max mem: 10794.0, experiment: run, epoch: 80, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 997ms, time_since_start: 05h 54m 50s 519ms, eta: 14m 38s 666ms\n","\u001b[32m2022-04-21T20:32:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0427, train/total_loss: 0.0000, train/total_loss/avg: 0.0427, max mem: 10794.0, experiment: run, epoch: 80, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 346ms, time_since_start: 05h 56m 25s 865ms, eta: 12m 55s 736ms\n","\u001b[32m2022-04-21T20:33:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0425, train/total_loss: 0.0000, train/total_loss/avg: 0.0425, max mem: 10794.0, experiment: run, epoch: 81, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 952ms, time_since_start: 05h 58m 817ms, eta: 11m 15s 968ms\n","\u001b[32m2022-04-21T20:35:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0423, train/total_loss: 0.0000, train/total_loss/avg: 0.0423, max mem: 10794.0, experiment: run, epoch: 81, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 476ms, time_since_start: 05h 59m 36s 294ms, eta: 09m 42s 599ms\n","\u001b[32m2022-04-21T20:36:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0421, train/total_loss: 0.0000, train/total_loss/avg: 0.0421, max mem: 10794.0, experiment: run, epoch: 81, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 458ms, time_since_start: 06h 01m 11s 752ms, eta: 08m 05s 404ms\n","\u001b[32m2022-04-21T20:38:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0419, train/total_loss: 0.0000, train/total_loss/avg: 0.0419, max mem: 10794.0, experiment: run, epoch: 82, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 262ms, time_since_start: 06h 02m 47s 015ms, eta: 06m 27s 528ms\n","\u001b[32m2022-04-21T20:40:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0417, train/total_loss: 0.0000, train/total_loss/avg: 0.0417, max mem: 10794.0, experiment: run, epoch: 82, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 378ms, time_since_start: 06h 04m 22s 394ms, eta: 04m 50s 999ms\n","\u001b[32m2022-04-21T20:41:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0415, train/total_loss: 0.0000, train/total_loss/avg: 0.0415, max mem: 10794.0, experiment: run, epoch: 82, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 457ms, time_since_start: 06h 05m 57s 851ms, eta: 03m 14s 159ms\n","\u001b[32m2022-04-21T20:43:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0413, train/total_loss: 0.0000, train/total_loss/avg: 0.0413, max mem: 10794.0, experiment: run, epoch: 83, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 966ms, time_since_start: 06h 07m 32s 817ms, eta: 01m 36s 580ms\n","\u001b[32m2022-04-21T20:44:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T20:44:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T20:45:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T20:45:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T20:45:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0411, train/total_loss: 0.0000, train/total_loss/avg: 0.0411, max mem: 10794.0, experiment: run, epoch: 83, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 0.84, time: 01m 59s 252ms, time_since_start: 06h 09m 32s 069ms, eta: 0ms\n","\u001b[32m2022-04-21T20:45:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T20:45:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T20:45:24 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T20:45:24 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T20:45:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/hateful_memes/cross_entropy: 2.7464, val/total_loss: 2.7464, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.4531, val/hateful_memes/roc_auc: 0.7115, num_updates: 22000, epoch: 83, iterations: 22000, max_updates: 22000, val_time: 05s 972ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.713515\n","\u001b[32m2022-04-21T20:45:25 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2022-04-21T20:45:25 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-04-21T20:45:25 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-04-21T20:45:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-21T20:45:42 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 14000\n","\u001b[32m2022-04-21T20:45:42 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 14000\n","\u001b[32m2022-04-21T20:45:42 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 53\n","\u001b[32m2022-04-21T20:45:48 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-04-21T20:45:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:19<00:00,  3.18it/s]\n","\u001b[32m2022-04-21T20:46:08 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 63\n","\u001b[32m2022-04-21T20:46:08 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T20:46:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, test/hateful_memes/cross_entropy: 2.1028, test/total_loss: 2.1028, test/hateful_memes/accuracy: 0.7155, test/hateful_memes/binary_f1: 0.5325, test/hateful_memes/roc_auc: 0.7472\n","\u001b[32m2022-04-21T20:46:08 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 06h 10m 21s 576ms\n"]}],"source":["!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml model=vilbert dataset=hateful_memes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31730,"status":"ok","timestamp":1650581606341,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"qy6a2cg_U4p1","outputId":"5cf4a3cd-8466-41b1-851d-e2bd9534f4d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-21T22:53:08 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml\n","\u001b[32m2022-04-21T22:53:08 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2022-04-21T22:53:08 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-21T22:53:08 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n","\u001b[32m2022-04-21T22:53:08 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-04-21T22:53:08 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-21T22:53:08 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-21T22:53:08 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False'])\n","\u001b[32m2022-04-21T22:53:08 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-21T22:53:08 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-04-21T22:53:08 | mmf_cli.run: \u001b[0mUsing seed 8676515\n","\u001b[32m2022-04-21T22:53:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-21T22:53:10 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T22:53:10 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T22:53:10 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T22:53:10 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bi_attention_type\": 1,\n","  \"bi_hidden_size\": 1024,\n","  \"bi_intermediate_size\": 1024,\n","  \"bi_num_attention_heads\": 8,\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"cut_first\": \"text\",\n","  \"dynamic_attention\": false,\n","  \"embedding_strategy\": \"plain\",\n","  \"fast_mode\": false,\n","  \"finetune_lr_multiplier\": 1,\n","  \"fixed_t_layer\": 0,\n","  \"fixed_v_layer\": 0,\n","  \"freeze_base\": false,\n","  \"fusion_method\": \"mul\",\n","  \"gradient_checkpointing\": false,\n","  \"hard_cap_seq_len\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"in_batch_pairs\": false,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"vilbert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_negative\": 128,\n","  \"objective\": 0,\n","  \"pad_token_id\": 0,\n","  \"pooling_method\": \"mul\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"t_biattention_id\": [\n","    6,\n","    7,\n","    8,\n","    9,\n","    10,\n","    11\n","  ],\n","  \"task_specific_tokens\": false,\n","  \"text_only\": false,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"v_attention_probs_dropout_prob\": 0.1,\n","  \"v_biattention_id\": [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","  ],\n","  \"v_feature_size\": 2048,\n","  \"v_hidden_act\": \"gelu\",\n","  \"v_hidden_dropout_prob\": 0.1,\n","  \"v_hidden_size\": 1024,\n","  \"v_initializer_range\": 0.02,\n","  \"v_intermediate_size\": 1024,\n","  \"v_num_attention_heads\": 8,\n","  \"v_num_hidden_layers\": 6,\n","  \"v_target_size\": 1601,\n","  \"visual_embedding_dim\": 2048,\n","  \"visual_target\": 0,\n","  \"visualization\": false,\n","  \"vocab_size\": 30522,\n","  \"with_coattention\": true\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.v_pooler.dense.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.t_pooler.dense.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.v_pooler.dense.weight', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-21T22:53:18 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-21T22:53:18 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-21T22:53:19 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T22:53:22 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T22:53:22 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-21T22:53:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-21T22:53:22 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 14000\n","\u001b[32m2022-04-21T22:53:22 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 14000\n","\u001b[32m2022-04-21T22:53:22 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 53\n","\u001b[32m2022-04-21T22:53:22 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-21T22:53:22 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n","  (model): ViLBERTForClassification(\n","    (bert): ViLBERTBase(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (v_embeddings): BertImageFeatureEmbeddings(\n","        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n","        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (v_layer): ModuleList(\n","          (0): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (c_layer): ModuleList(\n","          (0): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (t_pooler): BertTextPooler(\n","        (dense): Linear(in_features=768, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","      (v_pooler): BertImagePooler(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=1024, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-21T22:53:22 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n","\u001b[32m2022-04-21T22:53:22 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2022-04-21T22:53:22 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 17/17 [00:05<00:00,  3.06it/s]\n","\u001b[32m2022-04-21T22:53:28 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T22:53:28 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T22:53:28 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 2.2292, val/total_loss: 2.2292, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.5165, val/hateful_memes/roc_auc: 0.7135\n","\u001b[32m2022-04-21T22:53:28 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 09s 275ms\n"]}],"source":["!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml\" \\\n","model=vilbert \\\n","dataset=hateful_memes \\\n","run_type=val \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":360954,"status":"ok","timestamp":1650904907480,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"nVeTBtrgkVgi","outputId":"8818e224-7d9a-4bcd-d3c9-d3df61eb6025"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-25T16:35:51 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml\n","\u001b[32m2022-04-25T16:35:51 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2022-04-25T16:35:51 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-25T16:35:51 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n","\u001b[32m2022-04-25T16:35:51 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-04-25T16:35:51 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-25T16:35:51 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-25T16:35:51 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False'])\n","\u001b[32m2022-04-25T16:35:51 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-25T16:35:51 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n","\u001b[32m2022-04-25T16:35:51 | mmf_cli.run: \u001b[0mUsing seed 51823477\n","\u001b[32m2022-04-25T16:35:51 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [00:59<00:00, 172MB/s] \n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 143kB/s]  \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9c9z9ygi\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 22.4kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqvr0p53m\n","Downloading: 100% 570/570 [00:00<00:00, 491kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvzt8kxq5\n","Downloading: 100% 232k/232k [00:00<00:00, 268kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvhys8b6k\n","Downloading: 100% 466k/466k [00:01<00:00, 349kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-25T16:39:48 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T16:39:48 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T16:39:48 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T16:39:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bi_attention_type\": 1,\n","  \"bi_hidden_size\": 1024,\n","  \"bi_intermediate_size\": 1024,\n","  \"bi_num_attention_heads\": 8,\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"cut_first\": \"text\",\n","  \"dynamic_attention\": false,\n","  \"embedding_strategy\": \"plain\",\n","  \"fast_mode\": false,\n","  \"finetune_lr_multiplier\": 1,\n","  \"fixed_t_layer\": 0,\n","  \"fixed_v_layer\": 0,\n","  \"freeze_base\": false,\n","  \"fusion_method\": \"mul\",\n","  \"gradient_checkpointing\": false,\n","  \"hard_cap_seq_len\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"in_batch_pairs\": false,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"vilbert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_negative\": 128,\n","  \"objective\": 0,\n","  \"pad_token_id\": 0,\n","  \"pooling_method\": \"mul\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"t_biattention_id\": [\n","    6,\n","    7,\n","    8,\n","    9,\n","    10,\n","    11\n","  ],\n","  \"task_specific_tokens\": false,\n","  \"text_only\": false,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"v_attention_probs_dropout_prob\": 0.1,\n","  \"v_biattention_id\": [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","  ],\n","  \"v_feature_size\": 2048,\n","  \"v_hidden_act\": \"gelu\",\n","  \"v_hidden_dropout_prob\": 0.1,\n","  \"v_hidden_size\": 1024,\n","  \"v_initializer_range\": 0.02,\n","  \"v_intermediate_size\": 1024,\n","  \"v_num_attention_heads\": 8,\n","  \"v_num_hidden_layers\": 6,\n","  \"v_target_size\": 1601,\n","  \"visual_embedding_dim\": 2048,\n","  \"visual_target\": 0,\n","  \"visualization\": false,\n","  \"vocab_size\": 30522,\n","  \"with_coattention\": true\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmpcdxfj4bg\n","Downloading: 100% 440M/440M [00:05<00:00, 73.8MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.v_pooler.dense.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.t_pooler.dense.weight', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.v_pooler.dense.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.v_layer.4.attention.self.value.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-25T16:40:04 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-25T16:40:04 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-25T16:40:05 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-25T16:41:13 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-25T16:41:13 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-25T16:41:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-25T16:41:13 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 14000\n","\u001b[32m2022-04-25T16:41:13 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 14000\n","\u001b[32m2022-04-25T16:41:13 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 53\n","\u001b[32m2022-04-25T16:41:13 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-25T16:41:13 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n","  (model): ViLBERTForClassification(\n","    (bert): ViLBERTBase(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (v_embeddings): BertImageFeatureEmbeddings(\n","        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n","        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (v_layer): ModuleList(\n","          (0): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (c_layer): ModuleList(\n","          (0): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (t_pooler): BertTextPooler(\n","        (dense): Linear(in_features=768, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","      (v_pooler): BertImagePooler(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=1024, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-25T16:41:14 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n","\u001b[32m2022-04-25T16:41:14 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-04-25T16:41:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:31<00:00,  2.02it/s]\n","\u001b[32m2022-04-25T16:41:45 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 63\n","\u001b[32m2022-04-25T16:41:45 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T16:41:45 | mmf.trainers.callbacks.logistics: \u001b[0mtest/hateful_memes/cross_entropy: 2.1028, test/total_loss: 2.1028, test/hateful_memes/accuracy: 0.7155, test/hateful_memes/binary_f1: 0.5325, test/hateful_memes/roc_auc: 0.7472\n","\u001b[32m2022-04-25T16:41:45 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01m 40s 157ms\n"]}],"source":["!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml\" \\\n","model=vilbert \\\n","dataset=hateful_memes \\\n","run_type=test \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","source":["!mmf_predict config=\"mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml\" \\\n","model=vilbert \\\n","dataset=hateful_memes \\\n","run_type=test \\\n","checkpoint.resume_file=save_vilbert_cc/best.ckpt \\\n","checkpoint.resume_pretrained=False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HoDvVtNOfgaZ","executionInfo":{"status":"ok","timestamp":1651513294330,"user_tz":300,"elapsed":67422,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"}},"outputId":"6c1a71bc-295f-4471-a1df-7be97f395ab7"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-02T17:40:42 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml\n","\u001b[32m2022-05-02T17:40:42 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2022-05-02T17:40:42 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-05-02T17:40:42 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n","\u001b[32m2022-05-02T17:40:42 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save_vilbert_cc/best.ckpt\n","\u001b[32m2022-05-02T17:40:42 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","\u001b[32m2022-05-02T17:40:42 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-02T17:40:42 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-02T17:40:42 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save_vilbert_cc/best.ckpt', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n","\u001b[32m2022-05-02T17:40:42 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-02T17:40:42 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-05-02T17:40:42 | mmf_cli.run: \u001b[0mUsing seed 42334753\n","\u001b[32m2022-05-02T17:40:42 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-02T17:40:46 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T17:40:46 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T17:40:46 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T17:40:46 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bi_attention_type\": 1,\n","  \"bi_hidden_size\": 1024,\n","  \"bi_intermediate_size\": 1024,\n","  \"bi_num_attention_heads\": 8,\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"cut_first\": \"text\",\n","  \"dynamic_attention\": false,\n","  \"embedding_strategy\": \"plain\",\n","  \"fast_mode\": false,\n","  \"finetune_lr_multiplier\": 1,\n","  \"fixed_t_layer\": 0,\n","  \"fixed_v_layer\": 0,\n","  \"freeze_base\": false,\n","  \"fusion_method\": \"mul\",\n","  \"gradient_checkpointing\": false,\n","  \"hard_cap_seq_len\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"in_batch_pairs\": false,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"vilbert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_negative\": 128,\n","  \"objective\": 0,\n","  \"pad_token_id\": 0,\n","  \"pooling_method\": \"mul\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"t_biattention_id\": [\n","    6,\n","    7,\n","    8,\n","    9,\n","    10,\n","    11\n","  ],\n","  \"task_specific_tokens\": false,\n","  \"text_only\": false,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"v_attention_probs_dropout_prob\": 0.1,\n","  \"v_biattention_id\": [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","  ],\n","  \"v_feature_size\": 2048,\n","  \"v_hidden_act\": \"gelu\",\n","  \"v_hidden_dropout_prob\": 0.1,\n","  \"v_hidden_size\": 1024,\n","  \"v_initializer_range\": 0.02,\n","  \"v_intermediate_size\": 1024,\n","  \"v_num_attention_heads\": 8,\n","  \"v_num_hidden_layers\": 6,\n","  \"v_target_size\": 1601,\n","  \"visual_embedding_dim\": 2048,\n","  \"visual_target\": 0,\n","  \"visualization\": false,\n","  \"vocab_size\": 30522,\n","  \"with_coattention\": true\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_pooler.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.v_pooler.dense.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.t_pooler.dense.weight', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.2.v_output.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-05-02T17:40:54 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-02T17:40:54 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-02T17:40:54 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T17:41:16 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T17:41:16 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-05-02T17:41:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-02T17:41:16 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 14000\n","\u001b[32m2022-05-02T17:41:16 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 14000\n","\u001b[32m2022-05-02T17:41:16 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 53\n","\u001b[32m2022-05-02T17:41:16 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n","\u001b[32m2022-05-02T17:41:16 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:17<00:00,  3.70it/s]\n","\u001b[32m2022-05-02T17:41:33 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes/save/hateful_memes_vilbert_42334753/reports/hateful_memes_run_test_2022-05-02T17:41:33.csv\n","\u001b[32m2022-05-02T17:41:33 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 63\n","\u001b[32m2022-05-02T17:41:33 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"]}]},{"cell_type":"code","source":["!mmf_predict config=\"mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml\" \\\n","model=vilbert \\\n","dataset=hateful_memes \\\n","run_type=val \\\n","checkpoint.resume_file=save_vilbert_cc/best.ckpt \\\n","checkpoint.resume_pretrained=False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZlkP8CafxxY","executionInfo":{"status":"ok","timestamp":1651513318744,"user_tz":300,"elapsed":24415,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"}},"outputId":"ef48a31f-dfde-40dc-c349-229141dbf4b9"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-02T17:41:39 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml\n","\u001b[32m2022-05-02T17:41:39 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2022-05-02T17:41:39 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-05-02T17:41:39 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n","\u001b[32m2022-05-02T17:41:39 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save_vilbert_cc/best.ckpt\n","\u001b[32m2022-05-02T17:41:39 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","\u001b[32m2022-05-02T17:41:39 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-02T17:41:40 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-02T17:41:40 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/from_cc.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save_vilbert_cc/best.ckpt', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n","\u001b[32m2022-05-02T17:41:40 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-02T17:41:40 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-05-02T17:41:40 | mmf_cli.run: \u001b[0mUsing seed 40088199\n","\u001b[32m2022-05-02T17:41:40 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-02T17:41:43 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T17:41:43 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T17:41:43 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T17:41:43 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bi_attention_type\": 1,\n","  \"bi_hidden_size\": 1024,\n","  \"bi_intermediate_size\": 1024,\n","  \"bi_num_attention_heads\": 8,\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"cut_first\": \"text\",\n","  \"dynamic_attention\": false,\n","  \"embedding_strategy\": \"plain\",\n","  \"fast_mode\": false,\n","  \"finetune_lr_multiplier\": 1,\n","  \"fixed_t_layer\": 0,\n","  \"fixed_v_layer\": 0,\n","  \"freeze_base\": false,\n","  \"fusion_method\": \"mul\",\n","  \"gradient_checkpointing\": false,\n","  \"hard_cap_seq_len\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"in_batch_pairs\": false,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"vilbert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_negative\": 128,\n","  \"objective\": 0,\n","  \"pad_token_id\": 0,\n","  \"pooling_method\": \"mul\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"t_biattention_id\": [\n","    6,\n","    7,\n","    8,\n","    9,\n","    10,\n","    11\n","  ],\n","  \"task_specific_tokens\": false,\n","  \"text_only\": false,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"v_attention_probs_dropout_prob\": 0.1,\n","  \"v_biattention_id\": [\n","    0,\n","    1,\n","    2,\n","    3,\n","    4,\n","    5\n","  ],\n","  \"v_feature_size\": 2048,\n","  \"v_hidden_act\": \"gelu\",\n","  \"v_hidden_dropout_prob\": 0.1,\n","  \"v_hidden_size\": 1024,\n","  \"v_initializer_range\": 0.02,\n","  \"v_intermediate_size\": 1024,\n","  \"v_num_attention_heads\": 8,\n","  \"v_num_hidden_layers\": 6,\n","  \"v_target_size\": 1601,\n","  \"visual_embedding_dim\": 2048,\n","  \"visual_target\": 0,\n","  \"visualization\": false,\n","  \"vocab_size\": 30522,\n","  \"with_coattention\": true\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.v_pooler.dense.bias', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_pooler.dense.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.t_pooler.dense.weight', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-05-02T17:41:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-02T17:41:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-02T17:41:50 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T17:41:53 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T17:41:53 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-05-02T17:41:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-02T17:41:53 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 14000\n","\u001b[32m2022-05-02T17:41:53 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 14000\n","\u001b[32m2022-05-02T17:41:53 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 53\n","\u001b[32m2022-05-02T17:41:53 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n","\u001b[32m2022-05-02T17:41:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 17/17 [00:04<00:00,  3.54it/s]\n","\u001b[32m2022-05-02T17:41:58 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes/save/hateful_memes_vilbert_40088199/reports/hateful_memes_run_val_2022-05-02T17:41:58.csv\n","\u001b[32m2022-05-02T17:41:58 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 17\n","\u001b[32m2022-05-02T17:41:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"]}]},{"cell_type":"markdown","metadata":{"id":"QHZck_hnpQJw"},"source":["## **MMBT section**"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5075271,"status":"ok","timestamp":1651574827351,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"vNXQqoBOpTnm","outputId":"b9f28362-3d11-482b-ef56-9fbf9cc00a72"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-03T08:30:31 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml\n","\u001b[32m2022-05-03T08:30:31 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n","\u001b[32m2022-05-03T08:30:31 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-03T08:30:31 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-03T08:30:31 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml', 'model=mmbt', 'dataset=hateful_memes'])\n","\u001b[32m2022-05-03T08:30:31 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-03T08:30:31 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-05-03T08:30:31 | mmf_cli.run: \u001b[0mUsing seed 31732289\n","\u001b[32m2022-05-03T08:30:31 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 152kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpimim7xz6\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 24.9kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpnn61hri5\n","Downloading: 100% 570/570 [00:00<00:00, 439kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpw0r6ci5u\n","Downloading: 100% 232k/232k [00:00<00:00, 688kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpq4iex2pe\n","Downloading: 100% 466k/466k [00:00<00:00, 1.10MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-03T08:30:39 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T08:30:39 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T08:30:39 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T08:30:39 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdc45mxsx\n","Downloading: 100% 440M/440M [00:06<00:00, 63.8MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelJit: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModelJit from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModelJit from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertModelJit were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelJit for predictions without further training.\n","Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n","100% 230M/230M [00:03<00:00, 63.4MB/s]\n","\u001b[32m2022-05-03T08:30:59 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-03T08:30:59 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-03T08:30:59 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-05-03T08:30:59 | mmf.trainers.mmf_trainer: \u001b[0mMMBT(\n","  (model): MMBTForClassification(\n","    (bert): MMBTBase(\n","      (mmbt): MMBTModel(\n","        (transformer): BertModelJit(\n","          (embeddings): BertEmbeddingsJit(\n","            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","            (position_embeddings): Embedding(512, 768)\n","            (token_type_embeddings): Embedding(2, 768)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (encoder): BertEncoderJit(\n","            (layer): ModuleList(\n","              (0): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (1): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (2): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (3): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (4): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (5): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (6): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (7): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (8): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (9): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (10): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (11): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","            )\n","          )\n","          (pooler): BertPooler(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (activation): Tanh()\n","          )\n","        )\n","        (modal_encoder): ModalEmbeddings(\n","          (encoder): ResNet152ImageEncoder(\n","            (model): Sequential(\n","              (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): ReLU(inplace=True)\n","              (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","              (4): Sequential(\n","                (0): Bottleneck(\n","                  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                  (downsample): Sequential(\n","                    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  )\n","                )\n","                (1): Bottleneck(\n","                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (2): Bottleneck(\n","                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","              )\n","              (5): Sequential(\n","                (0): Bottleneck(\n","                  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                  (downsample): Sequential(\n","                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  )\n","                )\n","                (1): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (2): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (3): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (4): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (5): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (6): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (7): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","              )\n","              (6): Sequential(\n","                (0): Bottleneck(\n","                  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                  (downsample): Sequential(\n","                    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  )\n","                )\n","                (1): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (2): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (3): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (4): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (5): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (6): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (7): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (8): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (9): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (10): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (11): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (12): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (13): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (14): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (15): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (16): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (17): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (18): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (19): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (20): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (21): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (22): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (23): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (24): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (25): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (26): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (27): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (28): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (29): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (30): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (31): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (32): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (33): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (34): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (35): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","              )\n","              (7): Sequential(\n","                (0): Bottleneck(\n","                  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                  (downsample): Sequential(\n","                    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  )\n","                )\n","                (1): Bottleneck(\n","                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (2): Bottleneck(\n","                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","              )\n","            )\n","            (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n","          )\n","          (proj_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n","          (position_embeddings): Embedding(512, 768)\n","          (token_type_embeddings): Embedding(2, 768)\n","          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-05-03T08:30:59 | mmf.utils.general: \u001b[0mTotal Parameters: 169793346. Trained Parameters: 169793346\n","\u001b[32m2022-05-03T08:30:59 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T08:31:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T08:31:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","\n","\u001b[32m2022-05-03T08:31:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.7619, train/hateful_memes/cross_entropy/avg: 0.7619, train/total_loss: 0.7619, train/total_loss/avg: 0.7619, max mem: 11656.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.82, time: 55s 087ms, time_since_start: 55s 184ms, eta: 03h 24m 29s 357ms\n","\u001b[32m2022-05-03T08:32:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6621, train/hateful_memes/cross_entropy/avg: 0.7120, train/total_loss: 0.6621, train/total_loss/avg: 0.7120, max mem: 11656.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 251ms, time_since_start: 01m 49s 435ms, eta: 03h 20m 27s 944ms\n","\u001b[32m2022-05-03T08:33:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.7167, train/hateful_memes/cross_entropy/avg: 0.7135, train/total_loss: 0.7167, train/total_loss/avg: 0.7135, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 544ms, time_since_start: 02m 43s 980ms, eta: 03h 20m 37s 471ms\n","\u001b[32m2022-05-03T08:34:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.7007, train/hateful_memes/cross_entropy/avg: 0.7103, train/total_loss: 0.7007, train/total_loss/avg: 0.7103, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 276ms, time_since_start: 03m 38s 257ms, eta: 03h 18m 43s 090ms\n","\u001b[32m2022-05-03T08:35:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.7007, train/hateful_memes/cross_entropy/avg: 0.6707, train/total_loss: 0.7007, train/total_loss/avg: 0.6707, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 358ms, time_since_start: 04m 32s 615ms, eta: 03h 18m 05s 720ms\n","\u001b[32m2022-05-03T08:36:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.6621, train/hateful_memes/cross_entropy/avg: 0.6476, train/total_loss: 0.6621, train/total_loss/avg: 0.6476, max mem: 11656.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 678ms, time_since_start: 05m 27s 294ms, eta: 03h 18m 20s 014ms\n","\u001b[32m2022-05-03T08:37:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6621, train/hateful_memes/cross_entropy/avg: 0.6358, train/total_loss: 0.6621, train/total_loss/avg: 0.6358, max mem: 11656.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 417ms, time_since_start: 06m 21s 711ms, eta: 03h 16m 28s 058ms\n","\u001b[32m2022-05-03T08:38:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.6550, train/hateful_memes/cross_entropy/avg: 0.6382, train/total_loss: 0.6550, train/total_loss/avg: 0.6382, max mem: 11656.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 647ms, time_since_start: 07m 16s 359ms, eta: 03h 16m 22s 139ms\n","\u001b[32m2022-05-03T08:39:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.6550, train/hateful_memes/cross_entropy/avg: 0.6389, train/total_loss: 0.6550, train/total_loss/avg: 0.6389, max mem: 11656.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 498ms, time_since_start: 08m 10s 857ms, eta: 03h 14m 54s 734ms\n","\u001b[32m2022-05-03T08:40:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T08:40:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T08:40:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T08:40:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T08:40:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.6444, train/hateful_memes/cross_entropy/avg: 0.6358, train/total_loss: 0.6444, train/total_loss/avg: 0.6358, max mem: 11656.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00001, ups: 1.47, time: 01m 08s 497ms, time_since_start: 09m 19s 355ms, eta: 04h 03m 48s 985ms\n","\u001b[32m2022-05-03T08:40:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T08:40:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T08:40:22 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T08:40:22 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T08:40:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T08:40:29 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-03T08:40:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T08:40:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T08:40:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.7253, val/total_loss: 0.7253, val/hateful_memes/accuracy: 0.6315, val/hateful_memes/binary_f1: 0.3066, val/hateful_memes/roc_auc: 0.5985, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 30s 151ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.598456\n","\u001b[32m2022-05-03T08:41:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.6444, train/hateful_memes/cross_entropy/avg: 0.6295, train/total_loss: 0.6444, train/total_loss/avg: 0.6295, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 650ms, time_since_start: 10m 45s 158ms, eta: 03h 17m 08s 742ms\n","\u001b[32m2022-05-03T08:42:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.6079, train/hateful_memes/cross_entropy/avg: 0.6157, train/total_loss: 0.6079, train/total_loss/avg: 0.6157, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 369ms, time_since_start: 11m 39s 528ms, eta: 03h 11m 41s 084ms\n","\u001b[32m2022-05-03T08:43:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.6079, train/hateful_memes/cross_entropy/avg: 0.6080, train/total_loss: 0.6079, train/total_loss/avg: 0.6080, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 338ms, time_since_start: 12m 33s 867ms, eta: 03h 10m 39s 344ms\n","\u001b[32m2022-05-03T08:44:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.5659, train/hateful_memes/cross_entropy/avg: 0.6011, train/total_loss: 0.5659, train/total_loss/avg: 0.6011, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 726ms, time_since_start: 13m 28s 593ms, eta: 03h 11m 05s 396ms\n","\u001b[32m2022-05-03T08:45:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.5659, train/hateful_memes/cross_entropy/avg: 0.5795, train/total_loss: 0.5659, train/total_loss/avg: 0.5795, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 402ms, time_since_start: 14m 22s 996ms, eta: 03h 09m 02s 193ms\n","\u001b[32m2022-05-03T08:46:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.5652, train/hateful_memes/cross_entropy/avg: 0.5572, train/total_loss: 0.5652, train/total_loss/avg: 0.5572, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 683ms, time_since_start: 15m 17s 680ms, eta: 03h 09m 05s 118ms\n","\u001b[32m2022-05-03T08:47:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.5652, train/hateful_memes/cross_entropy/avg: 0.5438, train/total_loss: 0.5652, train/total_loss/avg: 0.5438, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 446ms, time_since_start: 16m 12s 126ms, eta: 03h 07m 20s 503ms\n","\u001b[32m2022-05-03T08:48:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.5319, train/hateful_memes/cross_entropy/avg: 0.5244, train/total_loss: 0.5319, train/total_loss/avg: 0.5244, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 331ms, time_since_start: 17m 06s 458ms, eta: 03h 06m 01s 493ms\n","\u001b[32m2022-05-03T08:49:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.5319, train/hateful_memes/cross_entropy/avg: 0.5045, train/total_loss: 0.5319, train/total_loss/avg: 0.5045, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 955ms, time_since_start: 18m 01s 413ms, eta: 03h 07m 13s 869ms\n","\u001b[32m2022-05-03T08:49:55 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T08:49:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T08:50:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T08:50:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T08:50:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.5161, train/hateful_memes/cross_entropy/avg: 0.4973, train/total_loss: 0.5161, train/total_loss/avg: 0.4973, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 1.47, time: 01m 08s 075ms, time_since_start: 19m 09s 489ms, eta: 03h 50m 46s 648ms\n","\u001b[32m2022-05-03T08:50:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T08:50:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T08:50:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T08:50:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T08:50:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T08:50:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T08:50:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T08:50:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.1800, val/total_loss: 1.1800, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.3699, val/hateful_memes/roc_auc: 0.5921, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 19s 292ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.598456\n","\u001b[32m2022-05-03T08:51:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.5124, train/hateful_memes/cross_entropy/avg: 0.4777, train/total_loss: 0.5124, train/total_loss/avg: 0.4777, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 050ms, time_since_start: 20m 23s 835ms, eta: 03h 05m 41s 344ms\n","\u001b[32m2022-05-03T08:52:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.5111, train/hateful_memes/cross_entropy/avg: 0.4611, train/total_loss: 0.5111, train/total_loss/avg: 0.4611, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 953ms, time_since_start: 21m 18s 788ms, eta: 03h 04m 25s 750ms\n","\u001b[32m2022-05-03T08:53:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.4641, train/hateful_memes/cross_entropy/avg: 0.4464, train/total_loss: 0.4641, train/total_loss/avg: 0.4464, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 546ms, time_since_start: 22m 13s 335ms, eta: 03h 02m 08s 375ms\n","\u001b[32m2022-05-03T08:54:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.3592, train/hateful_memes/cross_entropy/avg: 0.4323, train/total_loss: 0.3592, train/total_loss/avg: 0.4323, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 924ms, time_since_start: 23m 08s 259ms, eta: 03h 02m 28s 230ms\n","\u001b[32m2022-05-03T08:55:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.3286, train/hateful_memes/cross_entropy/avg: 0.4173, train/total_loss: 0.3286, train/total_loss/avg: 0.4173, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 419ms, time_since_start: 24m 02s 679ms, eta: 02h 59m 52s 232ms\n","\u001b[32m2022-05-03T08:55:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.2769, train/hateful_memes/cross_entropy/avg: 0.4016, train/total_loss: 0.2769, train/total_loss/avg: 0.4016, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 466ms, time_since_start: 24m 57s 145ms, eta: 02h 59m 06s 045ms\n","\u001b[32m2022-05-03T08:56:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.2236, train/hateful_memes/cross_entropy/avg: 0.3875, train/total_loss: 0.2236, train/total_loss/avg: 0.3875, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 761ms, time_since_start: 25m 51s 907ms, eta: 02h 59m 08s 708ms\n","\u001b[32m2022-05-03T08:57:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.1939, train/hateful_memes/cross_entropy/avg: 0.3782, train/total_loss: 0.1939, train/total_loss/avg: 0.3782, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 504ms, time_since_start: 26m 46s 411ms, eta: 02h 57m 22s 714ms\n","\u001b[32m2022-05-03T08:58:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.1476, train/hateful_memes/cross_entropy/avg: 0.3669, train/total_loss: 0.1476, train/total_loss/avg: 0.3669, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 426ms, time_since_start: 27m 40s 838ms, eta: 02h 56m 12s 204ms\n","\u001b[32m2022-05-03T08:59:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T08:59:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T08:59:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T08:59:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T08:59:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.1270, train/hateful_memes/cross_entropy/avg: 0.3550, train/total_loss: 0.1270, train/total_loss/avg: 0.3550, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 1.45, time: 01m 09s 409ms, time_since_start: 28m 50s 247ms, eta: 03h 43m 31s 991ms\n","\u001b[32m2022-05-03T08:59:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T08:59:49 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T08:59:52 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T08:59:52 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T08:59:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T08:59:59 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-03T09:00:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:00:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:00:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 2.0143, val/total_loss: 2.0143, val/hateful_memes/accuracy: 0.6315, val/hateful_memes/binary_f1: 0.2657, val/hateful_memes/roc_auc: 0.6036, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 27s 459ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.603577\n","\u001b[32m2022-05-03T09:01:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.1239, train/hateful_memes/cross_entropy/avg: 0.3444, train/total_loss: 0.1239, train/total_loss/avg: 0.3444, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 103ms, time_since_start: 30m 12s 812ms, eta: 02h 56m 31s 611ms\n","\u001b[32m2022-05-03T09:02:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.1117, train/hateful_memes/cross_entropy/avg: 0.3347, train/total_loss: 0.1117, train/total_loss/avg: 0.3347, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 068ms, time_since_start: 31m 07s 880ms, eta: 02h 55m 28s 863ms\n","\u001b[32m2022-05-03T09:03:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.1088, train/hateful_memes/cross_entropy/avg: 0.3253, train/total_loss: 0.1088, train/total_loss/avg: 0.3253, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 443ms, time_since_start: 32m 02s 323ms, eta: 02h 52m 33s 930ms\n","\u001b[32m2022-05-03T09:03:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0860, train/hateful_memes/cross_entropy/avg: 0.3162, train/total_loss: 0.0860, train/total_loss/avg: 0.3162, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 371ms, time_since_start: 32m 56s 695ms, eta: 02h 51m 25s 049ms\n","\u001b[32m2022-05-03T09:04:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0559, train/hateful_memes/cross_entropy/avg: 0.3072, train/total_loss: 0.0559, train/total_loss/avg: 0.3072, max mem: 11667.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 839ms, time_since_start: 33m 51s 534ms, eta: 02h 51m 57s 705ms\n","\u001b[32m2022-05-03T09:05:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0514, train/hateful_memes/cross_entropy/avg: 0.2988, train/total_loss: 0.0514, train/total_loss/avg: 0.2988, max mem: 11667.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 469ms, time_since_start: 34m 46s 004ms, eta: 02h 49m 52s 796ms\n","\u001b[32m2022-05-03T09:06:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0342, train/hateful_memes/cross_entropy/avg: 0.2908, train/total_loss: 0.0342, train/total_loss/avg: 0.2908, max mem: 11667.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 455ms, time_since_start: 35m 40s 460ms, eta: 02h 48m 54s 796ms\n","\u001b[32m2022-05-03T09:07:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0289, train/hateful_memes/cross_entropy/avg: 0.2833, train/total_loss: 0.0289, train/total_loss/avg: 0.2833, max mem: 11667.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 763ms, time_since_start: 36m 35s 223ms, eta: 02h 48m 56s 351ms\n","\u001b[32m2022-05-03T09:08:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0289, train/hateful_memes/cross_entropy/avg: 0.2774, train/total_loss: 0.0289, train/total_loss/avg: 0.2774, max mem: 11667.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 495ms, time_since_start: 37m 29s 718ms, eta: 02h 47m 11s 342ms\n","\u001b[32m2022-05-03T09:09:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T09:09:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T09:09:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:09:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:09:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0289, train/hateful_memes/cross_entropy/avg: 0.2713, train/total_loss: 0.0289, train/total_loss/avg: 0.2713, max mem: 11667.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 1.47, time: 01m 08s 552ms, time_since_start: 38m 38s 271ms, eta: 03h 29m 09s 258ms\n","\u001b[32m2022-05-03T09:09:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T09:09:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T09:09:41 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T09:09:41 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T09:09:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T09:09:47 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-03T09:09:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:10:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:10:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 2.0545, val/total_loss: 2.0545, val/hateful_memes/accuracy: 0.6407, val/hateful_memes/binary_f1: 0.2707, val/hateful_memes/roc_auc: 0.6182, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 27s 856ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.618191\n","\u001b[32m2022-05-03T09:11:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0245, train/hateful_memes/cross_entropy/avg: 0.2648, train/total_loss: 0.0245, train/total_loss/avg: 0.2648, max mem: 11667.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 160ms, time_since_start: 40m 01s 290ms, eta: 02h 47m 21s 626ms\n","\u001b[32m2022-05-03T09:11:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0245, train/hateful_memes/cross_entropy/avg: 0.2600, train/total_loss: 0.0245, train/total_loss/avg: 0.2600, max mem: 11667.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 501ms, time_since_start: 40m 55s 791ms, eta: 02h 44m 26s 225ms\n","\u001b[32m2022-05-03T09:12:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0245, train/hateful_memes/cross_entropy/avg: 0.2554, train/total_loss: 0.0245, train/total_loss/avg: 0.2554, max mem: 11667.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 855ms, time_since_start: 41m 50s 646ms, eta: 02h 44m 34s 399ms\n","\u001b[32m2022-05-03T09:13:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0199, train/hateful_memes/cross_entropy/avg: 0.2496, train/total_loss: 0.0199, train/total_loss/avg: 0.2496, max mem: 11667.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 452ms, time_since_start: 42m 45s 099ms, eta: 02h 42m 26s 518ms\n","\u001b[32m2022-05-03T09:14:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0199, train/hateful_memes/cross_entropy/avg: 0.2464, train/total_loss: 0.0199, train/total_loss/avg: 0.2464, max mem: 11667.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 446ms, time_since_start: 43m 39s 545ms, eta: 02h 41m 30s 191ms\n","\u001b[32m2022-05-03T09:15:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0245, train/hateful_memes/cross_entropy/avg: 0.2416, train/total_loss: 0.0245, train/total_loss/avg: 0.2416, max mem: 11667.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 823ms, time_since_start: 44m 34s 369ms, eta: 02h 41m 41s 391ms\n","\u001b[32m2022-05-03T09:16:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0273, train/hateful_memes/cross_entropy/avg: 0.2374, train/total_loss: 0.0273, train/total_loss/avg: 0.2374, max mem: 11667.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 447ms, time_since_start: 45m 28s 817ms, eta: 02h 39m 39s 620ms\n","\u001b[32m2022-05-03T09:17:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0245, train/hateful_memes/cross_entropy/avg: 0.2325, train/total_loss: 0.0245, train/total_loss/avg: 0.2325, max mem: 11667.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 705ms, time_since_start: 46m 23s 522ms, eta: 02h 39m 29s 361ms\n","\u001b[32m2022-05-03T09:18:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0141, train/hateful_memes/cross_entropy/avg: 0.2280, train/total_loss: 0.0141, train/total_loss/avg: 0.2280, max mem: 11667.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 229ms, time_since_start: 47m 17s 752ms, eta: 02h 37m 10s 969ms\n","\u001b[32m2022-05-03T09:19:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T09:19:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T09:19:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:19:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:19:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0141, train/hateful_memes/cross_entropy/avg: 0.2234, train/total_loss: 0.0141, train/total_loss/avg: 0.2234, max mem: 11667.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 1.41, time: 01m 11s 894ms, time_since_start: 48m 29s 647ms, eta: 03h 27m 09s 893ms\n","\u001b[32m2022-05-03T09:19:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T09:19:28 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T09:19:32 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T09:19:32 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T09:19:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T09:19:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:19:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:19:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 2.1521, val/total_loss: 2.1521, val/hateful_memes/accuracy: 0.6370, val/hateful_memes/binary_f1: 0.3636, val/hateful_memes/roc_auc: 0.6062, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 24s 820ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.618191\n","\u001b[32m2022-05-03T09:20:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0141, train/hateful_memes/cross_entropy/avg: 0.2194, train/total_loss: 0.0141, train/total_loss/avg: 0.2194, max mem: 11667.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 367ms, time_since_start: 49m 49s 837ms, eta: 02h 38m 36s 118ms\n","\u001b[32m2022-05-03T09:21:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0141, train/hateful_memes/cross_entropy/avg: 0.2159, train/total_loss: 0.0141, train/total_loss/avg: 0.2159, max mem: 11667.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 379ms, time_since_start: 50m 44s 217ms, eta: 02h 34m 51s 139ms\n","\u001b[32m2022-05-03T09:22:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0141, train/hateful_memes/cross_entropy/avg: 0.2123, train/total_loss: 0.0141, train/total_loss/avg: 0.2123, max mem: 11667.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 405ms, time_since_start: 51m 38s 622ms, eta: 02h 34m 106ms\n","\u001b[32m2022-05-03T09:23:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0114, train/hateful_memes/cross_entropy/avg: 0.2084, train/total_loss: 0.0114, train/total_loss/avg: 0.2084, max mem: 11667.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 738ms, time_since_start: 52m 33s 360ms, eta: 02h 34m 01s 033ms\n","\u001b[32m2022-05-03T09:24:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0192, train/hateful_memes/cross_entropy/avg: 0.2074, train/total_loss: 0.0192, train/total_loss/avg: 0.2074, max mem: 11667.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 387ms, time_since_start: 53m 27s 748ms, eta: 02h 32m 06s 519ms\n","\u001b[32m2022-05-03T09:25:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0244, train/hateful_memes/cross_entropy/avg: 0.2044, train/total_loss: 0.0244, train/total_loss/avg: 0.2044, max mem: 11667.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 814ms, time_since_start: 54m 22s 562ms, eta: 02h 32m 22s 328ms\n","\u001b[32m2022-05-03T09:26:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0244, train/hateful_memes/cross_entropy/avg: 0.2008, train/total_loss: 0.0244, train/total_loss/avg: 0.2008, max mem: 11667.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 356ms, time_since_start: 55m 16s 919ms, eta: 02h 30m 10s 788ms\n","\u001b[32m2022-05-03T09:27:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0273, train/hateful_memes/cross_entropy/avg: 0.1979, train/total_loss: 0.0273, train/total_loss/avg: 0.1979, max mem: 11667.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 421ms, time_since_start: 56m 11s 341ms, eta: 02h 29m 26s 240ms\n","\u001b[32m2022-05-03T09:28:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0244, train/hateful_memes/cross_entropy/avg: 0.1946, train/total_loss: 0.0244, train/total_loss/avg: 0.1946, max mem: 11667.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 748ms, time_since_start: 57m 06s 089ms, eta: 02h 29m 24s 285ms\n","\u001b[32m2022-05-03T09:28:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T09:28:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T09:29:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:29:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:29:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0244, train/hateful_memes/cross_entropy/avg: 0.1918, train/total_loss: 0.0244, train/total_loss/avg: 0.1918, max mem: 11667.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 1.45, time: 01m 09s 958ms, time_since_start: 58m 16s 048ms, eta: 03h 09m 43s 717ms\n","\u001b[32m2022-05-03T09:29:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T09:29:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T09:29:18 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T09:29:18 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T09:29:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T09:29:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:29:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:29:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.4332, val/total_loss: 2.4332, val/hateful_memes/accuracy: 0.6333, val/hateful_memes/binary_f1: 0.2929, val/hateful_memes/roc_auc: 0.5965, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 18s 168ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.618191\n","\u001b[32m2022-05-03T09:30:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0244, train/hateful_memes/cross_entropy/avg: 0.1887, train/total_loss: 0.0244, train/total_loss/avg: 0.1887, max mem: 11667.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 987ms, time_since_start: 59m 29s 205ms, eta: 02h 28m 11s 686ms\n","\u001b[32m2022-05-03T09:31:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0244, train/hateful_memes/cross_entropy/avg: 0.1864, train/total_loss: 0.0244, train/total_loss/avg: 0.1864, max mem: 11667.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 855ms, time_since_start: 01h 24s 061ms, eta: 02h 26m 54s 537ms\n","\u001b[32m2022-05-03T09:32:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0192, train/hateful_memes/cross_entropy/avg: 0.1836, train/total_loss: 0.0192, train/total_loss/avg: 0.1836, max mem: 11667.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 300ms, time_since_start: 01h 01m 18s 362ms, eta: 02h 24m 30s 156ms\n","\u001b[32m2022-05-03T09:33:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0192, train/hateful_memes/cross_entropy/avg: 0.1807, train/total_loss: 0.0192, train/total_loss/avg: 0.1807, max mem: 11667.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 752ms, time_since_start: 01h 02m 13s 115ms, eta: 02h 24m 46s 654ms\n","\u001b[32m2022-05-03T09:34:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0114, train/hateful_memes/cross_entropy/avg: 0.1780, train/total_loss: 0.0114, train/total_loss/avg: 0.1780, max mem: 11667.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 276ms, time_since_start: 01h 03m 07s 391ms, eta: 02h 22m 35s 801ms\n","\u001b[32m2022-05-03T09:35:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0114, train/hateful_memes/cross_entropy/avg: 0.1769, train/total_loss: 0.0114, train/total_loss/avg: 0.1769, max mem: 11667.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 346ms, time_since_start: 01h 04m 01s 738ms, eta: 02h 21m 51s 689ms\n","\u001b[32m2022-05-03T09:35:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0114, train/hateful_memes/cross_entropy/avg: 0.1760, train/total_loss: 0.0114, train/total_loss/avg: 0.1760, max mem: 11667.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 715ms, time_since_start: 01h 04m 56s 453ms, eta: 02h 21m 53s 747ms\n","\u001b[32m2022-05-03T09:36:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0114, train/hateful_memes/cross_entropy/avg: 0.1734, train/total_loss: 0.0114, train/total_loss/avg: 0.1734, max mem: 11667.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 451ms, time_since_start: 01h 05m 50s 904ms, eta: 02h 20m 17s 256ms\n","\u001b[32m2022-05-03T09:37:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.1710, train/total_loss: 0.0089, train/total_loss/avg: 0.1710, max mem: 11667.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 499ms, time_since_start: 01h 06m 45s 404ms, eta: 02h 19m 29s 365ms\n","\u001b[32m2022-05-03T09:38:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T09:38:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T09:38:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:38:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:38:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.1685, train/total_loss: 0.0089, train/total_loss/avg: 0.1685, max mem: 11667.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 1.45, time: 01m 09s 032ms, time_since_start: 01h 07m 54s 436ms, eta: 02h 55m 30s 895ms\n","\u001b[32m2022-05-03T09:38:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T09:38:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T09:38:57 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T09:38:57 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T09:38:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T09:39:03 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-03T09:39:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:39:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:39:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.5168, val/total_loss: 2.5168, val/hateful_memes/accuracy: 0.6389, val/hateful_memes/binary_f1: 0.2909, val/hateful_memes/roc_auc: 0.6290, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 31s 709ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.629033\n","\u001b[32m2022-05-03T09:40:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.1662, train/total_loss: 0.0056, train/total_loss/avg: 0.1662, max mem: 11667.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 116ms, time_since_start: 01h 09m 21s 265ms, eta: 02h 19m 12s 013ms\n","\u001b[32m2022-05-03T09:41:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.1639, train/total_loss: 0.0031, train/total_loss/avg: 0.1639, max mem: 11667.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 956ms, time_since_start: 01h 10m 16s 221ms, eta: 02h 17m 51s 879ms\n","\u001b[32m2022-05-03T09:42:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1617, train/total_loss: 0.0025, train/total_loss/avg: 0.1617, max mem: 11667.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 292ms, time_since_start: 01h 11m 10s 514ms, eta: 02h 15m 16s 624ms\n","\u001b[32m2022-05-03T09:43:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1595, train/total_loss: 0.0025, train/total_loss/avg: 0.1595, max mem: 11667.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 314ms, time_since_start: 01h 12m 04s 828ms, eta: 02h 14m 24s 713ms\n","\u001b[32m2022-05-03T09:43:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1574, train/total_loss: 0.0025, train/total_loss/avg: 0.1574, max mem: 11667.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 715ms, time_since_start: 01h 12m 59s 543ms, eta: 02h 14m 28s 599ms\n","\u001b[32m2022-05-03T09:44:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1554, train/total_loss: 0.0019, train/total_loss/avg: 0.1554, max mem: 11667.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 227ms, time_since_start: 01h 13m 53s 771ms, eta: 02h 12m 21s 547ms\n","\u001b[32m2022-05-03T09:45:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1534, train/total_loss: 0.0019, train/total_loss/avg: 0.1534, max mem: 11667.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 316ms, time_since_start: 01h 14m 48s 087ms, eta: 02h 11m 39s 256ms\n","\u001b[32m2022-05-03T09:46:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1515, train/total_loss: 0.0019, train/total_loss/avg: 0.1515, max mem: 11667.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 851ms, time_since_start: 01h 15m 42s 939ms, eta: 02h 12m 01s 391ms\n","\u001b[32m2022-05-03T09:47:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1495, train/total_loss: 0.0015, train/total_loss/avg: 0.1495, max mem: 11667.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 443ms, time_since_start: 01h 16m 37s 383ms, eta: 02h 10m 07s 042ms\n","\u001b[32m2022-05-03T09:48:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T09:48:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T09:48:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:48:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:48:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1477, train/total_loss: 0.0013, train/total_loss/avg: 0.1477, max mem: 11667.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 1.37, time: 01m 13s 169ms, time_since_start: 01h 17m 50s 553ms, eta: 02h 53m 37s 913ms\n","\u001b[32m2022-05-03T09:48:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T09:48:49 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T09:48:53 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T09:48:53 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T09:48:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T09:49:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:49:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:49:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.6070, val/total_loss: 2.6070, val/hateful_memes/accuracy: 0.6389, val/hateful_memes/binary_f1: 0.3478, val/hateful_memes/roc_auc: 0.6149, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 23s 243ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.629033\n","\u001b[32m2022-05-03T09:50:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1481, train/total_loss: 0.0013, train/total_loss/avg: 0.1481, max mem: 11667.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 973ms, time_since_start: 01h 19m 08s 771ms, eta: 02h 09m 31s 180ms\n","\u001b[32m2022-05-03T09:51:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1463, train/total_loss: 0.0013, train/total_loss/avg: 0.1463, max mem: 11667.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 419ms, time_since_start: 01h 20m 03s 191ms, eta: 02h 07m 17s 554ms\n","\u001b[32m2022-05-03T09:51:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1446, train/total_loss: 0.0013, train/total_loss/avg: 0.1446, max mem: 11667.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 724ms, time_since_start: 01h 20m 57s 915ms, eta: 02h 07m 04s 663ms\n","\u001b[32m2022-05-03T09:52:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1429, train/total_loss: 0.0013, train/total_loss/avg: 0.1429, max mem: 11667.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 303ms, time_since_start: 01h 21m 52s 218ms, eta: 02h 05m 10s 774ms\n","\u001b[32m2022-05-03T09:53:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1412, train/total_loss: 0.0013, train/total_loss/avg: 0.1412, max mem: 11667.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 443ms, time_since_start: 01h 22m 46s 662ms, eta: 02h 04m 34s 890ms\n","\u001b[32m2022-05-03T09:54:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1396, train/total_loss: 0.0013, train/total_loss/avg: 0.1396, max mem: 11667.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 599ms, time_since_start: 01h 23m 41s 262ms, eta: 02h 04m 731ms\n","\u001b[32m2022-05-03T09:55:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1382, train/total_loss: 0.0013, train/total_loss/avg: 0.1382, max mem: 11667.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 301ms, time_since_start: 01h 24m 35s 563ms, eta: 02h 02m 24s 819ms\n","\u001b[32m2022-05-03T09:56:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1367, train/total_loss: 0.0013, train/total_loss/avg: 0.1367, max mem: 11667.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 795ms, time_since_start: 01h 25m 30s 359ms, eta: 02h 02m 36s 023ms\n","\u001b[32m2022-05-03T09:57:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1352, train/total_loss: 0.0013, train/total_loss/avg: 0.1352, max mem: 11667.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 409ms, time_since_start: 01h 26m 24s 768ms, eta: 02h 48s 786ms\n","\u001b[32m2022-05-03T09:58:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T09:58:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T09:58:24 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:58:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:58:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1337, train/total_loss: 0.0014, train/total_loss/avg: 0.1337, max mem: 11667.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00001, ups: 1.45, time: 01m 09s 966ms, time_since_start: 01h 27m 34s 734ms, eta: 02h 34m 10s 223ms\n","\u001b[32m2022-05-03T09:58:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T09:58:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T09:58:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T09:58:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T09:58:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T09:58:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T09:58:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T09:58:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.7957, val/total_loss: 2.7957, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.3274, val/hateful_memes/roc_auc: 0.6079, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 22000, val_time: 18s 586ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.629033\n","\u001b[32m2022-05-03T09:59:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1322, train/total_loss: 0.0013, train/total_loss/avg: 0.1322, max mem: 11667.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 720ms, time_since_start: 01h 28m 49s 042ms, eta: 02h 01m 50s 088ms\n","\u001b[32m2022-05-03T10:00:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1312, train/total_loss: 0.0013, train/total_loss/avg: 0.1312, max mem: 11667.0, experiment: run, epoch: 35, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 349ms, time_since_start: 01h 29m 43s 392ms, eta: 01h 57m 55s 023ms\n","\u001b[32m2022-05-03T10:01:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1298, train/total_loss: 0.0013, train/total_loss/avg: 0.1298, max mem: 11667.0, experiment: run, epoch: 35, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 422ms, time_since_start: 01h 30m 37s 814ms, eta: 01h 57m 09s 133ms\n","\u001b[32m2022-05-03T10:02:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1285, train/total_loss: 0.0013, train/total_loss/avg: 0.1285, max mem: 11667.0, experiment: run, epoch: 36, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 779ms, time_since_start: 01h 31m 32s 594ms, eta: 01h 56m 59s 581ms\n","\u001b[32m2022-05-03T10:03:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1271, train/total_loss: 0.0013, train/total_loss/avg: 0.1271, max mem: 11667.0, experiment: run, epoch: 36, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 325ms, time_since_start: 01h 32m 26s 919ms, eta: 01h 55m 06s 095ms\n","\u001b[32m2022-05-03T10:04:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1258, train/total_loss: 0.0013, train/total_loss/avg: 0.1258, max mem: 11667.0, experiment: run, epoch: 37, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 752ms, time_since_start: 01h 33m 21s 672ms, eta: 01h 55m 04s 787ms\n","\u001b[32m2022-05-03T10:05:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1246, train/total_loss: 0.0013, train/total_loss/avg: 0.1246, max mem: 11667.0, experiment: run, epoch: 37, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 301ms, time_since_start: 01h 34m 15s 973ms, eta: 01h 53m 12s 586ms\n","\u001b[32m2022-05-03T10:06:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1233, train/total_loss: 0.0013, train/total_loss/avg: 0.1233, max mem: 11667.0, experiment: run, epoch: 37, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 463ms, time_since_start: 01h 35m 10s 437ms, eta: 01h 52m 37s 487ms\n","\u001b[32m2022-05-03T10:07:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1221, train/total_loss: 0.0014, train/total_loss/avg: 0.1221, max mem: 11667.0, experiment: run, epoch: 38, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 715ms, time_since_start: 01h 36m 05s 153ms, eta: 01h 52m 13s 178ms\n","\u001b[32m2022-05-03T10:07:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T10:07:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T10:08:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T10:08:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T10:08:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1209, train/total_loss: 0.0014, train/total_loss/avg: 0.1209, max mem: 11667.0, experiment: run, epoch: 38, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00001, ups: 1.45, time: 01m 09s 164ms, time_since_start: 01h 37m 14s 317ms, eta: 02h 20m 40s 832ms\n","\u001b[32m2022-05-03T10:08:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T10:08:13 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T10:08:17 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T10:08:17 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T10:08:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T10:08:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T10:08:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T10:08:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.8361, val/total_loss: 2.8361, val/hateful_memes/accuracy: 0.6370, val/hateful_memes/binary_f1: 0.3194, val/hateful_memes/roc_auc: 0.6076, num_updates: 10000, epoch: 38, iterations: 10000, max_updates: 22000, val_time: 22s 311ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.629033\n","\u001b[32m2022-05-03T10:09:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1197, train/total_loss: 0.0014, train/total_loss/avg: 0.1197, max mem: 11667.0, experiment: run, epoch: 38, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 186ms, time_since_start: 01h 38m 31s 817ms, eta: 01h 51m 18s 870ms\n","\u001b[32m2022-05-03T10:10:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1186, train/total_loss: 0.0014, train/total_loss/avg: 0.1186, max mem: 11667.0, experiment: run, epoch: 39, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 666ms, time_since_start: 01h 39m 26s 483ms, eta: 01h 49m 20s 340ms\n","\u001b[32m2022-05-03T10:11:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1192, train/total_loss: 0.0014, train/total_loss/avg: 0.1192, max mem: 11667.0, experiment: run, epoch: 39, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 343ms, time_since_start: 01h 40m 20s 827ms, eta: 01h 47m 46s 314ms\n","\u001b[32m2022-05-03T10:12:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1182, train/total_loss: 0.0014, train/total_loss/avg: 0.1182, max mem: 11667.0, experiment: run, epoch: 40, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 922ms, time_since_start: 01h 41m 15s 750ms, eta: 01h 47m 59s 306ms\n","\u001b[32m2022-05-03T10:13:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1171, train/total_loss: 0.0014, train/total_loss/avg: 0.1171, max mem: 11667.0, experiment: run, epoch: 40, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 420ms, time_since_start: 01h 42m 10s 171ms, eta: 01h 46m 04s 783ms\n","\u001b[32m2022-05-03T10:14:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1160, train/total_loss: 0.0014, train/total_loss/avg: 0.1160, max mem: 11667.0, experiment: run, epoch: 40, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 423ms, time_since_start: 01h 43m 04s 594ms, eta: 01h 45m 09s 716ms\n","\u001b[32m2022-05-03T10:14:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1149, train/total_loss: 0.0010, train/total_loss/avg: 0.1149, max mem: 11667.0, experiment: run, epoch: 41, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 687ms, time_since_start: 01h 43m 59s 281ms, eta: 01h 44m 44s 737ms\n","\u001b[32m2022-05-03T10:15:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1139, train/total_loss: 0.0010, train/total_loss/avg: 0.1139, max mem: 11667.0, experiment: run, epoch: 41, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 390ms, time_since_start: 01h 44m 53s 671ms, eta: 01h 43m 15s 246ms\n","\u001b[32m2022-05-03T10:16:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1128, train/total_loss: 0.0009, train/total_loss/avg: 0.1128, max mem: 11667.0, experiment: run, epoch: 41, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 382ms, time_since_start: 01h 45m 48s 053ms, eta: 01h 42m 19s 039ms\n","\u001b[32m2022-05-03T10:17:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T10:17:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T10:17:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T10:18:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T10:18:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1118, train/total_loss: 0.0006, train/total_loss/avg: 0.1118, max mem: 11667.0, experiment: run, epoch: 42, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00001, ups: 1.37, time: 01m 13s 170ms, time_since_start: 01h 47m 01s 223ms, eta: 02h 16m 25s 531ms\n","\u001b[32m2022-05-03T10:18:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T10:18:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T10:18:04 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T10:18:04 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T10:18:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T10:18:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T10:18:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T10:18:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.9954, val/total_loss: 2.9954, val/hateful_memes/accuracy: 0.6426, val/hateful_memes/binary_f1: 0.3458, val/hateful_memes/roc_auc: 0.6000, num_updates: 11000, epoch: 42, iterations: 11000, max_updates: 22000, val_time: 23s 605ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.629033\n","\u001b[32m2022-05-03T10:19:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1108, train/total_loss: 0.0006, train/total_loss/avg: 0.1108, max mem: 11667.0, experiment: run, epoch: 42, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 098ms, time_since_start: 01h 48m 20s 535ms, eta: 01h 41m 47s 861ms\n","\u001b[32m2022-05-03T10:20:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1098, train/total_loss: 0.0005, train/total_loss/avg: 0.1098, max mem: 11667.0, experiment: run, epoch: 43, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 851ms, time_since_start: 01h 49m 15s 386ms, eta: 01h 40m 24s 620ms\n","\u001b[32m2022-05-03T10:21:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1089, train/total_loss: 0.0004, train/total_loss/avg: 0.1089, max mem: 11667.0, experiment: run, epoch: 43, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 511ms, time_since_start: 01h 50m 09s 898ms, eta: 01h 38m 51s 933ms\n","\u001b[32m2022-05-03T10:22:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1079, train/total_loss: 0.0003, train/total_loss/avg: 0.1079, max mem: 11667.0, experiment: run, epoch: 43, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 395ms, time_since_start: 01h 51m 04s 293ms, eta: 01h 37m 43s 948ms\n","\u001b[32m2022-05-03T10:22:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1070, train/total_loss: 0.0003, train/total_loss/avg: 0.1070, max mem: 11667.0, experiment: run, epoch: 44, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 815ms, time_since_start: 01h 51m 59s 109ms, eta: 01h 37m 33s 522ms\n","\u001b[32m2022-05-03T10:23:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1060, train/total_loss: 0.0003, train/total_loss/avg: 0.1060, max mem: 11667.0, experiment: run, epoch: 44, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 517ms, time_since_start: 01h 52m 53s 627ms, eta: 01h 36m 06s 251ms\n","\u001b[32m2022-05-03T10:24:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1051, train/total_loss: 0.0002, train/total_loss/avg: 0.1051, max mem: 11667.0, experiment: run, epoch: 44, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 412ms, time_since_start: 01h 53m 48s 040ms, eta: 01h 34m 59s 808ms\n","\u001b[32m2022-05-03T10:25:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1042, train/total_loss: 0.0002, train/total_loss/avg: 0.1042, max mem: 11667.0, experiment: run, epoch: 45, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 673ms, time_since_start: 01h 54m 42s 714ms, eta: 01h 34m 31s 554ms\n","\u001b[32m2022-05-03T10:26:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1034, train/total_loss: 0.0002, train/total_loss/avg: 0.1034, max mem: 11667.0, experiment: run, epoch: 45, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 348ms, time_since_start: 01h 55m 37s 063ms, eta: 01h 33m 02s 559ms\n","\u001b[32m2022-05-03T10:27:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T10:27:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T10:27:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T10:27:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T10:27:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1025, train/total_loss: 0.0002, train/total_loss/avg: 0.1025, max mem: 11667.0, experiment: run, epoch: 46, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 716ms, time_since_start: 01h 56m 49s 780ms, eta: 02h 03m 15s 273ms\n","\u001b[32m2022-05-03T10:27:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T10:27:49 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T10:27:52 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T10:27:52 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T10:27:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 3.0057, val/total_loss: 3.0057, val/hateful_memes/accuracy: 0.6444, val/hateful_memes/binary_f1: 0.3287, val/hateful_memes/roc_auc: 0.6116, num_updates: 12000, epoch: 46, iterations: 12000, max_updates: 22000, val_time: 03s 618ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.629033\n","\u001b[32m2022-05-03T10:28:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1017, train/total_loss: 0.0002, train/total_loss/avg: 0.1017, max mem: 11667.0, experiment: run, epoch: 46, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0., ups: 1.82, time: 55s 207ms, time_since_start: 01h 57m 48s 753ms, eta: 01h 32m 38s 506ms\n","\u001b[32m2022-05-03T10:29:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1008, train/total_loss: 0.0002, train/total_loss/avg: 0.1008, max mem: 11667.0, experiment: run, epoch: 46, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 664ms, time_since_start: 01h 58m 43s 417ms, eta: 01h 30m 48s 156ms\n","\u001b[32m2022-05-03T10:30:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1000, train/total_loss: 0.0002, train/total_loss/avg: 0.1000, max mem: 11667.0, experiment: run, epoch: 47, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 675ms, time_since_start: 01h 59m 38s 093ms, eta: 01h 29m 53s 709ms\n","\u001b[32m2022-05-03T10:31:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0992, train/total_loss: 0.0001, train/total_loss/avg: 0.0992, max mem: 11667.0, experiment: run, epoch: 47, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 372ms, time_since_start: 02h 32s 466ms, eta: 01h 28m 28s 537ms\n","\u001b[32m2022-05-03T10:32:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0987, train/total_loss: 0.0001, train/total_loss/avg: 0.0987, max mem: 11667.0, experiment: run, epoch: 47, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 344ms, time_since_start: 02h 01m 26s 810ms, eta: 01h 27m 30s 479ms\n","\u001b[32m2022-05-03T10:33:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0979, train/total_loss: 0.0001, train/total_loss/avg: 0.0979, max mem: 11667.0, experiment: run, epoch: 48, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 844ms, time_since_start: 02h 02m 21s 655ms, eta: 01h 27m 23s 033ms\n","\u001b[32m2022-05-03T10:34:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0971, train/total_loss: 0.0001, train/total_loss/avg: 0.0971, max mem: 11667.0, experiment: run, epoch: 48, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 525ms, time_since_start: 02h 03m 16s 181ms, eta: 01h 25m 57s 101ms\n","\u001b[32m2022-05-03T10:35:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0964, train/total_loss: 0.0001, train/total_loss/avg: 0.0964, max mem: 11667.0, experiment: run, epoch: 49, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 750ms, time_since_start: 02h 04m 10s 932ms, eta: 01h 25m 22s 691ms\n","\u001b[32m2022-05-03T10:36:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0956, train/total_loss: 0.0001, train/total_loss/avg: 0.0956, max mem: 11667.0, experiment: run, epoch: 49, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 322ms, time_since_start: 02h 05m 05s 254ms, eta: 01h 23m 47s 359ms\n","\u001b[32m2022-05-03T10:36:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T10:36:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T10:37:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T10:37:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T10:37:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0949, train/total_loss: 0.0002, train/total_loss/avg: 0.0949, max mem: 11667.0, experiment: run, epoch: 49, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0., ups: 1.47, time: 01m 08s 291ms, time_since_start: 02h 06m 13s 546ms, eta: 01h 44m 10s 758ms\n","\u001b[32m2022-05-03T10:37:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T10:37:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T10:37:17 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T10:37:17 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T10:37:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.7626, val/total_loss: 2.7626, val/hateful_memes/accuracy: 0.6333, val/hateful_memes/binary_f1: 0.2929, val/hateful_memes/roc_auc: 0.6104, num_updates: 13000, epoch: 49, iterations: 13000, max_updates: 22000, val_time: 05s 341ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.629033\n","\u001b[32m2022-05-03T10:38:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0942, train/total_loss: 0.0001, train/total_loss/avg: 0.0942, max mem: 11667.0, experiment: run, epoch: 50, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0., ups: 1.82, time: 55s 394ms, time_since_start: 02h 07m 14s 283ms, eta: 01h 23m 33s 892ms\n","\u001b[32m2022-05-03T10:39:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0935, train/total_loss: 0.0002, train/total_loss/avg: 0.0935, max mem: 11667.0, experiment: run, epoch: 50, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 574ms, time_since_start: 02h 08m 08s 858ms, eta: 01h 21m 24s 244ms\n","\u001b[32m2022-05-03T10:40:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0931, train/total_loss: 0.0001, train/total_loss/avg: 0.0931, max mem: 11667.0, experiment: run, epoch: 50, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 289ms, time_since_start: 02h 09m 03s 148ms, eta: 01h 20m 03s 474ms\n","\u001b[32m2022-05-03T10:40:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0924, train/total_loss: 0.0001, train/total_loss/avg: 0.0924, max mem: 11667.0, experiment: run, epoch: 51, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0., ups: 1.82, time: 55s 049ms, time_since_start: 02h 09m 58s 197ms, eta: 01h 20m 14s 749ms\n","\u001b[32m2022-05-03T10:41:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0917, train/total_loss: 0.0001, train/total_loss/avg: 0.0917, max mem: 11667.0, experiment: run, epoch: 51, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 555ms, time_since_start: 02h 10m 52s 753ms, eta: 01h 18m 36s 028ms\n","\u001b[32m2022-05-03T10:42:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0910, train/total_loss: 0.0001, train/total_loss/avg: 0.0910, max mem: 11667.0, experiment: run, epoch: 52, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 878ms, time_since_start: 02h 11m 47s 632ms, eta: 01h 18m 08s 202ms\n","\u001b[32m2022-05-03T10:43:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0907, train/total_loss: 0.0002, train/total_loss/avg: 0.0907, max mem: 11667.0, experiment: run, epoch: 52, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 525ms, time_since_start: 02h 12m 42s 158ms, eta: 01h 16m 42s 584ms\n","\u001b[32m2022-05-03T10:44:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0901, train/total_loss: 0.0002, train/total_loss/avg: 0.0901, max mem: 11667.0, experiment: run, epoch: 52, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 506ms, time_since_start: 02h 13m 36s 664ms, eta: 01h 15m 45s 535ms\n","\u001b[32m2022-05-03T10:45:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0894, train/total_loss: 0.0002, train/total_loss/avg: 0.0894, max mem: 11667.0, experiment: run, epoch: 53, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 941ms, time_since_start: 02h 14m 31s 606ms, eta: 01h 15m 25s 925ms\n","\u001b[32m2022-05-03T10:46:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-03T10:46:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-03T10:46:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-03T10:46:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-03T10:46:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0888, train/total_loss: 0.0003, train/total_loss/avg: 0.0888, max mem: 11667.0, experiment: run, epoch: 53, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0., ups: 1.37, time: 01m 13s 856ms, time_since_start: 02h 15m 45s 462ms, eta: 01h 40m 08s 934ms\n","\u001b[32m2022-05-03T10:46:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-03T10:46:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-03T10:46:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T10:46:48 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T10:46:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.8171, val/total_loss: 2.8171, val/hateful_memes/accuracy: 0.6352, val/hateful_memes/binary_f1: 0.3411, val/hateful_memes/roc_auc: 0.6177, num_updates: 14000, epoch: 53, iterations: 14000, max_updates: 22000, val_time: 03s 623ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.629033\n","Traceback (most recent call last):\n","  File \"tools/run.py\", line 126, in <module>\n","    run()\n","  File \"tools/run.py\", line 122, in run\n","    main(configuration, predict=predict)\n","  File \"tools/run.py\", line 56, in main\n","    trainer.train()\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/trainers/mmf_trainer.py\", line 145, in train\n","    self.training_loop()\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/trainers/core/training_loop.py\", line 33, in training_loop\n","    self.run_training_epoch()\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/trainers/core/training_loop.py\", line 91, in run_training_epoch\n","    report = self.run_training_batch(batch, num_batches_for_this_update)\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/trainers/core/training_loop.py\", line 170, in run_training_batch\n","    self._backward(loss)\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/trainers/core/training_loop.py\", line 212, in _backward\n","    self.scaler.scale(loss).backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 255, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 149, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n","KeyboardInterrupt\n"]}],"source":["!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml model=mmbt dataset=hateful_memes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1650487711787,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"AD2gKPouu31e","outputId":"c5a1d1b9-f210-4b4d-9cc8-e566eca2463a"},"outputs":[{"name":"stdout","output_type":"stream","text":["configs   ensemble\t     README.md\tsave_vilbert\n","datasets  mmf-hateful-memes  save\ttools\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26804,"status":"ok","timestamp":1651574932079,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"WZQ0e1_kpqnW","outputId":"d5e70718-9dbc-4418-c640-bf062bad3b11"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-03T10:48:32 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml\n","\u001b[32m2022-05-03T10:48:32 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n","\u001b[32m2022-05-03T10:48:32 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-05-03T10:48:32 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n","\u001b[32m2022-05-03T10:48:32 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-05-03T10:48:32 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-03T10:48:32 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-03T10:48:32 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml', 'model=mmbt', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False'])\n","\u001b[32m2022-05-03T10:48:32 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-03T10:48:32 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-05-03T10:48:32 | mmf_cli.run: \u001b[0mUsing seed 32291586\n","\u001b[32m2022-05-03T10:48:32 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-03T10:48:35 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T10:48:35 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T10:48:35 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T10:48:35 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelJit: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModelJit from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModelJit from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertModelJit were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelJit for predictions without further training.\n","\u001b[32m2022-05-03T10:48:44 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-03T10:48:44 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-03T10:48:44 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T10:48:47 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T10:48:47 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-05-03T10:48:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-03T10:48:47 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 7000\n","\u001b[32m2022-05-03T10:48:47 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 7000\n","\u001b[32m2022-05-03T10:48:47 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 27\n","\u001b[32m2022-05-03T10:48:47 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-05-03T10:48:47 | mmf.trainers.mmf_trainer: \u001b[0mMMBT(\n","  (model): MMBTForClassification(\n","    (bert): MMBTBase(\n","      (mmbt): MMBTModel(\n","        (transformer): BertModelJit(\n","          (embeddings): BertEmbeddingsJit(\n","            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","            (position_embeddings): Embedding(512, 768)\n","            (token_type_embeddings): Embedding(2, 768)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (encoder): BertEncoderJit(\n","            (layer): ModuleList(\n","              (0): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (1): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (2): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (3): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (4): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (5): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (6): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (7): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (8): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (9): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (10): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","              (11): BertLayerJit(\n","                (attention): BertAttentionJit(\n","                  (self): BertSelfAttentionJit(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): BertSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): BertIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                )\n","                (output): BertOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","            )\n","          )\n","          (pooler): BertPooler(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (activation): Tanh()\n","          )\n","        )\n","        (modal_encoder): ModalEmbeddings(\n","          (encoder): ResNet152ImageEncoder(\n","            (model): Sequential(\n","              (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","              (2): ReLU(inplace=True)\n","              (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","              (4): Sequential(\n","                (0): Bottleneck(\n","                  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                  (downsample): Sequential(\n","                    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  )\n","                )\n","                (1): Bottleneck(\n","                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (2): Bottleneck(\n","                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","              )\n","              (5): Sequential(\n","                (0): Bottleneck(\n","                  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                  (downsample): Sequential(\n","                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  )\n","                )\n","                (1): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (2): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (3): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (4): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (5): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (6): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (7): Bottleneck(\n","                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","              )\n","              (6): Sequential(\n","                (0): Bottleneck(\n","                  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                  (downsample): Sequential(\n","                    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  )\n","                )\n","                (1): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (2): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (3): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (4): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (5): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (6): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (7): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (8): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (9): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (10): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (11): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (12): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (13): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (14): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (15): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (16): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (17): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (18): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (19): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (20): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (21): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (22): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (23): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (24): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (25): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (26): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (27): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (28): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (29): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (30): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (31): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (32): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (33): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (34): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (35): Bottleneck(\n","                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","              )\n","              (7): Sequential(\n","                (0): Bottleneck(\n","                  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                  (downsample): Sequential(\n","                    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  )\n","                )\n","                (1): Bottleneck(\n","                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","                (2): Bottleneck(\n","                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                  (relu): ReLU(inplace=True)\n","                )\n","              )\n","            )\n","            (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n","          )\n","          (proj_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n","          (position_embeddings): Embedding(512, 768)\n","          (token_type_embeddings): Embedding(2, 768)\n","          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-05-03T10:48:47 | mmf.utils.general: \u001b[0mTotal Parameters: 169793346. Trained Parameters: 169793346\n","\u001b[32m2022-05-03T10:48:47 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2022-05-03T10:48:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/17 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T10:48:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T10:48:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","\n","100% 17/17 [00:03<00:00,  4.95it/s]\n","\u001b[32m2022-05-03T10:48:50 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-03T10:48:50 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-03T10:48:51 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 2.5168, val/total_loss: 2.5168, val/hateful_memes/accuracy: 0.6389, val/hateful_memes/binary_f1: 0.2909, val/hateful_memes/roc_auc: 0.6290\n","\u001b[32m2022-05-03T10:48:51 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 06s 464ms\n"]}],"source":["!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml\" \\\n","model=mmbt \\\n","dataset=hateful_memes \\\n","run_type=val \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16854,"status":"ok","timestamp":1651574948917,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"XZr038wyqZla","outputId":"8b6fdc14-a4cd-46bd-c001-443a581ba658"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-03T10:48:56 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml\n","\u001b[32m2022-05-03T10:48:56 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n","\u001b[32m2022-05-03T10:48:56 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-05-03T10:48:56 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n","\u001b[32m2022-05-03T10:48:56 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save_mmbt_default/best.ckpt\n","\u001b[32m2022-05-03T10:48:56 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-03T10:48:56 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-03T10:48:56 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml', 'model=mmbt', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save_mmbt_default/best.ckpt', 'checkpoint.resume_pretrained=False'])\n","\u001b[32m2022-05-03T10:48:56 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-03T10:48:56 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-05-03T10:48:56 | mmf_cli.run: \u001b[0mUsing seed 57015931\n","\u001b[32m2022-05-03T10:48:56 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-03T10:49:00 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T10:49:00 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T10:49:00 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T10:49:00 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelJit: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModelJit from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModelJit from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertModelJit were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelJit for predictions without further training.\n","\u001b[32m2022-05-03T10:49:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-03T10:49:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/mmf_run\", line 8, in <module>\n","    sys.exit(run())\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf_cli/run.py\", line 133, in run\n","    main(configuration, predict=predict)\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf_cli/run.py\", line 52, in main\n","    trainer.load()\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/trainers/mmf_trainer.py\", line 46, in load\n","    self.on_init_start()\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/trainers/core/callback_hook.py\", line 20, in on_init_start\n","    callback.on_init_start(**kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/trainers/callbacks/checkpoint.py\", line 30, in on_init_start\n","    self._checkpoint.load_state_dict()\n","  File \"/usr/local/lib/python3.7/dist-packages/mmf/utils/checkpoint.py\", line 247, in load_state_dict\n","    raise RuntimeError(f\"{ckpt_config.resume_file} doesn't exist\")\n","RuntimeError: save_mmbt_default/best.ckpt doesn't exist\n"]}],"source":["!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml\" \\\n","model=mmbt \\\n","dataset=hateful_memes \\\n","run_type=test \\\n","checkpoint.resume_file=save_mmbt_default/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47700,"status":"ok","timestamp":1651574875050,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"Iv83UfrUSY6H","outputId":"824558be-8e74-4262-96d0-5f8f9e0001b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-03T10:47:20 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml\n","\u001b[32m2022-05-03T10:47:20 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n","\u001b[32m2022-05-03T10:47:20 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-05-03T10:47:20 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n","\u001b[32m2022-05-03T10:47:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-05-03T10:47:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","\u001b[32m2022-05-03T10:47:20 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-03T10:47:20 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-03T10:47:20 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml', 'model=mmbt', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n","\u001b[32m2022-05-03T10:47:20 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-03T10:47:20 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-05-03T10:47:20 | mmf_cli.run: \u001b[0mUsing seed 20123067\n","\u001b[32m2022-05-03T10:47:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-03T10:47:23 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T10:47:23 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T10:47:23 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T10:47:23 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelJit: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModelJit from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModelJit from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertModelJit were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelJit for predictions without further training.\n","\u001b[32m2022-05-03T10:47:35 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-03T10:47:35 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-03T10:47:35 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T10:47:43 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T10:47:43 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-05-03T10:47:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-03T10:47:43 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 7000\n","\u001b[32m2022-05-03T10:47:43 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 7000\n","\u001b[32m2022-05-03T10:47:43 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 27\n","\u001b[32m2022-05-03T10:47:43 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n","\u001b[32m2022-05-03T10:47:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/63 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T10:47:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T10:47:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","\n","100% 63/63 [00:11<00:00,  5.38it/s]\n","\u001b[32m2022-05-03T10:47:55 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes/save/hateful_memes_mmbt_20123067/reports/hateful_memes_run_test_2022-05-03T10:47:55.csv\n","\u001b[32m2022-05-03T10:47:55 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 63\n","\u001b[32m2022-05-03T10:47:55 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"]}],"source":["!mmf_predict config=\"mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml\" \\\n","model=mmbt \\\n","dataset=hateful_memes \\\n","run_type=test \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28722,"status":"ok","timestamp":1651574903770,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"Nkm_k14RvrND","outputId":"5ac1f3ae-2be5-4346-f9bc-395c282ca17e"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-03T10:48:01 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml\n","\u001b[32m2022-05-03T10:48:01 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n","\u001b[32m2022-05-03T10:48:01 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-05-03T10:48:01 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n","\u001b[32m2022-05-03T10:48:01 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-05-03T10:48:01 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","\u001b[32m2022-05-03T10:48:01 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-03T10:48:01 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-03T10:48:01 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml', 'model=mmbt', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n","\u001b[32m2022-05-03T10:48:01 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-03T10:48:01 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-05-03T10:48:01 | mmf_cli.run: \u001b[0mUsing seed 1628773\n","\u001b[32m2022-05-03T10:48:01 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-03T10:48:05 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T10:48:05 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T10:48:05 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-03T10:48:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelJit: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModelJit from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModelJit from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertModelJit were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelJit for predictions without further training.\n","\u001b[32m2022-05-03T10:48:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-03T10:48:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-03T10:48:13 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T10:48:16 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T10:48:16 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-05-03T10:48:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-03T10:48:16 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 7000\n","\u001b[32m2022-05-03T10:48:16 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 7000\n","\u001b[32m2022-05-03T10:48:16 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 27\n","\u001b[32m2022-05-03T10:48:16 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n","\u001b[32m2022-05-03T10:48:16 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/17 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T10:48:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-03T10:48:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","\n","100% 17/17 [00:03<00:00,  4.57it/s]\n","\u001b[32m2022-05-03T10:48:20 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes/save/hateful_memes_mmbt_1628773/reports/hateful_memes_run_val_2022-05-03T10:48:20.csv\n","\u001b[32m2022-05-03T10:48:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 17\n","\u001b[32m2022-05-03T10:48:20 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"]}],"source":["!mmf_predict config=\"mmf-hateful-memes/projects/hateful_memes/configs/mmbt/defaults.yaml\" \\\n","model=mmbt \\\n","dataset=hateful_memes \\\n","run_type=val \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"markdown","metadata":{"id":"UQ5X5tI0wsHK"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"jI6JjB4dwsUC"},"source":["# **Visual Bert**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"al1NNDMhvveQ","outputId":"b9e7f3c4-f1b7-4929-fa59-437d7bf2dc03"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-20T21:45:28 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml\n","\u001b[32m2022-04-20T21:45:28 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n","\u001b[32m2022-04-20T21:45:28 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-20T21:45:28 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-20T21:45:28 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-20T21:45:28 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml', 'training.batch_size=32', 'model=visual_bert', 'dataset=hateful_memes'])\n","\u001b[32m2022-04-20T21:45:28 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-20T21:45:28 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-04-20T21:45:28 | mmf_cli.run: \u001b[0mUsing seed 28865696\n","\u001b[32m2022-04-20T21:45:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-20T21:45:30 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T21:45:30 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T21:45:30 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-20T21:45:30 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-20T21:45:36 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-20T21:45:36 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-20T21:45:36 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-20T21:45:36 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-20T21:45:36 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-20T21:45:36 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-04-20T21:46:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.6563, train/hateful_memes/cross_entropy/avg: 0.6563, train/total_loss: 0.6563, train/total_loss/avg: 0.6563, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.22, time: 01m 22s 172ms, time_since_start: 01m 22s 240ms, eta: 05h 05m 01s 689ms\n","\u001b[32m2022-04-20T21:48:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6563, train/hateful_memes/cross_entropy/avg: 0.6822, train/total_loss: 0.6563, train/total_loss/avg: 0.6822, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 1.23, time: 01m 21s 491ms, time_since_start: 02m 43s 732ms, eta: 05h 01m 07s 226ms\n","\u001b[32m2022-04-20T21:49:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6563, train/hateful_memes/cross_entropy/avg: 0.6599, train/total_loss: 0.6563, train/total_loss/avg: 0.6599, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 1.23, time: 01m 21s 164ms, time_since_start: 04m 04s 896ms, eta: 04h 58m 32s 073ms\n","\u001b[32m2022-04-20T21:51:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6563, train/hateful_memes/cross_entropy/avg: 0.6646, train/total_loss: 0.6563, train/total_loss/avg: 0.6646, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 1.23, time: 01m 21s 244ms, time_since_start: 05m 26s 140ms, eta: 04h 57m 27s 108ms\n","\u001b[32m2022-04-20T21:52:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.6563, train/hateful_memes/cross_entropy/avg: 0.6598, train/total_loss: 0.6563, train/total_loss/avg: 0.6598, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 1.23, time: 01m 21s 310ms, time_since_start: 06m 47s 451ms, eta: 04h 56m 18s 967ms\n","\u001b[32m2022-04-20T21:53:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.6406, train/hateful_memes/cross_entropy/avg: 0.6347, train/total_loss: 0.6406, train/total_loss/avg: 0.6347, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 648ms, time_since_start: 08m 08s 100ms, eta: 04h 52m 32s 231ms\n","\u001b[32m2022-04-20T21:55:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6406, train/hateful_memes/cross_entropy/avg: 0.6166, train/total_loss: 0.6406, train/total_loss/avg: 0.6166, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 1.23, time: 01m 21s 385ms, time_since_start: 09m 29s 485ms, eta: 04h 53m 49s 852ms\n","\u001b[32m2022-04-20T21:56:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.6154, train/hateful_memes/cross_entropy/avg: 0.5910, train/total_loss: 0.6154, train/total_loss/avg: 0.5910, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 1.23, time: 01m 21s 097ms, time_since_start: 10m 50s 583ms, eta: 04h 51m 25s 011ms\n","\u001b[32m2022-04-20T21:57:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.6154, train/hateful_memes/cross_entropy/avg: 0.5801, train/total_loss: 0.6154, train/total_loss/avg: 0.5801, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 1.23, time: 01m 21s 202ms, time_since_start: 12m 11s 786ms, eta: 04h 50m 25s 011ms\n","\u001b[32m2022-04-20T21:59:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T21:59:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T21:59:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T21:59:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T21:59:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.5091, train/hateful_memes/cross_entropy/avg: 0.5708, train/total_loss: 0.5091, train/total_loss/avg: 0.5708, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 241ms, time_since_start: 13m 42s 027ms, eta: 05h 21m 12s 858ms\n","\u001b[32m2022-04-20T21:59:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T21:59:22 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-20T21:59:27 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T21:59:27 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T21:59:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T21:59:32 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-20T21:59:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T21:59:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T21:59:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.7176, val/total_loss: 0.7176, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.2472, val/hateful_memes/roc_auc: 0.6370, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 28s 800ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.637029\n","\u001b[32m2022-04-20T22:01:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.5091, train/hateful_memes/cross_entropy/avg: 0.5340, train/total_loss: 0.5091, train/total_loss/avg: 0.5340, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 1.23, time: 01m 21s 702ms, time_since_start: 15m 32s 531ms, eta: 04h 49m 26s 172ms\n","\u001b[32m2022-04-20T22:02:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.5082, train/hateful_memes/cross_entropy/avg: 0.5309, train/total_loss: 0.5082, train/total_loss/avg: 0.5309, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 1.23, time: 01m 21s 100ms, time_since_start: 16m 53s 632ms, eta: 04h 45m 55s 604ms\n","\u001b[32m2022-04-20T22:03:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.5082, train/hateful_memes/cross_entropy/avg: 0.5159, train/total_loss: 0.5082, train/total_loss/avg: 0.5159, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 1.23, time: 01m 21s 284ms, time_since_start: 18m 14s 916ms, eta: 04h 45m 11s 868ms\n","\u001b[32m2022-04-20T22:05:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.4972, train/hateful_memes/cross_entropy/avg: 0.5080, train/total_loss: 0.4972, train/total_loss/avg: 0.5080, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 778ms, time_since_start: 19m 35s 695ms, eta: 04h 42m 03s 309ms\n","\u001b[32m2022-04-20T22:06:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.4972, train/hateful_memes/cross_entropy/avg: 0.4983, train/total_loss: 0.4972, train/total_loss/avg: 0.4983, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 439ms, time_since_start: 20m 57s 134ms, eta: 04h 42m 58s 897ms\n","\u001b[32m2022-04-20T22:07:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.4931, train/hateful_memes/cross_entropy/avg: 0.4923, train/total_loss: 0.4931, train/total_loss/avg: 0.4923, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 699ms, time_since_start: 22m 17s 834ms, eta: 04h 39m 02s 597ms\n","\u001b[32m2022-04-20T22:09:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.4931, train/hateful_memes/cross_entropy/avg: 0.4891, train/total_loss: 0.4931, train/total_loss/avg: 0.4891, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 157ms, time_since_start: 23m 38s 991ms, eta: 04h 39m 14s 955ms\n","\u001b[32m2022-04-20T22:10:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.4862, train/hateful_memes/cross_entropy/avg: 0.4752, train/total_loss: 0.4862, train/total_loss/avg: 0.4752, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 1.23, time: 01m 21s 380ms, time_since_start: 25m 371ms, eta: 04h 38m 38s 356ms\n","\u001b[32m2022-04-20T22:11:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.4862, train/hateful_memes/cross_entropy/avg: 0.4559, train/total_loss: 0.4862, train/total_loss/avg: 0.4559, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 1.23, time: 01m 21s 040ms, time_since_start: 26m 21s 412ms, eta: 04h 36m 06s 003ms\n","\u001b[32m2022-04-20T22:13:19 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T22:13:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T22:13:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T22:13:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T22:13:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.4376, train/hateful_memes/cross_entropy/avg: 0.4447, train/total_loss: 0.4376, train/total_loss/avg: 0.4447, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.10, time: 01m 31s 856ms, time_since_start: 27m 53s 268ms, eta: 05h 11m 23s 601ms\n","\u001b[32m2022-04-20T22:13:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T22:13:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-20T22:13:35 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T22:13:35 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T22:13:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T22:13:40 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-20T22:13:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T22:13:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T22:13:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 0.9068, val/total_loss: 0.9068, val/hateful_memes/accuracy: 0.6556, val/hateful_memes/binary_f1: 0.4187, val/hateful_memes/roc_auc: 0.6503, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 25s 606ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.650294\n","\u001b[32m2022-04-20T22:15:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.4118, train/hateful_memes/cross_entropy/avg: 0.4319, train/total_loss: 0.4118, train/total_loss/avg: 0.4319, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 1.23, time: 01m 21s 965ms, time_since_start: 29m 40s 842ms, eta: 04h 36m 28s 405ms\n","\u001b[32m2022-04-20T22:16:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.4057, train/hateful_memes/cross_entropy/avg: 0.4209, train/total_loss: 0.4057, train/total_loss/avg: 0.4209, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 324ms, time_since_start: 31m 01s 167ms, eta: 04h 29m 34s 726ms\n","\u001b[32m2022-04-20T22:17:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.4028, train/hateful_memes/cross_entropy/avg: 0.4113, train/total_loss: 0.4028, train/total_loss/avg: 0.4113, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 830ms, time_since_start: 32m 21s 998ms, eta: 04h 29m 54s 324ms\n","\u001b[32m2022-04-20T22:19:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.3619, train/hateful_memes/cross_entropy/avg: 0.3959, train/total_loss: 0.3619, train/total_loss/avg: 0.3959, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 468ms, time_since_start: 33m 42s 467ms, eta: 04h 27m 20s 035ms\n","\u001b[32m2022-04-20T22:20:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.3352, train/hateful_memes/cross_entropy/avg: 0.3826, train/total_loss: 0.3352, train/total_loss/avg: 0.3826, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 895ms, time_since_start: 35m 03s 362ms, eta: 04h 27m 22s 760ms\n","\u001b[32m2022-04-20T22:22:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.3352, train/hateful_memes/cross_entropy/avg: 0.3850, train/total_loss: 0.3352, train/total_loss/avg: 0.3850, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 886ms, time_since_start: 36m 24s 249ms, eta: 04h 25m 58s 780ms\n","\u001b[32m2022-04-20T22:23:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.2391, train/hateful_memes/cross_entropy/avg: 0.3755, train/total_loss: 0.2391, train/total_loss/avg: 0.3755, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 108ms, time_since_start: 37m 44s 357ms, eta: 04h 22m 03s 704ms\n","\u001b[32m2022-04-20T22:24:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.2321, train/hateful_memes/cross_entropy/avg: 0.3643, train/total_loss: 0.2321, train/total_loss/avg: 0.3643, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 745ms, time_since_start: 39m 05s 102ms, eta: 04h 22m 46s 629ms\n","\u001b[32m2022-04-20T22:26:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.2011, train/hateful_memes/cross_entropy/avg: 0.3566, train/total_loss: 0.2011, train/total_loss/avg: 0.3566, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 800ms, time_since_start: 40m 25s 902ms, eta: 04h 21m 35s 165ms\n","\u001b[32m2022-04-20T22:27:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T22:27:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T22:27:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T22:27:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T22:27:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.1899, train/hateful_memes/cross_entropy/avg: 0.3451, train/total_loss: 0.1899, train/total_loss/avg: 0.3451, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.10, time: 01m 31s 480ms, time_since_start: 41m 57s 382ms, eta: 04h 54m 36s 738ms\n","\u001b[32m2022-04-20T22:27:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T22:27:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-20T22:27:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T22:27:39 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T22:27:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T22:27:44 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-20T22:27:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T22:27:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T22:27:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.4110, val/total_loss: 1.4110, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4836, val/hateful_memes/roc_auc: 0.6858, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 22s 921ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.685838\n","\u001b[32m2022-04-20T22:29:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.1899, train/hateful_memes/cross_entropy/avg: 0.3368, train/total_loss: 0.1899, train/total_loss/avg: 0.3368, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 1.22, time: 01m 22s 203ms, time_since_start: 43m 42s 509ms, eta: 04h 23m 20s 542ms\n","\u001b[32m2022-04-20T22:30:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.1747, train/hateful_memes/cross_entropy/avg: 0.3268, train/total_loss: 0.1747, train/total_loss/avg: 0.3268, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 265ms, time_since_start: 45m 02s 775ms, eta: 04h 15m 46s 500ms\n","\u001b[32m2022-04-20T22:31:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.1400, train/hateful_memes/cross_entropy/avg: 0.3171, train/total_loss: 0.1400, train/total_loss/avg: 0.3171, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 705ms, time_since_start: 46m 23s 481ms, eta: 04h 15m 48s 569ms\n","\u001b[32m2022-04-20T22:33:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.1275, train/hateful_memes/cross_entropy/avg: 0.3081, train/total_loss: 0.1275, train/total_loss/avg: 0.3081, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 665ms, time_since_start: 47m 44s 146ms, eta: 04h 14m 18s 784ms\n","\u001b[32m2022-04-20T22:34:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.1275, train/hateful_memes/cross_entropy/avg: 0.3048, train/total_loss: 0.1275, train/total_loss/avg: 0.3048, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 311ms, time_since_start: 49m 04s 458ms, eta: 04h 11m 50s 271ms\n","\u001b[32m2022-04-20T22:36:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.1091, train/hateful_memes/cross_entropy/avg: 0.2971, train/total_loss: 0.1091, train/total_loss/avg: 0.2971, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 737ms, time_since_start: 50m 25s 195ms, eta: 04h 11m 48s 202ms\n","\u001b[32m2022-04-20T22:37:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0889, train/hateful_memes/cross_entropy/avg: 0.2892, train/total_loss: 0.0889, train/total_loss/avg: 0.2892, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 667ms, time_since_start: 51m 45s 862ms, eta: 04h 10m 13s 097ms\n","\u001b[32m2022-04-20T22:38:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0639, train/hateful_memes/cross_entropy/avg: 0.2818, train/total_loss: 0.0639, train/total_loss/avg: 0.2818, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 113ms, time_since_start: 53m 05s 976ms, eta: 04h 07m 08s 532ms\n","\u001b[32m2022-04-20T22:40:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0627, train/hateful_memes/cross_entropy/avg: 0.2749, train/total_loss: 0.0627, train/total_loss/avg: 0.2749, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 558ms, time_since_start: 54m 26s 535ms, eta: 04h 07m 09s 043ms\n","\u001b[32m2022-04-20T22:41:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T22:41:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T22:41:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T22:41:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T22:41:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0414, train/hateful_memes/cross_entropy/avg: 0.2684, train/total_loss: 0.0414, train/total_loss/avg: 0.2684, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 934ms, time_since_start: 55m 57s 469ms, eta: 04h 37m 26s 425ms\n","\u001b[32m2022-04-20T22:41:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T22:41:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-20T22:41:38 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T22:41:38 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T22:41:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T22:41:44 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-20T22:41:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T22:41:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T22:41:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.4091, val/total_loss: 1.4091, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.4788, val/hateful_memes/roc_auc: 0.6892, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 24s 712ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.689250\n","\u001b[32m2022-04-20T22:43:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0249, train/hateful_memes/cross_entropy/avg: 0.2620, train/total_loss: 0.0249, train/total_loss/avg: 0.2620, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 898ms, time_since_start: 57m 44s 082ms, eta: 04h 08m 29s 046ms\n","\u001b[32m2022-04-20T22:44:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0163, train/hateful_memes/cross_entropy/avg: 0.2562, train/total_loss: 0.0163, train/total_loss/avg: 0.2562, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 765ms, time_since_start: 59m 04s 847ms, eta: 04h 03m 40s 596ms\n","\u001b[32m2022-04-20T22:46:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0158, train/hateful_memes/cross_entropy/avg: 0.2503, train/total_loss: 0.0158, train/total_loss/avg: 0.2503, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 249ms, time_since_start: 01h 25s 096ms, eta: 04h 45s 583ms\n","\u001b[32m2022-04-20T22:47:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0158, train/hateful_memes/cross_entropy/avg: 0.2494, train/total_loss: 0.0158, train/total_loss/avg: 0.2494, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 701ms, time_since_start: 01h 01m 45s 798ms, eta: 04h 44s 945ms\n","\u001b[32m2022-04-20T22:48:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0153, train/hateful_memes/cross_entropy/avg: 0.2440, train/total_loss: 0.0153, train/total_loss/avg: 0.2440, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 633ms, time_since_start: 01h 03m 06s 431ms, eta: 03h 59m 10s 814ms\n","\u001b[32m2022-04-20T22:50:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0153, train/hateful_memes/cross_entropy/avg: 0.2403, train/total_loss: 0.0153, train/total_loss/avg: 0.2403, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 226ms, time_since_start: 01h 04m 26s 658ms, eta: 03h 56m 36s 799ms\n","\u001b[32m2022-04-20T22:51:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0147, train/hateful_memes/cross_entropy/avg: 0.2352, train/total_loss: 0.0147, train/total_loss/avg: 0.2352, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 528ms, time_since_start: 01h 05m 47s 187ms, eta: 03h 56m 08s 270ms\n","\u001b[32m2022-04-20T22:52:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0147, train/hateful_memes/cross_entropy/avg: 0.2338, train/total_loss: 0.0147, train/total_loss/avg: 0.2338, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 878ms, time_since_start: 01h 07m 07s 066ms, eta: 03h 52m 52s 734ms\n","\u001b[32m2022-04-20T22:54:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0147, train/hateful_memes/cross_entropy/avg: 0.2297, train/total_loss: 0.0147, train/total_loss/avg: 0.2297, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 478ms, time_since_start: 01h 08m 27s 544ms, eta: 03h 53m 15s 716ms\n","\u001b[32m2022-04-20T22:55:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T22:55:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T22:55:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T22:55:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T22:55:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0147, train/hateful_memes/cross_entropy/avg: 0.2253, train/total_loss: 0.0147, train/total_loss/avg: 0.2253, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.11, time: 01m 30s 233ms, time_since_start: 01h 09m 57s 778ms, eta: 04h 20m 536ms\n","\u001b[32m2022-04-20T22:55:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T22:55:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-20T22:55:38 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T22:55:38 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T22:55:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T22:55:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T22:55:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T22:55:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.6753, val/total_loss: 1.6753, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.5098, val/hateful_memes/roc_auc: 0.6779, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 18s 070ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.689250\n","\u001b[32m2022-04-20T22:57:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0147, train/hateful_memes/cross_entropy/avg: 0.2218, train/total_loss: 0.0147, train/total_loss/avg: 0.2218, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 553ms, time_since_start: 01h 11m 37s 403ms, eta: 03h 53m 36s 877ms\n","\u001b[32m2022-04-20T22:58:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0099, train/hateful_memes/cross_entropy/avg: 0.2176, train/total_loss: 0.0099, train/total_loss/avg: 0.2176, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 767ms, time_since_start: 01h 12m 58s 171ms, eta: 03h 49m 59s 555ms\n","\u001b[32m2022-04-20T22:59:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0099, train/hateful_memes/cross_entropy/avg: 0.2135, train/total_loss: 0.0099, train/total_loss/avg: 0.2135, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 702ms, time_since_start: 01h 14m 18s 873ms, eta: 03h 48m 26s 414ms\n","\u001b[32m2022-04-20T23:01:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0147, train/hateful_memes/cross_entropy/avg: 0.2099, train/total_loss: 0.0147, train/total_loss/avg: 0.2099, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 068ms, time_since_start: 01h 15m 38s 942ms, eta: 03h 45m 17s 333ms\n","\u001b[32m2022-04-20T23:02:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.2061, train/total_loss: 0.0088, train/total_loss/avg: 0.2061, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 620ms, time_since_start: 01h 16m 59s 562ms, eta: 03h 45m 28s 589ms\n","\u001b[32m2022-04-20T23:03:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.2025, train/total_loss: 0.0067, train/total_loss/avg: 0.2025, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 241ms, time_since_start: 01h 18m 19s 804ms, eta: 03h 43m 03s 392ms\n","\u001b[32m2022-04-20T23:05:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1992, train/total_loss: 0.0088, train/total_loss/avg: 0.1992, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 522ms, time_since_start: 01h 19m 40s 327ms, eta: 03h 42m 28s 328ms\n","\u001b[32m2022-04-20T23:06:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1958, train/total_loss: 0.0088, train/total_loss/avg: 0.1958, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 630ms, time_since_start: 01h 21m 957ms, eta: 03h 41m 24s 120ms\n","\u001b[32m2022-04-20T23:07:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1927, train/total_loss: 0.0088, train/total_loss/avg: 0.1927, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 776ms, time_since_start: 01h 22m 20s 734ms, eta: 03h 37m 42s 419ms\n","\u001b[32m2022-04-20T23:09:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T23:09:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T23:09:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T23:09:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T23:09:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1904, train/total_loss: 0.0088, train/total_loss/avg: 0.1904, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 525ms, time_since_start: 01h 23m 50s 259ms, eta: 04h 02m 47s 561ms\n","\u001b[32m2022-04-20T23:09:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T23:09:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-20T23:09:31 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T23:09:31 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T23:09:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T23:09:38 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-20T23:09:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T23:09:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T23:09:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 1.6347, val/total_loss: 1.6347, val/hateful_memes/accuracy: 0.7000, val/hateful_memes/binary_f1: 0.5031, val/hateful_memes/roc_auc: 0.6929, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 25s 900ms, best_update: 6000, best_iteration: 6000, best_val/hateful_memes/roc_auc: 0.692893\n","\u001b[32m2022-04-20T23:11:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1874, train/total_loss: 0.0088, train/total_loss/avg: 0.1874, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 966ms, time_since_start: 01h 25m 38s 128ms, eta: 03h 40m 54s 243ms\n","\u001b[32m2022-04-20T23:12:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1849, train/total_loss: 0.0088, train/total_loss/avg: 0.1849, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 384ms, time_since_start: 01h 26m 58s 513ms, eta: 03h 35m 16s 707ms\n","\u001b[32m2022-04-20T23:13:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0112, train/hateful_memes/cross_entropy/avg: 0.1842, train/total_loss: 0.0112, train/total_loss/avg: 0.1842, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 520ms, time_since_start: 01h 28m 19s 034ms, eta: 03h 34m 16s 668ms\n","\u001b[32m2022-04-20T23:15:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1814, train/total_loss: 0.0088, train/total_loss/avg: 0.1814, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 993ms, time_since_start: 01h 29m 39s 028ms, eta: 03h 31m 31s 148ms\n","\u001b[32m2022-04-20T23:16:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1787, train/total_loss: 0.0088, train/total_loss/avg: 0.1787, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 599ms, time_since_start: 01h 30m 59s 628ms, eta: 03h 31m 45s 357ms\n","\u001b[32m2022-04-20T23:17:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0061, train/hateful_memes/cross_entropy/avg: 0.1760, train/total_loss: 0.0061, train/total_loss/avg: 0.1760, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 459ms, time_since_start: 01h 32m 20s 087ms, eta: 03h 30m 01s 378ms\n","\u001b[32m2022-04-20T23:19:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1743, train/total_loss: 0.0088, train/total_loss/avg: 0.1743, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 073ms, time_since_start: 01h 33m 40s 161ms, eta: 03h 27m 39s 582ms\n","\u001b[32m2022-04-20T23:20:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0061, train/hateful_memes/cross_entropy/avg: 0.1718, train/total_loss: 0.0061, train/total_loss/avg: 0.1718, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 445ms, time_since_start: 01h 35m 606ms, eta: 03h 27m 15s 612ms\n","\u001b[32m2022-04-20T23:21:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0061, train/hateful_memes/cross_entropy/avg: 0.1698, train/total_loss: 0.0061, train/total_loss/avg: 0.1698, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 383ms, time_since_start: 01h 36m 20s 990ms, eta: 03h 25m 44s 262ms\n","\u001b[32m2022-04-20T23:23:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T23:23:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T23:23:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T23:23:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T23:23:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0061, train/hateful_memes/cross_entropy/avg: 0.1676, train/total_loss: 0.0061, train/total_loss/avg: 0.1676, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 173ms, time_since_start: 01h 37m 50s 164ms, eta: 03h 46m 43s 424ms\n","\u001b[32m2022-04-20T23:23:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T23:23:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-20T23:23:31 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T23:23:31 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T23:23:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T23:23:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T23:23:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T23:23:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 1.6976, val/total_loss: 1.6976, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.5251, val/hateful_memes/roc_auc: 0.6826, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 17s 818ms, best_update: 6000, best_iteration: 6000, best_val/hateful_memes/roc_auc: 0.692893\n","\u001b[32m2022-04-20T23:25:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0057, train/hateful_memes/cross_entropy/avg: 0.1652, train/total_loss: 0.0057, train/total_loss/avg: 0.1652, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 604ms, time_since_start: 01h 39m 29s 588ms, eta: 03h 26m 05s 733ms\n","\u001b[32m2022-04-20T23:26:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0061, train/hateful_memes/cross_entropy/avg: 0.1630, train/total_loss: 0.0061, train/total_loss/avg: 0.1630, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 610ms, time_since_start: 01h 40m 50s 199ms, eta: 03h 22m 13s 186ms\n","\u001b[32m2022-04-20T23:27:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0084, train/hateful_memes/cross_entropy/avg: 0.1625, train/total_loss: 0.0084, train/total_loss/avg: 0.1625, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 655ms, time_since_start: 01h 42m 10s 855ms, eta: 03h 20m 57s 956ms\n","\u001b[32m2022-04-20T23:29:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0061, train/hateful_memes/cross_entropy/avg: 0.1604, train/total_loss: 0.0061, train/total_loss/avg: 0.1604, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 669ms, time_since_start: 01h 43m 31s 524ms, eta: 03h 19m 38s 011ms\n","\u001b[32m2022-04-20T23:30:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0061, train/hateful_memes/cross_entropy/avg: 0.1583, train/total_loss: 0.0061, train/total_loss/avg: 0.1583, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 222ms, time_since_start: 01h 44m 51s 747ms, eta: 03h 17m 10s 045ms\n","\u001b[32m2022-04-20T23:31:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0061, train/hateful_memes/cross_entropy/avg: 0.1562, train/total_loss: 0.0061, train/total_loss/avg: 0.1562, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 610ms, time_since_start: 01h 46m 12s 358ms, eta: 03h 16m 45s 272ms\n","\u001b[32m2022-04-20T23:33:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0057, train/hateful_memes/cross_entropy/avg: 0.1542, train/total_loss: 0.0057, train/total_loss/avg: 0.1542, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 710ms, time_since_start: 01h 47m 33s 068ms, eta: 03h 15m 37s 804ms\n","\u001b[32m2022-04-20T23:34:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0057, train/hateful_memes/cross_entropy/avg: 0.1523, train/total_loss: 0.0057, train/total_loss/avg: 0.1523, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 215ms, time_since_start: 01h 48m 53s 284ms, eta: 03h 13m 04s 275ms\n","\u001b[32m2022-04-20T23:35:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0053, train/hateful_memes/cross_entropy/avg: 0.1504, train/total_loss: 0.0053, train/total_loss/avg: 0.1504, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 597ms, time_since_start: 01h 50m 13s 882ms, eta: 03h 12m 37s 513ms\n","\u001b[32m2022-04-20T23:37:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T23:37:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T23:37:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T23:37:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T23:37:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1485, train/total_loss: 0.0050, train/total_loss/avg: 0.1485, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 737ms, time_since_start: 01h 51m 44s 620ms, eta: 03h 35m 19s 211ms\n","\u001b[32m2022-04-20T23:37:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T23:37:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-20T23:37:25 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T23:37:25 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T23:37:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T23:37:32 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-20T23:37:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T23:37:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T23:37:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 1.7085, val/total_loss: 1.7085, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.5251, val/hateful_memes/roc_auc: 0.6973, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 25s 794ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.697298\n","\u001b[32m2022-04-20T23:39:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1468, train/total_loss: 0.0050, train/total_loss/avg: 0.1468, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 1.23, time: 01m 21s 804ms, time_since_start: 01h 53m 32s 221ms, eta: 03h 12m 44s 138ms\n","\u001b[32m2022-04-20T23:40:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0046, train/hateful_memes/cross_entropy/avg: 0.1450, train/total_loss: 0.0046, train/total_loss/avg: 0.1450, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 667ms, time_since_start: 01h 54m 52s 888ms, eta: 03h 08m 41s 313ms\n","\u001b[32m2022-04-20T23:41:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0046, train/hateful_memes/cross_entropy/avg: 0.1452, train/total_loss: 0.0046, train/total_loss/avg: 0.1452, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 188ms, time_since_start: 01h 56m 13s 077ms, eta: 03h 06m 12s 615ms\n","\u001b[32m2022-04-20T23:43:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1437, train/total_loss: 0.0050, train/total_loss/avg: 0.1437, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 491ms, time_since_start: 01h 57m 33s 568ms, eta: 03h 05m 32s 880ms\n","\u001b[32m2022-04-20T23:44:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1421, train/total_loss: 0.0050, train/total_loss/avg: 0.1421, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 618ms, time_since_start: 01h 58m 54s 186ms, eta: 03h 04m 28s 454ms\n","\u001b[32m2022-04-20T23:45:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1404, train/total_loss: 0.0050, train/total_loss/avg: 0.1404, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 174ms, time_since_start: 02h 14s 360ms, eta: 03h 02m 06s 013ms\n","\u001b[32m2022-04-20T23:47:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0036, train/hateful_memes/cross_entropy/avg: 0.1388, train/total_loss: 0.0036, train/total_loss/avg: 0.1388, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 532ms, time_since_start: 02h 01m 34s 892ms, eta: 03h 01m 32s 881ms\n","\u001b[32m2022-04-20T23:48:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1372, train/total_loss: 0.0028, train/total_loss/avg: 0.1372, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 285ms, time_since_start: 02h 02m 55s 178ms, eta: 02h 59m 37s 862ms\n","\u001b[32m2022-04-20T23:49:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1359, train/total_loss: 0.0028, train/total_loss/avg: 0.1359, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 239ms, time_since_start: 02h 04m 15s 418ms, eta: 02h 58m 10s 060ms\n","\u001b[32m2022-04-20T23:51:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-20T23:51:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T23:51:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T23:51:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T23:51:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1344, train/total_loss: 0.0023, train/total_loss/avg: 0.1344, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 416ms, time_since_start: 02h 05m 47s 834ms, eta: 03h 23m 38s 341ms\n","\u001b[32m2022-04-20T23:51:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-20T23:51:24 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-20T23:51:28 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-20T23:51:28 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-20T23:51:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-20T23:51:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-20T23:51:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-20T23:51:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 1.7841, val/total_loss: 1.7841, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4909, val/hateful_memes/roc_auc: 0.6764, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 22000, val_time: 19s 567ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.697298\n","\u001b[32m2022-04-20T23:53:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1329, train/total_loss: 0.0023, train/total_loss/avg: 0.1329, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 837ms, time_since_start: 02h 07m 28s 240ms, eta: 02h 56m 45s 262ms\n","\u001b[32m2022-04-20T23:54:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1316, train/total_loss: 0.0023, train/total_loss/avg: 0.1316, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 423ms, time_since_start: 02h 08m 48s 664ms, eta: 02h 54m 29s 152ms\n","\u001b[32m2022-04-20T23:55:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1302, train/total_loss: 0.0017, train/total_loss/avg: 0.1302, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 467ms, time_since_start: 02h 10m 09s 131ms, eta: 02h 53m 13s 076ms\n","\u001b[32m2022-04-20T23:57:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1288, train/total_loss: 0.0017, train/total_loss/avg: 0.1288, max mem: 9224.0, experiment: run, epoch: 36, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 133ms, time_since_start: 02h 11m 29s 264ms, eta: 02h 51m 08s 481ms\n","\u001b[32m2022-04-20T23:58:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1275, train/total_loss: 0.0014, train/total_loss/avg: 0.1275, max mem: 9224.0, experiment: run, epoch: 36, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 425ms, time_since_start: 02h 12m 49s 690ms, eta: 02h 50m 24s 079ms\n","\u001b[32m2022-04-20T23:59:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1262, train/total_loss: 0.0017, train/total_loss/avg: 0.1262, max mem: 9224.0, experiment: run, epoch: 37, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 002ms, time_since_start: 02h 14m 09s 692ms, eta: 02h 48m 08s 942ms\n","\u001b[32m2022-04-21T00:01:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1249, train/total_loss: 0.0017, train/total_loss/avg: 0.1249, max mem: 9224.0, experiment: run, epoch: 37, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 349ms, time_since_start: 02h 15m 30s 042ms, eta: 02h 47m 31s 061ms\n","\u001b[32m2022-04-21T00:02:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1239, train/total_loss: 0.0020, train/total_loss/avg: 0.1239, max mem: 9224.0, experiment: run, epoch: 37, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 303ms, time_since_start: 02h 16m 50s 346ms, eta: 02h 46m 03s 548ms\n","\u001b[32m2022-04-21T00:03:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1228, train/total_loss: 0.0020, train/total_loss/avg: 0.1228, max mem: 9224.0, experiment: run, epoch: 38, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 141ms, time_since_start: 02h 18m 10s 487ms, eta: 02h 44m 21s 978ms\n","\u001b[32m2022-04-21T00:05:07 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T00:05:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T00:05:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T00:05:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T00:05:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1216, train/total_loss: 0.0017, train/total_loss/avg: 0.1216, max mem: 9224.0, experiment: run, epoch: 38, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 758ms, time_since_start: 02h 19m 43s 246ms, eta: 03h 08m 40s 252ms\n","\u001b[32m2022-04-21T00:05:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T00:05:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T00:05:24 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T00:05:24 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T00:05:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T00:05:29 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-21T00:05:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T00:05:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T00:05:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 1.7167, val/total_loss: 1.7167, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.4756, val/hateful_memes/roc_auc: 0.7028, num_updates: 10000, epoch: 38, iterations: 10000, max_updates: 22000, val_time: 25s 145ms, best_update: 10000, best_iteration: 10000, best_val/hateful_memes/roc_auc: 0.702846\n","\u001b[32m2022-04-21T00:07:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1210, train/total_loss: 0.0017, train/total_loss/avg: 0.1210, max mem: 9224.0, experiment: run, epoch: 38, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 1.23, time: 01m 21s 793ms, time_since_start: 02h 21m 30s 187ms, eta: 02h 44m 58s 922ms\n","\u001b[32m2022-04-21T00:08:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1200, train/total_loss: 0.0020, train/total_loss/avg: 0.1200, max mem: 9224.0, experiment: run, epoch: 39, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 1.27, time: 01m 19s 869ms, time_since_start: 02h 22m 50s 056ms, eta: 02h 39m 44s 795ms\n","\u001b[32m2022-04-21T00:09:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1205, train/total_loss: 0.0020, train/total_loss/avg: 0.1205, max mem: 9224.0, experiment: run, epoch: 39, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 506ms, time_since_start: 02h 24m 10s 563ms, eta: 02h 39m 39s 416ms\n","\u001b[32m2022-04-21T00:11:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1194, train/total_loss: 0.0013, train/total_loss/avg: 0.1194, max mem: 9224.0, experiment: run, epoch: 40, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 383ms, time_since_start: 02h 25m 30s 946ms, eta: 02h 38m 02s 970ms\n","\u001b[32m2022-04-21T00:12:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1183, train/total_loss: 0.0013, train/total_loss/avg: 0.1183, max mem: 9224.0, experiment: run, epoch: 40, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 562ms, time_since_start: 02h 26m 51s 508ms, eta: 02h 37m 02s 167ms\n","\u001b[32m2022-04-21T00:13:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1172, train/total_loss: 0.0013, train/total_loss/avg: 0.1172, max mem: 9224.0, experiment: run, epoch: 40, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 489ms, time_since_start: 02h 28m 11s 998ms, eta: 02h 35m 31s 796ms\n","\u001b[32m2022-04-21T00:15:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1161, train/total_loss: 0.0013, train/total_loss/avg: 0.1161, max mem: 9224.0, experiment: run, epoch: 41, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 1.27, time: 01m 19s 866ms, time_since_start: 02h 29m 31s 865ms, eta: 02h 32m 58s 388ms\n","\u001b[32m2022-04-21T00:16:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1150, train/total_loss: 0.0009, train/total_loss/avg: 0.1150, max mem: 9224.0, experiment: run, epoch: 41, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 402ms, time_since_start: 02h 30m 52s 267ms, eta: 02h 32m 38s 153ms\n","\u001b[32m2022-04-21T00:17:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1139, train/total_loss: 0.0009, train/total_loss/avg: 0.1139, max mem: 9224.0, experiment: run, epoch: 41, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 688ms, time_since_start: 02h 32m 12s 956ms, eta: 02h 31m 48s 662ms\n","\u001b[32m2022-04-21T00:19:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T00:19:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T00:19:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T00:19:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T00:19:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1129, train/total_loss: 0.0009, train/total_loss/avg: 0.1129, max mem: 9224.0, experiment: run, epoch: 42, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 154ms, time_since_start: 02h 33m 44s 110ms, eta: 02h 49m 57s 405ms\n","\u001b[32m2022-04-21T00:19:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T00:19:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T00:19:25 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T00:19:25 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T00:19:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T00:19:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T00:19:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T00:19:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.2914, val/total_loss: 2.2914, val/hateful_memes/accuracy: 0.7037, val/hateful_memes/binary_f1: 0.5429, val/hateful_memes/roc_auc: 0.6896, num_updates: 11000, epoch: 42, iterations: 11000, max_updates: 22000, val_time: 18s 446ms, best_update: 10000, best_iteration: 10000, best_val/hateful_memes/roc_auc: 0.702846\n","\u001b[32m2022-04-21T00:21:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1119, train/total_loss: 0.0009, train/total_loss/avg: 0.1119, max mem: 9224.0, experiment: run, epoch: 42, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 1.23, time: 01m 21s 663ms, time_since_start: 02h 35m 24s 222ms, eta: 02h 30m 52s 688ms\n","\u001b[32m2022-04-21T00:22:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1109, train/total_loss: 0.0006, train/total_loss/avg: 0.1109, max mem: 9224.0, experiment: run, epoch: 43, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 086ms, time_since_start: 02h 36m 44s 308ms, eta: 02h 26m 36s 329ms\n","\u001b[32m2022-04-21T00:23:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1099, train/total_loss: 0.0007, train/total_loss/avg: 0.1099, max mem: 9224.0, experiment: run, epoch: 43, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 533ms, time_since_start: 02h 38m 04s 841ms, eta: 02h 26m 03s 530ms\n","\u001b[32m2022-04-21T00:25:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1090, train/total_loss: 0.0006, train/total_loss/avg: 0.1090, max mem: 9224.0, experiment: run, epoch: 43, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 592ms, time_since_start: 02h 39m 25s 433ms, eta: 02h 24m 48s 015ms\n","\u001b[32m2022-04-21T00:26:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1080, train/total_loss: 0.0007, train/total_loss/avg: 0.1080, max mem: 9224.0, experiment: run, epoch: 44, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 426ms, time_since_start: 02h 40m 45s 860ms, eta: 02h 23m 08s 366ms\n","\u001b[32m2022-04-21T00:27:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1071, train/total_loss: 0.0006, train/total_loss/avg: 0.1071, max mem: 9224.0, experiment: run, epoch: 44, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 716ms, time_since_start: 02h 42m 06s 576ms, eta: 02h 22m 17s 208ms\n","\u001b[32m2022-04-21T00:29:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1062, train/total_loss: 0.0007, train/total_loss/avg: 0.1062, max mem: 9224.0, experiment: run, epoch: 44, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 585ms, time_since_start: 02h 43m 27s 162ms, eta: 02h 20m 41s 372ms\n","\u001b[32m2022-04-21T00:30:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1053, train/total_loss: 0.0007, train/total_loss/avg: 0.1053, max mem: 9224.0, experiment: run, epoch: 45, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 163ms, time_since_start: 02h 44m 47s 326ms, eta: 02h 18m 35s 730ms\n","\u001b[32m2022-04-21T00:31:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1044, train/total_loss: 0.0007, train/total_loss/avg: 0.1044, max mem: 9224.0, experiment: run, epoch: 45, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 704ms, time_since_start: 02h 46m 08s 030ms, eta: 02h 18m 09s 706ms\n","\u001b[32m2022-04-21T00:33:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T00:33:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T00:33:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T00:33:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T00:33:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1036, train/total_loss: 0.0007, train/total_loss/avg: 0.1036, max mem: 9224.0, experiment: run, epoch: 46, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 1.08, time: 01m 33s 590ms, time_since_start: 02h 47m 41s 620ms, eta: 02h 38m 38s 158ms\n","\u001b[32m2022-04-21T00:33:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T00:33:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T00:33:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T00:33:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T00:33:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T00:33:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T00:33:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T00:33:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.0286, val/total_loss: 2.0286, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4481, val/hateful_memes/roc_auc: 0.6807, num_updates: 12000, epoch: 46, iterations: 12000, max_updates: 22000, val_time: 18s 650ms, best_update: 10000, best_iteration: 10000, best_val/hateful_memes/roc_auc: 0.702846\n","\u001b[32m2022-04-21T00:34:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1032, train/total_loss: 0.0007, train/total_loss/avg: 0.1032, max mem: 9224.0, experiment: run, epoch: 46, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 1.23, time: 01m 21s 450ms, time_since_start: 02h 49m 21s 723ms, eta: 02h 16m 40s 705ms\n","\u001b[32m2022-04-21T00:36:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1026, train/total_loss: 0.0007, train/total_loss/avg: 0.1026, max mem: 9224.0, experiment: run, epoch: 46, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 748ms, time_since_start: 02h 50m 42s 472ms, eta: 02h 14m 07s 906ms\n","\u001b[32m2022-04-21T00:37:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1017, train/total_loss: 0.0007, train/total_loss/avg: 0.1017, max mem: 9224.0, experiment: run, epoch: 47, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 390ms, time_since_start: 02h 52m 02s 862ms, eta: 02h 12m 10s 420ms\n","\u001b[32m2022-04-21T00:39:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1009, train/total_loss: 0.0005, train/total_loss/avg: 0.1009, max mem: 9224.0, experiment: run, epoch: 47, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 830ms, time_since_start: 02h 53m 23s 693ms, eta: 02h 11m 31s 645ms\n","\u001b[32m2022-04-21T00:40:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1001, train/total_loss: 0.0004, train/total_loss/avg: 0.1001, max mem: 9224.0, experiment: run, epoch: 47, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 619ms, time_since_start: 02h 54m 44s 313ms, eta: 02h 09m 49s 082ms\n","\u001b[32m2022-04-21T00:41:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0993, train/total_loss: 0.0004, train/total_loss/avg: 0.0993, max mem: 9224.0, experiment: run, epoch: 48, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 650ms, time_since_start: 02h 56m 04s 963ms, eta: 02h 08m 30s 002ms\n","\u001b[32m2022-04-21T00:43:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0985, train/total_loss: 0.0004, train/total_loss/avg: 0.0985, max mem: 9224.0, experiment: run, epoch: 48, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 813ms, time_since_start: 02h 57m 25s 777ms, eta: 02h 07m 23s 440ms\n","\u001b[32m2022-04-21T00:44:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0978, train/total_loss: 0.0003, train/total_loss/avg: 0.0978, max mem: 9224.0, experiment: run, epoch: 49, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 155ms, time_since_start: 02h 58m 45s 933ms, eta: 02h 04m 59s 715ms\n","\u001b[32m2022-04-21T00:45:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0970, train/total_loss: 0.0003, train/total_loss/avg: 0.0970, max mem: 9224.0, experiment: run, epoch: 49, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 698ms, time_since_start: 03h 06s 631ms, eta: 02h 04m 28s 392ms\n","\u001b[32m2022-04-21T00:47:03 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T00:47:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T00:47:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T00:47:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T00:47:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0963, train/total_loss: 0.0002, train/total_loss/avg: 0.0963, max mem: 9224.0, experiment: run, epoch: 49, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 493ms, time_since_start: 03h 01m 39s 125ms, eta: 02h 21m 05s 966ms\n","\u001b[32m2022-04-21T00:47:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T00:47:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T00:47:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T00:47:20 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T00:47:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T00:47:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T00:47:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T00:47:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.1366, val/total_loss: 2.1366, val/hateful_memes/accuracy: 0.7037, val/hateful_memes/binary_f1: 0.5531, val/hateful_memes/roc_auc: 0.6995, num_updates: 13000, epoch: 49, iterations: 13000, max_updates: 22000, val_time: 17s 260ms, best_update: 10000, best_iteration: 10000, best_val/hateful_memes/roc_auc: 0.702846\n","\u001b[32m2022-04-21T00:48:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0955, train/total_loss: 0.0003, train/total_loss/avg: 0.0955, max mem: 9224.0, experiment: run, epoch: 50, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 1.23, time: 01m 21s 555ms, time_since_start: 03h 03m 17s 942ms, eta: 02h 03m 01s 793ms\n","\u001b[32m2022-04-21T00:50:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0948, train/total_loss: 0.0004, train/total_loss/avg: 0.0948, max mem: 9224.0, experiment: run, epoch: 50, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 707ms, time_since_start: 03h 04m 38s 650ms, eta: 02h 23s 011ms\n","\u001b[32m2022-04-21T00:51:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0944, train/total_loss: 0.0003, train/total_loss/avg: 0.0944, max mem: 9224.0, experiment: run, epoch: 50, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 945ms, time_since_start: 03h 05m 58s 595ms, eta: 01h 57m 53s 489ms\n","\u001b[32m2022-04-21T00:52:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0937, train/total_loss: 0.0004, train/total_loss/avg: 0.0937, max mem: 9224.0, experiment: run, epoch: 51, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 969ms, time_since_start: 03h 07m 19s 564ms, eta: 01h 58m 01s 725ms\n","\u001b[32m2022-04-21T00:54:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0930, train/total_loss: 0.0003, train/total_loss/avg: 0.0930, max mem: 9224.0, experiment: run, epoch: 51, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 716ms, time_since_start: 03h 08m 40s 281ms, eta: 01h 56m 17s 507ms\n","\u001b[32m2022-04-21T00:55:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0923, train/total_loss: 0.0003, train/total_loss/avg: 0.0923, max mem: 9224.0, experiment: run, epoch: 52, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 507ms, time_since_start: 03h 10m 788ms, eta: 01h 54m 37s 591ms\n","\u001b[32m2022-04-21T00:56:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0917, train/total_loss: 0.0003, train/total_loss/avg: 0.0917, max mem: 9224.0, experiment: run, epoch: 52, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 665ms, time_since_start: 03h 11m 21s 454ms, eta: 01h 53m 29s 070ms\n","\u001b[32m2022-04-21T00:58:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0910, train/total_loss: 0.0002, train/total_loss/avg: 0.0910, max mem: 9224.0, experiment: run, epoch: 52, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 493ms, time_since_start: 03h 12m 41s 948ms, eta: 01h 51m 52s 716ms\n","\u001b[32m2022-04-21T00:59:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0904, train/total_loss: 0.0002, train/total_loss/avg: 0.0904, max mem: 9224.0, experiment: run, epoch: 53, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 900ms, time_since_start: 03h 14m 01s 849ms, eta: 01h 49m 42s 002ms\n","\u001b[32m2022-04-21T01:00:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T01:00:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T01:01:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T01:01:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T01:01:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0897, train/total_loss: 0.0002, train/total_loss/avg: 0.0897, max mem: 9224.0, experiment: run, epoch: 53, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 016ms, time_since_start: 03h 15m 31s 866ms, eta: 02h 02m 03s 782ms\n","\u001b[32m2022-04-21T01:01:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T01:01:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T01:01:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T01:01:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T01:01:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T01:01:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T01:01:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T01:01:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.3228, val/total_loss: 2.3228, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.4832, val/hateful_memes/roc_auc: 0.6843, num_updates: 14000, epoch: 53, iterations: 14000, max_updates: 22000, val_time: 16s 316ms, best_update: 10000, best_iteration: 10000, best_val/hateful_memes/roc_auc: 0.702846\n","\u001b[32m2022-04-21T01:02:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0891, train/total_loss: 0.0002, train/total_loss/avg: 0.0891, max mem: 9224.0, experiment: run, epoch: 54, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 1.23, time: 01m 21s 097ms, time_since_start: 03h 17m 09s 282ms, eta: 01h 48m 35s 641ms\n","\u001b[32m2022-04-21T01:04:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0885, train/total_loss: 0.0002, train/total_loss/avg: 0.0885, max mem: 9224.0, experiment: run, epoch: 54, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 572ms, time_since_start: 03h 18m 29s 855ms, eta: 01h 46m 31s 532ms\n","\u001b[32m2022-04-21T01:05:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0879, train/total_loss: 0.0002, train/total_loss/avg: 0.0879, max mem: 9224.0, experiment: run, epoch: 54, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 482ms, time_since_start: 03h 19m 50s 338ms, eta: 01h 45m 02s 499ms\n","\u001b[32m2022-04-21T01:06:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0873, train/total_loss: 0.0002, train/total_loss/avg: 0.0873, max mem: 9224.0, experiment: run, epoch: 55, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 833ms, time_since_start: 03h 21m 10s 171ms, eta: 01h 42m 50s 457ms\n","\u001b[32m2022-04-21T01:08:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0867, train/total_loss: 0.0002, train/total_loss/avg: 0.0867, max mem: 9224.0, experiment: run, epoch: 55, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 438ms, time_since_start: 03h 22m 30s 609ms, eta: 01h 42m 15s 430ms\n","\u001b[32m2022-04-21T01:09:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0861, train/total_loss: 0.0002, train/total_loss/avg: 0.0861, max mem: 9224.0, experiment: run, epoch: 55, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 446ms, time_since_start: 03h 23m 51s 056ms, eta: 01h 40m 54s 280ms\n","\u001b[32m2022-04-21T01:10:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0855, train/total_loss: 0.0002, train/total_loss/avg: 0.0855, max mem: 9224.0, experiment: run, epoch: 56, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 092ms, time_since_start: 03h 25m 11s 148ms, eta: 01h 39m 06s 144ms\n","\u001b[32m2022-04-21T01:12:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0850, train/total_loss: 0.0002, train/total_loss/avg: 0.0850, max mem: 9224.0, experiment: run, epoch: 56, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 484ms, time_since_start: 03h 26m 31s 633ms, eta: 01h 38m 13s 369ms\n","\u001b[32m2022-04-21T01:13:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0844, train/total_loss: 0.0002, train/total_loss/avg: 0.0844, max mem: 9224.0, experiment: run, epoch: 57, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 812ms, time_since_start: 03h 27m 51s 445ms, eta: 01h 36m 02s 996ms\n","\u001b[32m2022-04-21T01:14:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T01:14:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T01:14:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T01:14:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T01:14:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0838, train/total_loss: 0.0002, train/total_loss/avg: 0.0838, max mem: 9224.0, experiment: run, epoch: 57, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 074ms, time_since_start: 03h 29m 20s 519ms, eta: 01h 45m 41s 223ms\n","\u001b[32m2022-04-21T01:14:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T01:14:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T01:15:01 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T01:15:01 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T01:15:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.5231, val/total_loss: 2.5231, val/hateful_memes/accuracy: 0.6981, val/hateful_memes/binary_f1: 0.5220, val/hateful_memes/roc_auc: 0.6847, num_updates: 15000, epoch: 57, iterations: 15000, max_updates: 22000, val_time: 05s 254ms, best_update: 10000, best_iteration: 10000, best_val/hateful_memes/roc_auc: 0.702846\n","\u001b[32m2022-04-21T01:16:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0833, train/total_loss: 0.0002, train/total_loss/avg: 0.0833, max mem: 9224.0, experiment: run, epoch: 57, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 1.23, time: 01m 21s 524ms, time_since_start: 03h 30m 47s 301ms, eta: 01h 35m 20s 833ms\n","\u001b[32m2022-04-21T01:17:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0827, train/total_loss: 0.0002, train/total_loss/avg: 0.0827, max mem: 9224.0, experiment: run, epoch: 58, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 205ms, time_since_start: 03h 32m 07s 507ms, eta: 01h 32m 26s 722ms\n","\u001b[32m2022-04-21T01:19:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0822, train/total_loss: 0.0002, train/total_loss/avg: 0.0822, max mem: 9224.0, experiment: run, epoch: 58, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 534ms, time_since_start: 03h 33m 28s 041ms, eta: 01h 31m 27s 554ms\n","\u001b[32m2022-04-21T01:20:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0816, train/total_loss: 0.0002, train/total_loss/avg: 0.0816, max mem: 9224.0, experiment: run, epoch: 58, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 387ms, time_since_start: 03h 34m 48s 429ms, eta: 01h 29m 55s 749ms\n","\u001b[32m2022-04-21T01:21:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0811, train/total_loss: 0.0001, train/total_loss/avg: 0.0811, max mem: 9224.0, experiment: run, epoch: 59, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 976ms, time_since_start: 03h 36m 08s 405ms, eta: 01h 28m 06s 864ms\n","\u001b[32m2022-04-21T01:23:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0806, train/total_loss: 0.0001, train/total_loss/avg: 0.0806, max mem: 9224.0, experiment: run, epoch: 59, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 330ms, time_since_start: 03h 37m 28s 735ms, eta: 01h 27m 08s 524ms\n","\u001b[32m2022-04-21T01:24:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0801, train/total_loss: 0.0001, train/total_loss/avg: 0.0801, max mem: 9224.0, experiment: run, epoch: 60, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 018ms, time_since_start: 03h 38m 48s 754ms, eta: 01h 25m 26s 844ms\n","\u001b[32m2022-04-21T01:25:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0796, train/total_loss: 0.0001, train/total_loss/avg: 0.0796, max mem: 9224.0, experiment: run, epoch: 60, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 450ms, time_since_start: 03h 40m 09s 204ms, eta: 01h 24m 32s 729ms\n","\u001b[32m2022-04-21T01:27:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0791, train/total_loss: 0.0001, train/total_loss/avg: 0.0791, max mem: 9224.0, experiment: run, epoch: 60, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 503ms, time_since_start: 03h 41m 29s 708ms, eta: 01h 23m 14s 191ms\n","\u001b[32m2022-04-21T01:28:26 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T01:28:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T01:28:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T01:28:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T01:28:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0786, train/total_loss: 0.0001, train/total_loss/avg: 0.0786, max mem: 9224.0, experiment: run, epoch: 61, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 1.06, time: 01m 34s 013ms, time_since_start: 03h 43m 03s 721ms, eta: 01h 35m 36s 701ms\n","\u001b[32m2022-04-21T01:28:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T01:28:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T01:28:44 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T01:28:44 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T01:28:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 2.7388, val/total_loss: 2.7388, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.5119, val/hateful_memes/roc_auc: 0.6894, num_updates: 16000, epoch: 61, iterations: 16000, max_updates: 22000, val_time: 04s 899ms, best_update: 10000, best_iteration: 10000, best_val/hateful_memes/roc_auc: 0.702846\n","\u001b[32m2022-04-21T01:30:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0781, train/total_loss: 0.0001, train/total_loss/avg: 0.0781, max mem: 9224.0, experiment: run, epoch: 61, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 1.23, time: 01m 21s 420ms, time_since_start: 03h 44m 30s 042ms, eta: 01h 21m 25s 448ms\n","\u001b[32m2022-04-21T01:31:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0776, train/total_loss: 0.0001, train/total_loss/avg: 0.0776, max mem: 9224.0, experiment: run, epoch: 61, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 616ms, time_since_start: 03h 45m 50s 658ms, eta: 01h 19m 15s 231ms\n","\u001b[32m2022-04-21T01:32:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0771, train/total_loss: 0.0001, train/total_loss/avg: 0.0771, max mem: 9224.0, experiment: run, epoch: 62, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 296ms, time_since_start: 03h 47m 10s 955ms, eta: 01h 17m 34s 715ms\n","\u001b[32m2022-04-21T01:34:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0767, train/total_loss: 0.0001, train/total_loss/avg: 0.0767, max mem: 9224.0, experiment: run, epoch: 62, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 547ms, time_since_start: 03h 48m 31s 503ms, eta: 01h 16m 27s 368ms\n","\u001b[32m2022-04-21T01:35:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0762, train/total_loss: 0.0001, train/total_loss/avg: 0.0762, max mem: 9224.0, experiment: run, epoch: 63, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 999ms, time_since_start: 03h 49m 51s 503ms, eta: 01h 14m 34s 779ms\n","\u001b[32m2022-04-21T01:36:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0757, train/total_loss: 0.0001, train/total_loss/avg: 0.0757, max mem: 9224.0, experiment: run, epoch: 63, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 608ms, time_since_start: 03h 51m 12s 111ms, eta: 01h 13m 46s 864ms\n","\u001b[32m2022-04-21T01:38:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0753, train/total_loss: 0.0001, train/total_loss/avg: 0.0753, max mem: 9224.0, experiment: run, epoch: 63, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 931ms, time_since_start: 03h 52m 33s 043ms, eta: 01h 12m 42s 310ms\n","\u001b[32m2022-04-21T01:39:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0748, train/total_loss: 0.0001, train/total_loss/avg: 0.0748, max mem: 9224.0, experiment: run, epoch: 64, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 378ms, time_since_start: 03h 53m 53s 422ms, eta: 01h 10m 50s 756ms\n","\u001b[32m2022-04-21T01:40:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0744, train/total_loss: 0.0001, train/total_loss/avg: 0.0744, max mem: 9224.0, experiment: run, epoch: 64, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 696ms, time_since_start: 03h 55m 14s 118ms, eta: 01h 09m 45s 461ms\n","\u001b[32m2022-04-21T01:42:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T01:42:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T01:42:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T01:42:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T01:42:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0739, train/total_loss: 0.0001, train/total_loss/avg: 0.0739, max mem: 9224.0, experiment: run, epoch: 64, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 393ms, time_since_start: 03h 56m 43s 512ms, eta: 01h 15m 45s 663ms\n","\u001b[32m2022-04-21T01:42:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T01:42:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T01:42:24 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T01:42:24 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T01:42:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 2.1558, val/total_loss: 2.1558, val/hateful_memes/accuracy: 0.6741, val/hateful_memes/binary_f1: 0.4824, val/hateful_memes/roc_auc: 0.6740, num_updates: 17000, epoch: 64, iterations: 17000, max_updates: 22000, val_time: 05s 223ms, best_update: 10000, best_iteration: 10000, best_val/hateful_memes/roc_auc: 0.702846\n","\u001b[32m2022-04-21T01:43:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0735, train/total_loss: 0.0001, train/total_loss/avg: 0.0735, max mem: 9224.0, experiment: run, epoch: 65, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 1.23, time: 01m 21s 158ms, time_since_start: 03h 58m 09s 895ms, eta: 01h 07m 24s 379ms\n","\u001b[32m2022-04-21T01:45:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0731, train/total_loss: 0.0001, train/total_loss/avg: 0.0731, max mem: 9224.0, experiment: run, epoch: 65, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 851ms, time_since_start: 03h 59m 30s 747ms, eta: 01h 05m 46s 872ms\n","\u001b[32m2022-04-21T01:46:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0727, train/total_loss: 0.0000, train/total_loss/avg: 0.0727, max mem: 9224.0, experiment: run, epoch: 66, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 386ms, time_since_start: 04h 51s 133ms, eta: 01h 04m 02s 373ms\n","\u001b[32m2022-04-21T01:47:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0723, train/total_loss: 0.0000, train/total_loss/avg: 0.0723, max mem: 9224.0, experiment: run, epoch: 66, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 753ms, time_since_start: 04h 02m 11s 887ms, eta: 01h 02m 57s 811ms\n","\u001b[32m2022-04-21T01:49:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0718, train/total_loss: 0.0000, train/total_loss/avg: 0.0718, max mem: 9224.0, experiment: run, epoch: 66, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 683ms, time_since_start: 04h 03m 32s 570ms, eta: 01h 01m 32s 477ms\n","\u001b[32m2022-04-21T01:50:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0714, train/total_loss: 0.0000, train/total_loss/avg: 0.0714, max mem: 9224.0, experiment: run, epoch: 67, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 197ms, time_since_start: 04h 04m 52s 767ms, eta: 59m 48s 662ms\n","\u001b[32m2022-04-21T01:51:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0710, train/total_loss: 0.0000, train/total_loss/avg: 0.0710, max mem: 9224.0, experiment: run, epoch: 67, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 728ms, time_since_start: 04h 06m 13s 496ms, eta: 58m 50s 317ms\n","\u001b[32m2022-04-21T01:53:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0706, train/total_loss: 0.0000, train/total_loss/avg: 0.0706, max mem: 9224.0, experiment: run, epoch: 67, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 610ms, time_since_start: 04h 07m 34s 106ms, eta: 57m 23s 204ms\n","\u001b[32m2022-04-21T01:54:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0702, train/total_loss: 0.0000, train/total_loss/avg: 0.0702, max mem: 9224.0, experiment: run, epoch: 68, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 483ms, time_since_start: 04h 08m 54s 590ms, eta: 55m 55s 938ms\n","\u001b[32m2022-04-21T01:55:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T01:55:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T01:55:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T01:56:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T01:56:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0698, train/total_loss: 0.0000, train/total_loss/avg: 0.0698, max mem: 9224.0, experiment: run, epoch: 68, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 160ms, time_since_start: 04h 10m 27s 750ms, eta: 01h 03m 09s 750ms\n","\u001b[32m2022-04-21T01:56:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T01:56:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T01:56:09 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T01:56:09 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T01:56:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 2.9332, val/total_loss: 2.9332, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.5015, val/hateful_memes/roc_auc: 0.6802, num_updates: 18000, epoch: 68, iterations: 18000, max_updates: 22000, val_time: 06s 658ms, best_update: 10000, best_iteration: 10000, best_val/hateful_memes/roc_auc: 0.702846\n","\u001b[32m2022-04-21T01:57:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0695, train/total_loss: 0.0000, train/total_loss/avg: 0.0695, max mem: 9224.0, experiment: run, epoch: 69, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 1.23, time: 01m 21s 335ms, time_since_start: 04h 11m 55s 749ms, eta: 53m 46s 003ms\n","\u001b[32m2022-04-21T01:58:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0691, train/total_loss: 0.0000, train/total_loss/avg: 0.0691, max mem: 9224.0, experiment: run, epoch: 69, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 763ms, time_since_start: 04h 13m 16s 512ms, eta: 52m 01s 190ms\n","\u001b[32m2022-04-21T02:00:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0687, train/total_loss: 0.0000, train/total_loss/avg: 0.0687, max mem: 9224.0, experiment: run, epoch: 69, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 701ms, time_since_start: 04h 14m 37s 213ms, eta: 50m 36s 700ms\n","\u001b[32m2022-04-21T02:01:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0683, train/total_loss: 0.0000, train/total_loss/avg: 0.0683, max mem: 9224.0, experiment: run, epoch: 70, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 436ms, time_since_start: 04h 15m 57s 650ms, eta: 49m 04s 955ms\n","\u001b[32m2022-04-21T02:02:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0679, train/total_loss: 0.0000, train/total_loss/avg: 0.0679, max mem: 9224.0, experiment: run, epoch: 70, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 807ms, time_since_start: 04h 17m 18s 458ms, eta: 47m 56s 358ms\n","\u001b[32m2022-04-21T02:04:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0676, train/total_loss: 0.0000, train/total_loss/avg: 0.0676, max mem: 9224.0, experiment: run, epoch: 70, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 725ms, time_since_start: 04h 18m 39s 184ms, eta: 46m 31s 333ms\n","\u001b[32m2022-04-21T02:05:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0672, train/total_loss: 0.0000, train/total_loss/avg: 0.0672, max mem: 9224.0, experiment: run, epoch: 71, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 169ms, time_since_start: 04h 19m 59s 354ms, eta: 44m 50s 583ms\n","\u001b[32m2022-04-21T02:06:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0669, train/total_loss: 0.0000, train/total_loss/avg: 0.0669, max mem: 9224.0, experiment: run, epoch: 71, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 808ms, time_since_start: 04h 21m 20s 163ms, eta: 43m 49s 843ms\n","\u001b[32m2022-04-21T02:08:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0665, train/total_loss: 0.0000, train/total_loss/avg: 0.0665, max mem: 9224.0, experiment: run, epoch: 72, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 384ms, time_since_start: 04h 22m 40s 547ms, eta: 42m 14s 286ms\n","\u001b[32m2022-04-21T02:09:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T02:09:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T02:09:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T02:09:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T02:09:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0662, train/total_loss: 0.0000, train/total_loss/avg: 0.0662, max mem: 9224.0, experiment: run, epoch: 72, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 281ms, time_since_start: 04h 24m 11s 829ms, eta: 46m 25s 002ms\n","\u001b[32m2022-04-21T02:09:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T02:09:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T02:09:53 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T02:09:53 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T02:09:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.1788, val/total_loss: 3.1788, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4909, val/hateful_memes/roc_auc: 0.6798, num_updates: 19000, epoch: 72, iterations: 19000, max_updates: 22000, val_time: 05s 539ms, best_update: 10000, best_iteration: 10000, best_val/hateful_memes/roc_auc: 0.702846\n","\u001b[32m2022-04-21T02:11:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0658, train/total_loss: 0.0000, train/total_loss/avg: 0.0658, max mem: 9224.0, experiment: run, epoch: 72, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 1.23, time: 01m 21s 737ms, time_since_start: 04h 25m 39s 108ms, eta: 40m 10s 677ms\n","\u001b[32m2022-04-21T02:12:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0655, train/total_loss: 0.0000, train/total_loss/avg: 0.0655, max mem: 9224.0, experiment: run, epoch: 73, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 745ms, time_since_start: 04h 26m 59s 853ms, eta: 38m 19s 301ms\n","\u001b[32m2022-04-21T02:13:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0651, train/total_loss: 0.0000, train/total_loss/avg: 0.0651, max mem: 9224.0, experiment: run, epoch: 73, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 1.23, time: 01m 21s 218ms, time_since_start: 04h 28m 21s 072ms, eta: 37m 10s 186ms\n","\u001b[32m2022-04-21T02:15:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0648, train/total_loss: 0.0000, train/total_loss/avg: 0.0648, max mem: 9224.0, experiment: run, epoch: 73, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 798ms, time_since_start: 04h 29m 41s 870ms, eta: 35m 36s 464ms\n","\u001b[32m2022-04-21T02:16:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0645, train/total_loss: 0.0000, train/total_loss/avg: 0.0645, max mem: 9224.0, experiment: run, epoch: 74, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 591ms, time_since_start: 04h 31m 02s 462ms, eta: 34m 09s 052ms\n","\u001b[32m2022-04-21T02:17:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0641, train/total_loss: 0.0000, train/total_loss/avg: 0.0641, max mem: 9224.0, experiment: run, epoch: 74, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 864ms, time_since_start: 04h 32m 23s 327ms, eta: 32m 53s 744ms\n","\u001b[32m2022-04-21T02:19:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0638, train/total_loss: 0.0000, train/total_loss/avg: 0.0638, max mem: 9224.0, experiment: run, epoch: 75, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 242ms, time_since_start: 04h 33m 43s 569ms, eta: 31m 16s 951ms\n","\u001b[32m2022-04-21T02:20:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0635, train/total_loss: 0.0000, train/total_loss/avg: 0.0635, max mem: 9224.0, experiment: run, epoch: 75, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 846ms, time_since_start: 04h 35m 04s 416ms, eta: 30m 08s 856ms\n","\u001b[32m2022-04-21T02:22:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0632, train/total_loss: 0.0000, train/total_loss/avg: 0.0632, max mem: 9224.0, experiment: run, epoch: 75, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 787ms, time_since_start: 04h 36m 25s 203ms, eta: 28m 45s 374ms\n","\u001b[32m2022-04-21T02:23:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T02:23:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T02:23:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T02:23:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T02:23:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0628, train/total_loss: 0.0000, train/total_loss/avg: 0.0628, max mem: 9224.0, experiment: run, epoch: 76, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 054ms, time_since_start: 04h 37m 55s 258ms, eta: 30m 31s 709ms\n","\u001b[32m2022-04-21T02:23:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T02:23:31 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T02:23:36 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T02:23:36 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T02:23:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 2.5364, val/total_loss: 2.5364, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4970, val/hateful_memes/roc_auc: 0.6804, num_updates: 20000, epoch: 76, iterations: 20000, max_updates: 22000, val_time: 05s 498ms, best_update: 10000, best_iteration: 10000, best_val/hateful_memes/roc_auc: 0.702846\n","\u001b[32m2022-04-21T02:24:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0625, train/total_loss: 0.0000, train/total_loss/avg: 0.0625, max mem: 9224.0, experiment: run, epoch: 76, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.23, time: 01m 21s 396ms, time_since_start: 04h 39m 22s 154ms, eta: 26m 12s 818ms\n","\u001b[32m2022-04-21T02:26:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0622, train/total_loss: 0.0000, train/total_loss/avg: 0.0622, max mem: 9224.0, experiment: run, epoch: 76, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 505ms, time_since_start: 04h 40m 42s 659ms, eta: 24m 33s 727ms\n","\u001b[32m2022-04-21T02:27:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0619, train/total_loss: 0.0000, train/total_loss/avg: 0.0619, max mem: 9224.0, experiment: run, epoch: 77, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.27, time: 01m 19s 793ms, time_since_start: 04h 42m 02s 452ms, eta: 22m 59s 547ms\n","\u001b[32m2022-04-21T02:28:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0616, train/total_loss: 0.0000, train/total_loss/avg: 0.0616, max mem: 9224.0, experiment: run, epoch: 77, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 529ms, time_since_start: 04h 43m 22s 981ms, eta: 21m 50s 371ms\n","\u001b[32m2022-04-21T02:30:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0613, train/total_loss: 0.0000, train/total_loss/avg: 0.0613, max mem: 9224.0, experiment: run, epoch: 78, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 125ms, time_since_start: 04h 44m 43s 106ms, eta: 20m 22s 308ms\n","\u001b[32m2022-04-21T02:31:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0610, train/total_loss: 0.0000, train/total_loss/avg: 0.0610, max mem: 9224.0, experiment: run, epoch: 78, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 542ms, time_since_start: 04h 46m 03s 649ms, eta: 19m 06s 765ms\n","\u001b[32m2022-04-21T02:33:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0607, train/total_loss: 0.0001, train/total_loss/avg: 0.0607, max mem: 9224.0, experiment: run, epoch: 78, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 680ms, time_since_start: 04h 47m 24s 329ms, eta: 17m 46s 674ms\n","\u001b[32m2022-04-21T02:34:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0604, train/total_loss: 0.0001, train/total_loss/avg: 0.0604, max mem: 9224.0, experiment: run, epoch: 79, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 454ms, time_since_start: 04h 48m 44s 784ms, eta: 16m 21s 867ms\n","\u001b[32m2022-04-21T02:35:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0601, train/total_loss: 0.0001, train/total_loss/avg: 0.0601, max mem: 9224.0, experiment: run, epoch: 79, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 693ms, time_since_start: 04h 50m 05s 478ms, eta: 15m 02s 723ms\n","\u001b[32m2022-04-21T02:37:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T02:37:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T02:37:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T02:37:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T02:37:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0599, train/total_loss: 0.0001, train/total_loss/avg: 0.0599, max mem: 9224.0, experiment: run, epoch: 79, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 415ms, time_since_start: 04h 51m 34s 893ms, eta: 15m 09s 351ms\n","\u001b[32m2022-04-21T02:37:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T02:37:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T02:37:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T02:37:15 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T02:37:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 2.8045, val/total_loss: 2.8045, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.5059, val/hateful_memes/roc_auc: 0.6802, num_updates: 21000, epoch: 79, iterations: 21000, max_updates: 22000, val_time: 04s 830ms, best_update: 10000, best_iteration: 10000, best_val/hateful_memes/roc_auc: 0.702846\n","\u001b[32m2022-04-21T02:38:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0596, train/total_loss: 0.0001, train/total_loss/avg: 0.0596, max mem: 9224.0, experiment: run, epoch: 80, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 1.23, time: 01m 21s 385ms, time_since_start: 04h 53m 01s 110ms, eta: 12m 24s 925ms\n","\u001b[32m2022-04-21T02:39:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0593, train/total_loss: 0.0001, train/total_loss/avg: 0.0593, max mem: 9224.0, experiment: run, epoch: 80, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 814ms, time_since_start: 04h 54m 21s 925ms, eta: 10m 57s 504ms\n","\u001b[32m2022-04-21T02:41:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0590, train/total_loss: 0.0000, train/total_loss/avg: 0.0590, max mem: 9224.0, experiment: run, epoch: 81, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 230ms, time_since_start: 04h 55m 42s 155ms, eta: 09m 31s 158ms\n","\u001b[32m2022-04-21T02:42:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0587, train/total_loss: 0.0000, train/total_loss/avg: 0.0587, max mem: 9224.0, experiment: run, epoch: 81, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 706ms, time_since_start: 04h 57m 02s 862ms, eta: 08m 12s 472ms\n","\u001b[32m2022-04-21T02:44:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0585, train/total_loss: 0.0000, train/total_loss/avg: 0.0585, max mem: 9224.0, experiment: run, epoch: 81, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 800ms, time_since_start: 04h 58m 23s 662ms, eta: 06m 50s 868ms\n","\u001b[32m2022-04-21T02:45:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0582, train/total_loss: 0.0000, train/total_loss/avg: 0.0582, max mem: 9224.0, experiment: run, epoch: 82, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 649ms, time_since_start: 04h 59m 44s 312ms, eta: 05m 28s 083ms\n","\u001b[32m2022-04-21T02:46:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0579, train/total_loss: 0.0000, train/total_loss/avg: 0.0579, max mem: 9224.0, experiment: run, epoch: 82, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 818ms, time_since_start: 05h 01m 05s 130ms, eta: 04m 06s 576ms\n","\u001b[32m2022-04-21T02:48:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0577, train/total_loss: 0.0000, train/total_loss/avg: 0.0577, max mem: 9224.0, experiment: run, epoch: 82, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 793ms, time_since_start: 05h 02m 25s 923ms, eta: 02m 44s 333ms\n"]}],"source":["!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml \\\n","training.batch_size=32 \\\n","model=visual_bert dataset=hateful_memes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22484,"status":"ok","timestamp":1650511437533,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"IgVfrVOdy7ie","outputId":"14ebda06-aa8f-4396-851a-e930af0dac70"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-21T03:23:41 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml\n","\u001b[32m2022-04-21T03:23:41 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-21T03:23:41 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-21T03:23:41 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n","\u001b[32m2022-04-21T03:23:41 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-04-21T03:23:41 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-21T03:23:41 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-21T03:23:41 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False'])\n","\u001b[32m2022-04-21T03:23:41 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-21T03:23:41 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-04-21T03:23:41 | mmf_cli.run: \u001b[0mUsing seed 41822346\n","\u001b[32m2022-04-21T03:23:41 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-21T03:23:43 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T03:23:43 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T03:23:43 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T03:23:43 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.bias', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-21T03:23:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-21T03:23:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-21T03:23:49 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T03:23:51 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T03:23:51 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-21T03:23:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-21T03:23:51 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 10000\n","\u001b[32m2022-04-21T03:23:51 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 10000\n","\u001b[32m2022-04-21T03:23:51 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 38\n","\u001b[32m2022-04-21T03:23:51 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-21T03:23:51 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-21T03:23:51 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-21T03:23:51 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2022-04-21T03:23:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 9/9 [00:05<00:00,  1.78it/s]\n","\u001b[32m2022-04-21T03:23:56 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 9\n","\u001b[32m2022-04-21T03:23:56 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T03:23:56 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 1.7167, val/total_loss: 1.7167, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.4756, val/hateful_memes/roc_auc: 0.7028\n","\u001b[32m2022-04-21T03:23:56 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 07s 370ms\n"]}],"source":["!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml\" \\\n","model=visual_bert \\\n","dataset=hateful_memes \\\n","run_type=val \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73388,"status":"ok","timestamp":1650906449312,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"yRL8Mx-Rr3gU","outputId":"6ff8ae35-7fe9-4d80-f5a5-051d6686486f"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-25T17:06:21 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml\n","\u001b[32m2022-04-25T17:06:21 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-25T17:06:21 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-25T17:06:21 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n","\u001b[32m2022-04-25T17:06:21 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save_visual_bert_default_bs32/best.ckpt\n","\u001b[32m2022-04-25T17:06:21 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-25T17:06:21 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-25T17:06:21 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save_visual_bert_default_bs32/best.ckpt', 'checkpoint.resume_pretrained=False'])\n","\u001b[32m2022-04-25T17:06:21 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-25T17:06:21 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n","\u001b[32m2022-04-25T17:06:21 | mmf_cli.run: \u001b[0mUsing seed 21114096\n","\u001b[32m2022-04-25T17:06:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-25T17:06:29 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T17:06:29 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T17:06:29 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T17:06:29 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.position_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-25T17:06:35 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-25T17:06:35 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-25T17:06:35 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-25T17:06:58 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-25T17:06:58 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-25T17:06:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-25T17:06:58 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 10000\n","\u001b[32m2022-04-25T17:06:58 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 10000\n","\u001b[32m2022-04-25T17:06:58 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 38\n","\u001b[32m2022-04-25T17:06:58 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-25T17:06:58 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-25T17:06:58 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-25T17:06:58 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-04-25T17:06:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 32/32 [00:29<00:00,  1.10it/s]\n","\u001b[32m2022-04-25T17:07:27 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 32\n","\u001b[32m2022-04-25T17:07:27 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T17:07:28 | mmf.trainers.callbacks.logistics: \u001b[0mtest/hateful_memes/cross_entropy: 1.7315, test/total_loss: 1.7315, test/hateful_memes/accuracy: 0.6985, test/hateful_memes/binary_f1: 0.4668, test/hateful_memes/roc_auc: 0.7151\n","\u001b[32m2022-04-25T17:07:28 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 52s 262ms\n"]}],"source":["!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml\" \\\n","model=visual_bert \\\n","dataset=hateful_memes \\\n","run_type=test \\\n","checkpoint.resume_file=save_visual_bert_default_bs32/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23074,"status":"ok","timestamp":1650511687408,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"ryZ5SGoXzXR5","outputId":"2756dcbf-7439-4ae1-d6c2-0f2c32f8c98f"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-21T03:27:51 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml\n","\u001b[32m2022-04-21T03:27:51 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-21T03:27:51 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-21T03:27:51 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n","\u001b[32m2022-04-21T03:27:51 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-04-21T03:27:51 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","\u001b[32m2022-04-21T03:27:51 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-21T03:27:51 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-21T03:27:51 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n","\u001b[32m2022-04-21T03:27:51 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-21T03:27:51 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-04-21T03:27:51 | mmf_cli.run: \u001b[0mUsing seed 51093018\n","\u001b[32m2022-04-21T03:27:51 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-21T03:27:52 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T03:27:52 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T03:27:52 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T03:27:52 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.bias', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.position_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-21T03:27:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-21T03:27:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-21T03:27:58 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T03:28:00 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T03:28:00 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-21T03:28:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-21T03:28:00 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 10000\n","\u001b[32m2022-04-21T03:28:00 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 10000\n","\u001b[32m2022-04-21T03:28:00 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 38\n","\u001b[32m2022-04-21T03:28:00 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n","\u001b[32m2022-04-21T03:28:00 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 9/9 [00:06<00:00,  1.49it/s]\n","\u001b[32m2022-04-21T03:28:06 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes/save/hateful_memes_visual_bert_51093018/reports/hateful_memes_run_val_2022-04-21T03:28:06.csv\n","\u001b[32m2022-04-21T03:28:06 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 9\n","\u001b[32m2022-04-21T03:28:06 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"]}],"source":["!mmf_predict config=\"mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml\" \\\n","model=visual_bert \\\n","dataset=hateful_memes \\\n","run_type=val \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38172,"status":"ok","timestamp":1650511731761,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"mY5q7uI7KVMY","outputId":"bd28f162-e130-4e40-9b95-6d50d3f59d9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-21T03:28:20 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml\n","\u001b[32m2022-04-21T03:28:20 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-21T03:28:20 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-21T03:28:20 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n","\u001b[32m2022-04-21T03:28:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-04-21T03:28:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","\u001b[32m2022-04-21T03:28:20 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-21T03:28:20 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-21T03:28:20 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n","\u001b[32m2022-04-21T03:28:20 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-21T03:28:20 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-04-21T03:28:20 | mmf_cli.run: \u001b[0mUsing seed 20218220\n","\u001b[32m2022-04-21T03:28:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-21T03:28:21 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T03:28:21 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T03:28:21 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T03:28:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-21T03:28:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-21T03:28:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-21T03:28:27 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T03:28:29 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T03:28:29 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-21T03:28:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-21T03:28:29 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 10000\n","\u001b[32m2022-04-21T03:28:29 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 10000\n","\u001b[32m2022-04-21T03:28:29 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 38\n","\u001b[32m2022-04-21T03:28:29 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n","\u001b[32m2022-04-21T03:28:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 32/32 [00:21<00:00,  1.48it/s]\n","\u001b[32m2022-04-21T03:28:51 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes/save/hateful_memes_visual_bert_20218220/reports/hateful_memes_run_test_2022-04-21T03:28:51.csv\n","\u001b[32m2022-04-21T03:28:51 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 32\n","\u001b[32m2022-04-21T03:28:51 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"]}],"source":["!mmf_predict config=\"mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/defaults.yaml\" \\\n","model=visual_bert \\\n","dataset=hateful_memes \\\n","run_type=test \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"markdown","metadata":{"id":"cVhvucsVLFZb"},"source":["# **Visual Bert COCO**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Qdu_xceKcVz","outputId":"9c2f6069-49db-4b46-e5ba-b02f7f145688"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-21T03:35:20 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco.yaml\n","\u001b[32m2022-04-21T03:35:20 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n","\u001b[32m2022-04-21T03:35:20 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-21T03:35:20 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-21T03:35:20 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-21T03:35:21 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco.yaml', 'training.batch_size=32', 'model=visual_bert', 'dataset=hateful_memes'])\n","\u001b[32m2022-04-21T03:35:21 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-21T03:35:21 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-04-21T03:35:21 | mmf_cli.run: \u001b[0mUsing seed 21045833\n","\u001b[32m2022-04-21T03:35:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-21T03:35:22 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T03:35:22 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T03:35:22 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T03:35:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-21T03:35:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-21T03:35:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-21T03:35:28 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T03:35:29 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T03:35:29 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T03:35:29 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T03:35:29 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2022-04-21T03:35:29 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-21T03:35:29 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-21T03:35:29 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-21T03:35:29 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-04-21T03:36:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.5515, train/hateful_memes/cross_entropy/avg: 0.5515, train/total_loss: 0.5515, train/total_loss/avg: 0.5515, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.23, time: 01m 21s 493ms, time_since_start: 01m 22s 462ms, eta: 05h 02m 30s 576ms\n","\u001b[32m2022-04-21T03:38:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.5515, train/hateful_memes/cross_entropy/avg: 0.5801, train/total_loss: 0.5515, train/total_loss/avg: 0.5801, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 482ms, time_since_start: 02m 42s 944ms, eta: 04h 57m 23s 374ms\n","\u001b[32m2022-04-21T03:39:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.5515, train/hateful_memes/cross_entropy/avg: 0.5603, train/total_loss: 0.5515, train/total_loss/avg: 0.5603, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 265ms, time_since_start: 04m 03s 210ms, eta: 04h 55m 13s 765ms\n","\u001b[32m2022-04-21T03:40:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.5464, train/hateful_memes/cross_entropy/avg: 0.5569, train/total_loss: 0.5464, train/total_loss/avg: 0.5569, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 502ms, time_since_start: 05m 23s 712ms, eta: 04h 54m 44s 148ms\n","\u001b[32m2022-04-21T03:42:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.5464, train/hateful_memes/cross_entropy/avg: 0.5190, train/total_loss: 0.5464, train/total_loss/avg: 0.5190, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 546ms, time_since_start: 06m 44s 258ms, eta: 04h 53m 31s 846ms\n","\u001b[32m2022-04-21T03:43:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.5464, train/hateful_memes/cross_entropy/avg: 0.5247, train/total_loss: 0.5464, train/total_loss/avg: 0.5247, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 973ms, time_since_start: 08m 04s 232ms, eta: 04h 50m 05s 207ms\n","\u001b[32m2022-04-21T03:44:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.5464, train/hateful_memes/cross_entropy/avg: 0.5088, train/total_loss: 0.5464, train/total_loss/avg: 0.5088, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 380ms, time_since_start: 09m 24s 612ms, eta: 04h 50m 12s 015ms\n","\u001b[32m2022-04-21T03:46:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.5209, train/hateful_memes/cross_entropy/avg: 0.4953, train/total_loss: 0.5209, train/total_loss/avg: 0.4953, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 130ms, time_since_start: 10m 44s 743ms, eta: 04h 47m 56s 546ms\n","\u001b[32m2022-04-21T03:47:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.5209, train/hateful_memes/cross_entropy/avg: 0.4746, train/total_loss: 0.5209, train/total_loss/avg: 0.4746, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 345ms, time_since_start: 12m 05s 088ms, eta: 04h 47m 21s 110ms\n","\u001b[32m2022-04-21T03:48:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T03:48:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T03:49:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T03:49:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T03:49:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.4138, train/hateful_memes/cross_entropy/avg: 0.4494, train/total_loss: 0.4138, train/total_loss/avg: 0.4494, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 474ms, time_since_start: 13m 36s 563ms, eta: 05h 25m 36s 316ms\n","\u001b[32m2022-04-21T03:49:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T03:49:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T03:49:10 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T03:49:10 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T03:49:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T03:49:14 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-21T03:49:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T03:49:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T03:49:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.9312, val/total_loss: 0.9312, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.3902, val/hateful_memes/roc_auc: 0.7175, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 21s 228ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.717544\n","\u001b[32m2022-04-21T03:50:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.4138, train/hateful_memes/cross_entropy/avg: 0.4220, train/total_loss: 0.4138, train/total_loss/avg: 0.4220, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 1.23, time: 01m 21s 410ms, time_since_start: 15m 19s 205ms, eta: 04h 48m 24s 147ms\n","\u001b[32m2022-04-21T03:52:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.4004, train/hateful_memes/cross_entropy/avg: 0.3943, train/total_loss: 0.4004, train/total_loss/avg: 0.3943, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 550ms, time_since_start: 16m 39s 755ms, eta: 04h 43m 59s 399ms\n","\u001b[32m2022-04-21T03:53:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.4004, train/hateful_memes/cross_entropy/avg: 0.3787, train/total_loss: 0.4004, train/total_loss/avg: 0.3787, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 403ms, time_since_start: 18m 158ms, eta: 04h 42m 06s 364ms\n","\u001b[32m2022-04-21T03:54:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.3675, train/hateful_memes/cross_entropy/avg: 0.3548, train/total_loss: 0.3675, train/total_loss/avg: 0.3548, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 205ms, time_since_start: 19m 20s 364ms, eta: 04h 40m 03s 244ms\n","\u001b[32m2022-04-21T03:56:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.3675, train/hateful_memes/cross_entropy/avg: 0.3409, train/total_loss: 0.3675, train/total_loss/avg: 0.3409, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 650ms, time_since_start: 20m 41s 014ms, eta: 04h 40m 14s 354ms\n","\u001b[32m2022-04-21T03:57:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.3095, train/hateful_memes/cross_entropy/avg: 0.3255, train/total_loss: 0.3095, train/total_loss/avg: 0.3255, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 047ms, time_since_start: 22m 01s 062ms, eta: 04h 36m 47s 326ms\n","\u001b[32m2022-04-21T03:58:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.3095, train/hateful_memes/cross_entropy/avg: 0.3113, train/total_loss: 0.3095, train/total_loss/avg: 0.3113, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 118ms, time_since_start: 23m 21s 180ms, eta: 04h 35m 40s 490ms\n","\u001b[32m2022-04-21T04:00:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.2223, train/hateful_memes/cross_entropy/avg: 0.3017, train/total_loss: 0.2223, train/total_loss/avg: 0.3017, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 304ms, time_since_start: 24m 41s 485ms, eta: 04h 34m 57s 306ms\n","\u001b[32m2022-04-21T04:01:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.2223, train/hateful_memes/cross_entropy/avg: 0.2885, train/total_loss: 0.2223, train/total_loss/avg: 0.2885, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 1.27, time: 01m 19s 967ms, time_since_start: 26m 01s 453ms, eta: 04h 32m 26s 740ms\n","\u001b[32m2022-04-21T04:02:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T04:02:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T04:02:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T04:02:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T04:02:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.1922, train/hateful_memes/cross_entropy/avg: 0.2796, train/total_loss: 0.1922, train/total_loss/avg: 0.2796, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.12, time: 01m 29s 233ms, time_since_start: 27m 30s 686ms, eta: 05h 02m 30s 078ms\n","\u001b[32m2022-04-21T04:02:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T04:02:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T04:03:03 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T04:03:03 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T04:03:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T04:03:10 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-21T04:03:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T04:03:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T04:03:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.0138, val/total_loss: 1.0138, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.5307, val/hateful_memes/roc_auc: 0.7214, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 21s 970ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.721441\n","\u001b[32m2022-04-21T04:04:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.1478, train/hateful_memes/cross_entropy/avg: 0.2691, train/total_loss: 0.1478, train/total_loss/avg: 0.2691, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 1.23, time: 01m 21s 697ms, time_since_start: 29m 14s 355ms, eta: 04h 35m 34s 202ms\n","\u001b[32m2022-04-21T04:06:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.1478, train/hateful_memes/cross_entropy/avg: 0.2728, train/total_loss: 0.1478, train/total_loss/avg: 0.2728, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 353ms, time_since_start: 30m 34s 709ms, eta: 04h 29m 40s 504ms\n","\u001b[32m2022-04-21T04:07:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.1478, train/hateful_memes/cross_entropy/avg: 0.2744, train/total_loss: 0.1478, train/total_loss/avg: 0.2744, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 614ms, time_since_start: 31m 55s 324ms, eta: 04h 29m 11s 051ms\n","\u001b[32m2022-04-21T04:08:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.1457, train/hateful_memes/cross_entropy/avg: 0.2646, train/total_loss: 0.1457, train/total_loss/avg: 0.2646, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 413ms, time_since_start: 33m 15s 738ms, eta: 04h 27m 09s 082ms\n","\u001b[32m2022-04-21T04:10:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.1384, train/hateful_memes/cross_entropy/avg: 0.2574, train/total_loss: 0.1384, train/total_loss/avg: 0.2574, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 748ms, time_since_start: 34m 36s 486ms, eta: 04h 26m 53s 630ms\n","\u001b[32m2022-04-21T04:11:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.1112, train/hateful_memes/cross_entropy/avg: 0.2487, train/total_loss: 0.1112, train/total_loss/avg: 0.2487, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 600ms, time_since_start: 35m 57s 086ms, eta: 04h 25m 02s 257ms\n","\u001b[32m2022-04-21T04:12:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.0942, train/hateful_memes/cross_entropy/avg: 0.2408, train/total_loss: 0.0942, train/total_loss/avg: 0.2408, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 062ms, time_since_start: 37m 17s 148ms, eta: 04h 21m 54s 657ms\n","\u001b[32m2022-04-21T04:14:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.0896, train/hateful_memes/cross_entropy/avg: 0.2329, train/total_loss: 0.0896, train/total_loss/avg: 0.2329, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 778ms, time_since_start: 38m 37s 927ms, eta: 04h 22m 53s 189ms\n","\u001b[32m2022-04-21T04:15:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.0842, train/hateful_memes/cross_entropy/avg: 0.2253, train/total_loss: 0.0842, train/total_loss/avg: 0.2253, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 831ms, time_since_start: 39m 58s 759ms, eta: 04h 21m 41s 279ms\n","\u001b[32m2022-04-21T04:16:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T04:16:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T04:16:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T04:16:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T04:16:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.0839, train/hateful_memes/cross_entropy/avg: 0.2183, train/total_loss: 0.0839, train/total_loss/avg: 0.2183, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.10, time: 01m 31s 775ms, time_since_start: 41m 30s 534ms, eta: 04h 55m 33s 708ms\n","\u001b[32m2022-04-21T04:16:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T04:16:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T04:17:03 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T04:17:03 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T04:17:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T04:17:08 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-21T04:17:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T04:17:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T04:17:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.5560, val/total_loss: 1.5560, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4880, val/hateful_memes/roc_auc: 0.7220, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 23s 182ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T04:18:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.0576, train/hateful_memes/cross_entropy/avg: 0.2125, train/total_loss: 0.0576, train/total_loss/avg: 0.2125, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 1.23, time: 01m 21s 773ms, time_since_start: 43m 15s 492ms, eta: 04h 21m 57s 957ms\n","\u001b[32m2022-04-21T04:20:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.0576, train/hateful_memes/cross_entropy/avg: 0.2143, train/total_loss: 0.0576, train/total_loss/avg: 0.2143, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 1.27, time: 01m 19s 977ms, time_since_start: 44m 35s 470ms, eta: 04h 14m 51s 415ms\n","\u001b[32m2022-04-21T04:21:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0514, train/hateful_memes/cross_entropy/avg: 0.2089, train/total_loss: 0.0514, train/total_loss/avg: 0.2089, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 443ms, time_since_start: 45m 55s 913ms, eta: 04h 14m 58s 654ms\n","\u001b[32m2022-04-21T04:22:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0514, train/hateful_memes/cross_entropy/avg: 0.2029, train/total_loss: 0.0514, train/total_loss/avg: 0.2029, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 483ms, time_since_start: 47m 16s 397ms, eta: 04h 13m 44s 517ms\n","\u001b[32m2022-04-21T04:24:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0514, train/hateful_memes/cross_entropy/avg: 0.1994, train/total_loss: 0.0514, train/total_loss/avg: 0.1994, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 1.27, time: 01m 19s 974ms, time_since_start: 48m 36s 372ms, eta: 04h 10m 46s 875ms\n","\u001b[32m2022-04-21T04:25:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0405, train/hateful_memes/cross_entropy/avg: 0.1940, train/total_loss: 0.0405, train/total_loss/avg: 0.1940, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 478ms, time_since_start: 49m 56s 851ms, eta: 04h 10m 59s 860ms\n","\u001b[32m2022-04-21T04:26:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0405, train/hateful_memes/cross_entropy/avg: 0.1902, train/total_loss: 0.0405, train/total_loss/avg: 0.1902, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 392ms, time_since_start: 51m 17s 244ms, eta: 04h 09m 21s 949ms\n","\u001b[32m2022-04-21T04:28:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0375, train/hateful_memes/cross_entropy/avg: 0.1852, train/total_loss: 0.0375, train/total_loss/avg: 0.1852, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 1.27, time: 01m 19s 902ms, time_since_start: 52m 37s 146ms, eta: 04h 06m 29s 400ms\n","\u001b[32m2022-04-21T04:29:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0375, train/hateful_memes/cross_entropy/avg: 0.1827, train/total_loss: 0.0375, train/total_loss/avg: 0.1827, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 419ms, time_since_start: 53m 57s 565ms, eta: 04h 06m 43s 327ms\n","\u001b[32m2022-04-21T04:30:46 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T04:30:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T04:30:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T04:30:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T04:30:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0361, train/hateful_memes/cross_entropy/avg: 0.1782, train/total_loss: 0.0361, train/total_loss/avg: 0.1782, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.12, time: 01m 29s 949ms, time_since_start: 55m 27s 514ms, eta: 04h 34m 26s 068ms\n","\u001b[32m2022-04-21T04:30:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T04:30:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T04:31:00 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T04:31:00 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T04:31:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T04:31:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T04:31:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T04:31:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.6268, val/total_loss: 1.6268, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4986, val/hateful_memes/roc_auc: 0.6995, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 15s 800ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T04:32:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0347, train/hateful_memes/cross_entropy/avg: 0.1739, train/total_loss: 0.0347, train/total_loss/avg: 0.1739, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 716ms, time_since_start: 57m 05s 032ms, eta: 04h 07m 55s 885ms\n","\u001b[32m2022-04-21T04:33:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0332, train/hateful_memes/cross_entropy/avg: 0.1706, train/total_loss: 0.0332, train/total_loss/avg: 0.1706, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 539ms, time_since_start: 58m 25s 571ms, eta: 04h 02m 59s 681ms\n","\u001b[32m2022-04-21T04:35:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0326, train/hateful_memes/cross_entropy/avg: 0.1666, train/total_loss: 0.0326, train/total_loss/avg: 0.1666, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 194ms, time_since_start: 59m 45s 766ms, eta: 04h 35s 803ms\n","\u001b[32m2022-04-21T04:36:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0221, train/hateful_memes/cross_entropy/avg: 0.1633, train/total_loss: 0.0221, train/total_loss/avg: 0.1633, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 688ms, time_since_start: 01h 01m 06s 455ms, eta: 04h 42s 661ms\n","\u001b[32m2022-04-21T04:37:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0221, train/hateful_memes/cross_entropy/avg: 0.1612, train/total_loss: 0.0221, train/total_loss/avg: 0.1612, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 656ms, time_since_start: 01h 02m 27s 111ms, eta: 03h 59m 14s 767ms\n","\u001b[32m2022-04-21T04:39:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0221, train/hateful_memes/cross_entropy/avg: 0.1588, train/total_loss: 0.0221, train/total_loss/avg: 0.1588, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 358ms, time_since_start: 01h 03m 47s 469ms, eta: 03h 56m 59s 998ms\n","\u001b[32m2022-04-21T04:40:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0199, train/hateful_memes/cross_entropy/avg: 0.1555, train/total_loss: 0.0199, train/total_loss/avg: 0.1555, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 388ms, time_since_start: 01h 05m 07s 858ms, eta: 03h 55m 43s 658ms\n","\u001b[32m2022-04-21T04:41:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0158, train/hateful_memes/cross_entropy/avg: 0.1524, train/total_loss: 0.0158, train/total_loss/avg: 0.1524, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 764ms, time_since_start: 01h 06m 27s 623ms, eta: 03h 52m 32s 772ms\n","\u001b[32m2022-04-21T04:43:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0221, train/hateful_memes/cross_entropy/avg: 0.1499, train/total_loss: 0.0221, train/total_loss/avg: 0.1499, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 425ms, time_since_start: 01h 07m 48s 049ms, eta: 03h 53m 06s 644ms\n","\u001b[32m2022-04-21T04:44:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T04:44:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T04:44:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T04:44:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T04:44:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0309, train/hateful_memes/cross_entropy/avg: 0.1480, train/total_loss: 0.0309, train/total_loss/avg: 0.1480, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 168ms, time_since_start: 01h 09m 17s 218ms, eta: 04h 16m 56s 409ms\n","\u001b[32m2022-04-21T04:44:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T04:44:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T04:44:51 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T04:44:51 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T04:44:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T04:44:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T04:45:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T04:45:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.1631, val/total_loss: 1.1631, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.5074, val/hateful_memes/roc_auc: 0.7105, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 15s 005ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T04:46:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0221, train/hateful_memes/cross_entropy/avg: 0.1451, train/total_loss: 0.0221, train/total_loss/avg: 0.1451, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 365ms, time_since_start: 01h 10m 53s 590ms, eta: 03h 53m 04s 615ms\n","\u001b[32m2022-04-21T04:47:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1424, train/total_loss: 0.0088, train/total_loss/avg: 0.1424, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 434ms, time_since_start: 01h 12m 14s 025ms, eta: 03h 49m 02s 711ms\n","\u001b[32m2022-04-21T04:49:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0047, train/hateful_memes/cross_entropy/avg: 0.1397, train/total_loss: 0.0047, train/total_loss/avg: 0.1397, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 410ms, time_since_start: 01h 13m 34s 435ms, eta: 03h 47m 36s 835ms\n","\u001b[32m2022-04-21T04:50:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1379, train/total_loss: 0.0088, train/total_loss/avg: 0.1379, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 854ms, time_since_start: 01h 14m 54s 290ms, eta: 03h 44m 41s 202ms\n","\u001b[32m2022-04-21T04:51:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1354, train/total_loss: 0.0025, train/total_loss/avg: 0.1354, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 343ms, time_since_start: 01h 16m 14s 633ms, eta: 03h 44m 41s 968ms\n","\u001b[32m2022-04-21T04:53:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1330, train/total_loss: 0.0025, train/total_loss/avg: 0.1330, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 190ms, time_since_start: 01h 17m 34s 823ms, eta: 03h 42m 54s 759ms\n","\u001b[32m2022-04-21T04:54:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1307, train/total_loss: 0.0021, train/total_loss/avg: 0.1307, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 266ms, time_since_start: 01h 18m 55s 090ms, eta: 03h 41m 45s 849ms\n","\u001b[32m2022-04-21T04:55:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1290, train/total_loss: 0.0025, train/total_loss/avg: 0.1290, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 183ms, time_since_start: 01h 20m 15s 273ms, eta: 03h 40m 10s 517ms\n","\u001b[32m2022-04-21T04:57:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1269, train/total_loss: 0.0025, train/total_loss/avg: 0.1269, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 687ms, time_since_start: 01h 21m 34s 960ms, eta: 03h 37m 27s 756ms\n","\u001b[32m2022-04-21T04:58:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T04:58:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T04:58:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T04:58:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T04:58:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1248, train/total_loss: 0.0021, train/total_loss/avg: 0.1248, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 042ms, time_since_start: 01h 23m 04s 002ms, eta: 04h 01m 28s 954ms\n","\u001b[32m2022-04-21T04:58:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T04:58:32 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T04:58:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T04:58:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T04:58:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T04:58:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T04:58:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T04:58:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 1.8791, val/total_loss: 1.8791, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.5519, val/hateful_memes/roc_auc: 0.7064, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 16s 928ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T05:00:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1227, train/total_loss: 0.0021, train/total_loss/avg: 0.1227, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 350ms, time_since_start: 01h 24m 42s 283ms, eta: 03h 39m 14s 676ms\n","\u001b[32m2022-04-21T05:01:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1208, train/total_loss: 0.0021, train/total_loss/avg: 0.1208, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 143ms, time_since_start: 01h 26m 02s 427ms, eta: 03h 34m 37s 991ms\n","\u001b[32m2022-04-21T05:02:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1190, train/total_loss: 0.0021, train/total_loss/avg: 0.1190, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 325ms, time_since_start: 01h 27m 22s 753ms, eta: 03h 33m 45s 554ms\n","\u001b[32m2022-04-21T05:04:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1171, train/total_loss: 0.0021, train/total_loss/avg: 0.1171, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 932ms, time_since_start: 01h 28m 42s 685ms, eta: 03h 31m 21s 481ms\n","\u001b[32m2022-04-21T05:05:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1179, train/total_loss: 0.0021, train/total_loss/avg: 0.1179, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 260ms, time_since_start: 01h 30m 02s 945ms, eta: 03h 30m 51s 795ms\n","\u001b[32m2022-04-21T05:06:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1161, train/total_loss: 0.0021, train/total_loss/avg: 0.1161, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 268ms, time_since_start: 01h 31m 23s 214ms, eta: 03h 29m 31s 440ms\n","\u001b[32m2022-04-21T05:08:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1149, train/total_loss: 0.0023, train/total_loss/avg: 0.1149, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 876ms, time_since_start: 01h 32m 43s 091ms, eta: 03h 27m 08s 928ms\n","\u001b[32m2022-04-21T05:09:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1132, train/total_loss: 0.0021, train/total_loss/avg: 0.1132, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 075ms, time_since_start: 01h 34m 03s 166ms, eta: 03h 26m 18s 429ms\n","\u001b[32m2022-04-21T05:10:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1118, train/total_loss: 0.0021, train/total_loss/avg: 0.1118, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 097ms, time_since_start: 01h 35m 23s 264ms, eta: 03h 25m 406ms\n","\u001b[32m2022-04-21T05:12:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T05:12:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T05:12:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T05:12:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T05:12:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1103, train/total_loss: 0.0021, train/total_loss/avg: 0.1103, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 469ms, time_since_start: 01h 36m 51s 733ms, eta: 03h 44m 55s 954ms\n","\u001b[32m2022-04-21T05:12:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T05:12:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T05:12:25 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T05:12:25 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T05:12:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T05:12:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T05:12:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T05:12:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 1.6508, val/total_loss: 1.6508, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.5154, val/hateful_memes/roc_auc: 0.6988, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 17s 697ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T05:13:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1089, train/total_loss: 0.0023, train/total_loss/avg: 0.1089, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 232ms, time_since_start: 01h 38m 30s 664ms, eta: 03h 25m 09s 382ms\n","\u001b[32m2022-04-21T05:15:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1074, train/total_loss: 0.0023, train/total_loss/avg: 0.1074, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 029ms, time_since_start: 01h 39m 50s 694ms, eta: 03h 20m 45s 765ms\n","\u001b[32m2022-04-21T05:16:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1060, train/total_loss: 0.0023, train/total_loss/avg: 0.1060, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 213ms, time_since_start: 01h 41m 10s 908ms, eta: 03h 19m 51s 804ms\n","\u001b[32m2022-04-21T05:17:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1046, train/total_loss: 0.0023, train/total_loss/avg: 0.1046, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 260ms, time_since_start: 01h 42m 31s 168ms, eta: 03h 18m 37s 178ms\n","\u001b[32m2022-04-21T05:19:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1050, train/total_loss: 0.0028, train/total_loss/avg: 0.1050, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 844ms, time_since_start: 01h 43m 51s 012ms, eta: 03h 16m 14s 296ms\n","\u001b[32m2022-04-21T05:20:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1038, train/total_loss: 0.0033, train/total_loss/avg: 0.1038, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 284ms, time_since_start: 01h 45m 11s 297ms, eta: 03h 15m 57s 530ms\n","\u001b[32m2022-04-21T05:22:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0038, train/hateful_memes/cross_entropy/avg: 0.1027, train/total_loss: 0.0038, train/total_loss/avg: 0.1027, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 305ms, time_since_start: 01h 46m 31s 603ms, eta: 03h 14m 38s 985ms\n","\u001b[32m2022-04-21T05:23:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0038, train/hateful_memes/cross_entropy/avg: 0.1015, train/total_loss: 0.0038, train/total_loss/avg: 0.1015, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 041ms, time_since_start: 01h 47m 51s 645ms, eta: 03h 12m 39s 183ms\n","\u001b[32m2022-04-21T05:24:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.1024, train/total_loss: 0.0041, train/total_loss/avg: 0.1024, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 144ms, time_since_start: 01h 49m 11s 789ms, eta: 03h 11m 32s 435ms\n","\u001b[32m2022-04-21T05:26:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T05:26:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T05:26:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T05:26:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T05:26:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.1012, train/total_loss: 0.0041, train/total_loss/avg: 0.1012, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 118ms, time_since_start: 01h 50m 41s 908ms, eta: 03h 33m 51s 108ms\n","\u001b[32m2022-04-21T05:26:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T05:26:10 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T05:26:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T05:26:15 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T05:26:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.0673, val/total_loss: 2.0673, val/hateful_memes/accuracy: 0.7222, val/hateful_memes/binary_f1: 0.5455, val/hateful_memes/roc_auc: 0.7082, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 04s 856ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T05:27:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.0999, train/total_loss: 0.0041, train/total_loss/avg: 0.0999, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 1.23, time: 01m 21s 312ms, time_since_start: 01h 52m 08s 079ms, eta: 03h 11m 34s 518ms\n","\u001b[32m2022-04-21T05:28:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0055, train/hateful_memes/cross_entropy/avg: 0.0999, train/total_loss: 0.0055, train/total_loss/avg: 0.0999, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 438ms, time_since_start: 01h 53m 28s 517ms, eta: 03h 08m 09s 228ms\n","\u001b[32m2022-04-21T05:30:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0055, train/hateful_memes/cross_entropy/avg: 0.0987, train/total_loss: 0.0055, train/total_loss/avg: 0.0987, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 193ms, time_since_start: 01h 54m 48s 711ms, eta: 03h 06m 13s 291ms\n","\u001b[32m2022-04-21T05:31:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0055, train/hateful_memes/cross_entropy/avg: 0.0976, train/total_loss: 0.0055, train/total_loss/avg: 0.0976, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 286ms, time_since_start: 01h 56m 08s 997ms, eta: 03h 05m 04s 547ms\n","\u001b[32m2022-04-21T05:32:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0964, train/total_loss: 0.0033, train/total_loss/avg: 0.0964, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 287ms, time_since_start: 01h 57m 29s 284ms, eta: 03h 03m 43s 043ms\n","\u001b[32m2022-04-21T05:34:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.0953, train/total_loss: 0.0028, train/total_loss/avg: 0.0953, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 1.27, time: 01m 19s 704ms, time_since_start: 01h 58m 48s 989ms, eta: 03h 01m 01s 969ms\n","\u001b[32m2022-04-21T05:35:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.0942, train/total_loss: 0.0017, train/total_loss/avg: 0.0942, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 424ms, time_since_start: 02h 09s 414ms, eta: 03h 01m 18s 333ms\n","\u001b[32m2022-04-21T05:36:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.0932, train/total_loss: 0.0028, train/total_loss/avg: 0.0932, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 1.27, time: 01m 19s 929ms, time_since_start: 02h 01m 29s 343ms, eta: 02h 58m 50s 049ms\n","\u001b[32m2022-04-21T05:38:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.0922, train/total_loss: 0.0017, train/total_loss/avg: 0.0922, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 449ms, time_since_start: 02h 02m 49s 792ms, eta: 02h 58m 38s 033ms\n","\u001b[32m2022-04-21T05:39:38 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T05:39:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T05:39:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T05:39:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T05:39:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0911, train/total_loss: 0.0009, train/total_loss/avg: 0.0911, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 1.08, time: 01m 33s 374ms, time_since_start: 02h 04m 23s 167ms, eta: 03h 25m 45s 053ms\n","\u001b[32m2022-04-21T05:39:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T05:39:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T05:39:56 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T05:39:56 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T05:39:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.1453, val/total_loss: 2.1453, val/hateful_memes/accuracy: 0.7037, val/hateful_memes/binary_f1: 0.5031, val/hateful_memes/roc_auc: 0.7125, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 22000, val_time: 05s 198ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T05:41:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0902, train/total_loss: 0.0009, train/total_loss/avg: 0.0902, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 998ms, time_since_start: 02h 05m 49s 366ms, eta: 02h 57m 06s 462ms\n","\u001b[32m2022-04-21T05:42:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.0892, train/total_loss: 0.0017, train/total_loss/avg: 0.0892, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 399ms, time_since_start: 02h 07m 09s 765ms, eta: 02h 54m 26s 078ms\n","\u001b[32m2022-04-21T05:43:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0883, train/total_loss: 0.0009, train/total_loss/avg: 0.0883, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 406ms, time_since_start: 02h 08m 30s 172ms, eta: 02h 53m 05s 211ms\n","\u001b[32m2022-04-21T05:45:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0873, train/total_loss: 0.0009, train/total_loss/avg: 0.0873, max mem: 9224.0, experiment: run, epoch: 36, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 285ms, time_since_start: 02h 09m 50s 457ms, eta: 02h 51m 27s 941ms\n","\u001b[32m2022-04-21T05:46:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0865, train/total_loss: 0.0009, train/total_loss/avg: 0.0865, max mem: 9224.0, experiment: run, epoch: 36, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 560ms, time_since_start: 02h 11m 11s 018ms, eta: 02h 50m 41s 316ms\n","\u001b[32m2022-04-21T05:47:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0856, train/total_loss: 0.0009, train/total_loss/avg: 0.0856, max mem: 9224.0, experiment: run, epoch: 37, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 1.27, time: 01m 19s 892ms, time_since_start: 02h 12m 30s 910ms, eta: 02h 47m 55s 041ms\n","\u001b[32m2022-04-21T05:49:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0847, train/total_loss: 0.0009, train/total_loss/avg: 0.0847, max mem: 9224.0, experiment: run, epoch: 37, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 570ms, time_since_start: 02h 13m 51s 481ms, eta: 02h 47m 58s 621ms\n","\u001b[32m2022-04-21T05:50:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0839, train/total_loss: 0.0009, train/total_loss/avg: 0.0839, max mem: 9224.0, experiment: run, epoch: 37, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 478ms, time_since_start: 02h 15m 11s 959ms, eta: 02h 46m 25s 241ms\n","\u001b[32m2022-04-21T05:52:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0830, train/total_loss: 0.0007, train/total_loss/avg: 0.0830, max mem: 9224.0, experiment: run, epoch: 38, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 187ms, time_since_start: 02h 16m 32s 146ms, eta: 02h 44m 27s 610ms\n","\u001b[32m2022-04-21T05:53:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T05:53:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T05:53:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T05:53:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T05:53:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0822, train/total_loss: 0.0007, train/total_loss/avg: 0.0822, max mem: 9224.0, experiment: run, epoch: 38, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 793ms, time_since_start: 02h 18m 02s 940ms, eta: 03h 04m 40s 458ms\n","\u001b[32m2022-04-21T05:53:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T05:53:32 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T05:53:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T05:53:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T05:53:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.1809, val/total_loss: 2.1809, val/hateful_memes/accuracy: 0.7130, val/hateful_memes/binary_f1: 0.5481, val/hateful_memes/roc_auc: 0.7164, num_updates: 10000, epoch: 38, iterations: 10000, max_updates: 22000, val_time: 05s 803ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T05:54:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0814, train/total_loss: 0.0007, train/total_loss/avg: 0.0814, max mem: 9224.0, experiment: run, epoch: 38, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 1.23, time: 01m 21s 377ms, time_since_start: 02h 19m 30s 122ms, eta: 02h 44m 08s 506ms\n","\u001b[32m2022-04-21T05:56:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0806, train/total_loss: 0.0007, train/total_loss/avg: 0.0806, max mem: 9224.0, experiment: run, epoch: 39, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 1.27, time: 01m 19s 974ms, time_since_start: 02h 20m 50s 096ms, eta: 02h 39m 57s 366ms\n","\u001b[32m2022-04-21T05:57:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0798, train/total_loss: 0.0003, train/total_loss/avg: 0.0798, max mem: 9224.0, experiment: run, epoch: 39, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 454ms, time_since_start: 02h 22m 10s 551ms, eta: 02h 39m 33s 216ms\n","\u001b[32m2022-04-21T05:58:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0790, train/total_loss: 0.0003, train/total_loss/avg: 0.0790, max mem: 9224.0, experiment: run, epoch: 40, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 324ms, time_since_start: 02h 23m 30s 875ms, eta: 02h 37m 56s 020ms\n","\u001b[32m2022-04-21T06:00:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0783, train/total_loss: 0.0007, train/total_loss/avg: 0.0783, max mem: 9224.0, experiment: run, epoch: 40, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 488ms, time_since_start: 02h 24m 51s 363ms, eta: 02h 36m 53s 486ms\n","\u001b[32m2022-04-21T06:01:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0776, train/total_loss: 0.0007, train/total_loss/avg: 0.0776, max mem: 9224.0, experiment: run, epoch: 40, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 415ms, time_since_start: 02h 26m 11s 778ms, eta: 02h 35m 23s 163ms\n","\u001b[32m2022-04-21T06:03:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0769, train/total_loss: 0.0007, train/total_loss/avg: 0.0769, max mem: 9224.0, experiment: run, epoch: 41, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 1.27, time: 01m 19s 867ms, time_since_start: 02h 27m 31s 645ms, eta: 02h 32m 58s 410ms\n","\u001b[32m2022-04-21T06:04:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0762, train/total_loss: 0.0007, train/total_loss/avg: 0.0762, max mem: 9224.0, experiment: run, epoch: 41, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 515ms, time_since_start: 02h 28m 52s 161ms, eta: 02h 32m 51s 029ms\n","\u001b[32m2022-04-21T06:05:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0755, train/total_loss: 0.0007, train/total_loss/avg: 0.0755, max mem: 9224.0, experiment: run, epoch: 41, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 414ms, time_since_start: 02h 30m 12s 575ms, eta: 02h 31m 17s 748ms\n","\u001b[32m2022-04-21T06:07:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T06:07:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T06:07:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T06:07:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T06:07:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0748, train/total_loss: 0.0007, train/total_loss/avg: 0.0748, max mem: 9224.0, experiment: run, epoch: 42, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 177ms, time_since_start: 02h 31m 44s 752ms, eta: 02h 51m 51s 847ms\n","\u001b[32m2022-04-21T06:07:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T06:07:13 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T06:07:18 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T06:07:18 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T06:07:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.3842, val/total_loss: 2.3842, val/hateful_memes/accuracy: 0.7093, val/hateful_memes/binary_f1: 0.5078, val/hateful_memes/roc_auc: 0.7072, num_updates: 11000, epoch: 42, iterations: 11000, max_updates: 22000, val_time: 05s 468ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T06:08:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0741, train/total_loss: 0.0007, train/total_loss/avg: 0.0741, max mem: 9224.0, experiment: run, epoch: 42, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 1.23, time: 01m 21s 433ms, time_since_start: 02h 33m 11s 656ms, eta: 02h 30m 27s 122ms\n","\u001b[32m2022-04-21T06:10:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0735, train/total_loss: 0.0007, train/total_loss/avg: 0.0735, max mem: 9224.0, experiment: run, epoch: 43, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 1.27, time: 01m 19s 812ms, time_since_start: 02h 34m 31s 469ms, eta: 02h 26m 06s 307ms\n","\u001b[32m2022-04-21T06:11:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0728, train/total_loss: 0.0007, train/total_loss/avg: 0.0728, max mem: 9224.0, experiment: run, epoch: 43, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 436ms, time_since_start: 02h 35m 51s 905ms, eta: 02h 25m 53s 045ms\n","\u001b[32m2022-04-21T06:12:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0722, train/total_loss: 0.0007, train/total_loss/avg: 0.0722, max mem: 9224.0, experiment: run, epoch: 43, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 600ms, time_since_start: 02h 37m 12s 506ms, eta: 02h 24m 48s 896ms\n","\u001b[32m2022-04-21T06:14:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0716, train/total_loss: 0.0007, train/total_loss/avg: 0.0716, max mem: 9224.0, experiment: run, epoch: 44, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 126ms, time_since_start: 02h 38m 32s 632ms, eta: 02h 22m 36s 271ms\n","\u001b[32m2022-04-21T06:15:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0710, train/total_loss: 0.0007, train/total_loss/avg: 0.0710, max mem: 9224.0, experiment: run, epoch: 44, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 457ms, time_since_start: 02h 39m 53s 090ms, eta: 02h 21m 49s 866ms\n","\u001b[32m2022-04-21T06:16:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0704, train/total_loss: 0.0003, train/total_loss/avg: 0.0704, max mem: 9224.0, experiment: run, epoch: 44, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 244ms, time_since_start: 02h 41m 13s 334ms, eta: 02h 20m 05s 649ms\n","\u001b[32m2022-04-21T06:18:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0698, train/total_loss: 0.0003, train/total_loss/avg: 0.0698, max mem: 9224.0, experiment: run, epoch: 45, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 019ms, time_since_start: 02h 42m 33s 354ms, eta: 02h 18m 20s 760ms\n","\u001b[32m2022-04-21T06:19:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0694, train/total_loss: 0.0003, train/total_loss/avg: 0.0694, max mem: 9224.0, experiment: run, epoch: 45, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 436ms, time_since_start: 02h 43m 53s 790ms, eta: 02h 17m 42s 188ms\n","\u001b[32m2022-04-21T06:20:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T06:20:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T06:20:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T06:20:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T06:20:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0688, train/total_loss: 0.0002, train/total_loss/avg: 0.0688, max mem: 9224.0, experiment: run, epoch: 46, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 758ms, time_since_start: 02h 45m 24s 549ms, eta: 02h 33m 50s 120ms\n","\u001b[32m2022-04-21T06:20:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T06:20:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T06:20:58 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T06:20:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T06:20:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.0212, val/total_loss: 2.0212, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.5319, val/hateful_memes/roc_auc: 0.7136, num_updates: 12000, epoch: 46, iterations: 12000, max_updates: 22000, val_time: 05s 211ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T06:22:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.0687, train/total_loss: 0.0008, train/total_loss/avg: 0.0687, max mem: 9224.0, experiment: run, epoch: 46, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 1.23, time: 01m 21s 370ms, time_since_start: 02h 46m 51s 133ms, eta: 02h 16m 32s 623ms\n","\u001b[32m2022-04-21T06:23:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.0681, train/total_loss: 0.0008, train/total_loss/avg: 0.0681, max mem: 9224.0, experiment: run, epoch: 46, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 458ms, time_since_start: 02h 48m 11s 591ms, eta: 02h 13m 38s 939ms\n","\u001b[32m2022-04-21T06:25:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.0676, train/total_loss: 0.0008, train/total_loss/avg: 0.0676, max mem: 9224.0, experiment: run, epoch: 47, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 998ms, time_since_start: 02h 49m 31s 590ms, eta: 02h 11m 31s 812ms\n","\u001b[32m2022-04-21T06:26:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.0670, train/total_loss: 0.0008, train/total_loss/avg: 0.0670, max mem: 9224.0, experiment: run, epoch: 47, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 319ms, time_since_start: 02h 50m 51s 909ms, eta: 02h 10m 41s 751ms\n","\u001b[32m2022-04-21T06:27:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0665, train/total_loss: 0.0002, train/total_loss/avg: 0.0665, max mem: 9224.0, experiment: run, epoch: 47, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 147ms, time_since_start: 02h 52m 12s 057ms, eta: 02h 09m 03s 468ms\n","\u001b[32m2022-04-21T06:29:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0660, train/total_loss: 0.0002, train/total_loss/avg: 0.0660, max mem: 9224.0, experiment: run, epoch: 48, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 255ms, time_since_start: 02h 53m 32s 313ms, eta: 02h 07m 52s 307ms\n","\u001b[32m2022-04-21T06:30:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0654, train/total_loss: 0.0002, train/total_loss/avg: 0.0654, max mem: 9224.0, experiment: run, epoch: 48, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 233ms, time_since_start: 02h 54m 52s 546ms, eta: 02h 06m 28s 555ms\n","\u001b[32m2022-04-21T06:31:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0649, train/total_loss: 0.0002, train/total_loss/avg: 0.0649, max mem: 9224.0, experiment: run, epoch: 49, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 601ms, time_since_start: 02h 56m 12s 148ms, eta: 02h 04m 07s 846ms\n","\u001b[32m2022-04-21T06:33:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0644, train/total_loss: 0.0002, train/total_loss/avg: 0.0644, max mem: 9224.0, experiment: run, epoch: 49, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 157ms, time_since_start: 02h 57m 32s 305ms, eta: 02h 03m 38s 301ms\n","\u001b[32m2022-04-21T06:34:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T06:34:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T06:34:24 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T06:34:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T06:34:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0639, train/total_loss: 0.0002, train/total_loss/avg: 0.0639, max mem: 9224.0, experiment: run, epoch: 49, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 204ms, time_since_start: 02h 59m 01s 509ms, eta: 02h 16m 04s 874ms\n","\u001b[32m2022-04-21T06:34:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T06:34:30 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T06:34:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T06:34:34 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T06:34:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.6679, val/total_loss: 2.6679, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.4728, val/hateful_memes/roc_auc: 0.7144, num_updates: 13000, epoch: 49, iterations: 13000, max_updates: 22000, val_time: 04s 953ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T06:35:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0634, train/total_loss: 0.0002, train/total_loss/avg: 0.0634, max mem: 9224.0, experiment: run, epoch: 50, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 952ms, time_since_start: 03h 27s 417ms, eta: 02h 02m 07s 215ms\n","\u001b[32m2022-04-21T06:37:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0630, train/total_loss: 0.0001, train/total_loss/avg: 0.0630, max mem: 9224.0, experiment: run, epoch: 50, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 174ms, time_since_start: 03h 01m 47s 592ms, eta: 01h 59m 35s 319ms\n","\u001b[32m2022-04-21T06:38:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0627, train/total_loss: 0.0001, train/total_loss/avg: 0.0627, max mem: 9224.0, experiment: run, epoch: 50, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 469ms, time_since_start: 03h 03m 07s 061ms, eta: 01h 57m 11s 351ms\n","\u001b[32m2022-04-21T06:39:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0622, train/total_loss: 0.0001, train/total_loss/avg: 0.0622, max mem: 9224.0, experiment: run, epoch: 51, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 364ms, time_since_start: 03h 04m 27s 425ms, eta: 01h 57m 08s 817ms\n","\u001b[32m2022-04-21T06:41:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0617, train/total_loss: 0.0001, train/total_loss/avg: 0.0617, max mem: 9224.0, experiment: run, epoch: 51, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 201ms, time_since_start: 03h 05m 47s 627ms, eta: 01h 55m 33s 057ms\n","\u001b[32m2022-04-21T06:42:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0614, train/total_loss: 0.0001, train/total_loss/avg: 0.0614, max mem: 9224.0, experiment: run, epoch: 52, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 006ms, time_since_start: 03h 07m 07s 634ms, eta: 01h 53m 54s 805ms\n","\u001b[32m2022-04-21T06:43:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0610, train/total_loss: 0.0001, train/total_loss/avg: 0.0610, max mem: 9224.0, experiment: run, epoch: 52, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 203ms, time_since_start: 03h 08m 27s 838ms, eta: 01h 52m 50s 088ms\n","\u001b[32m2022-04-21T06:45:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0605, train/total_loss: 0.0001, train/total_loss/avg: 0.0605, max mem: 9224.0, experiment: run, epoch: 52, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 201ms, time_since_start: 03h 09m 48s 039ms, eta: 01h 51m 28s 303ms\n","\u001b[32m2022-04-21T06:46:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0601, train/total_loss: 0.0001, train/total_loss/avg: 0.0601, max mem: 9224.0, experiment: run, epoch: 53, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 604ms, time_since_start: 03h 11m 07s 643ms, eta: 01h 49m 17s 563ms\n","\u001b[32m2022-04-21T06:47:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T06:47:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T06:48:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T06:48:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T06:48:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0597, train/total_loss: 0.0001, train/total_loss/avg: 0.0597, max mem: 9224.0, experiment: run, epoch: 53, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 520ms, time_since_start: 03h 12m 39s 164ms, eta: 02h 04m 06s 102ms\n","\u001b[32m2022-04-21T06:48:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T06:48:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T06:48:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T06:48:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T06:48:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.4563, val/total_loss: 2.4563, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.4566, val/hateful_memes/roc_auc: 0.7008, num_updates: 14000, epoch: 53, iterations: 14000, max_updates: 22000, val_time: 05s 137ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T06:49:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0592, train/total_loss: 0.0001, train/total_loss/avg: 0.0592, max mem: 9224.0, experiment: run, epoch: 54, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 1.23, time: 01m 21s 055ms, time_since_start: 03h 14m 05s 359ms, eta: 01h 48m 32s 231ms\n","\u001b[32m2022-04-21T06:50:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0588, train/total_loss: 0.0001, train/total_loss/avg: 0.0588, max mem: 9224.0, experiment: run, epoch: 54, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 285ms, time_since_start: 03h 15m 25s 644ms, eta: 01h 46m 08s 730ms\n","\u001b[32m2022-04-21T06:52:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0584, train/total_loss: 0.0001, train/total_loss/avg: 0.0584, max mem: 9224.0, experiment: run, epoch: 54, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 265ms, time_since_start: 03h 16m 45s 910ms, eta: 01h 44m 45s 497ms\n","\u001b[32m2022-04-21T06:53:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0580, train/total_loss: 0.0001, train/total_loss/avg: 0.0580, max mem: 9224.0, experiment: run, epoch: 55, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 647ms, time_since_start: 03h 18m 05s 557ms, eta: 01h 42m 36s 121ms\n","\u001b[32m2022-04-21T06:54:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0576, train/total_loss: 0.0001, train/total_loss/avg: 0.0576, max mem: 9224.0, experiment: run, epoch: 55, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 353ms, time_since_start: 03h 19m 25s 911ms, eta: 01h 42m 08s 996ms\n","\u001b[32m2022-04-21T06:56:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0572, train/total_loss: 0.0001, train/total_loss/avg: 0.0572, max mem: 9224.0, experiment: run, epoch: 55, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 269ms, time_since_start: 03h 20m 46s 181ms, eta: 01h 40m 40s 940ms\n","\u001b[32m2022-04-21T06:57:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0568, train/total_loss: 0.0001, train/total_loss/avg: 0.0568, max mem: 9224.0, experiment: run, epoch: 56, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 116ms, time_since_start: 03h 22m 06s 298ms, eta: 01h 39m 07s 940ms\n","\u001b[32m2022-04-21T06:58:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0564, train/total_loss: 0.0001, train/total_loss/avg: 0.0564, max mem: 9224.0, experiment: run, epoch: 56, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 289ms, time_since_start: 03h 23m 26s 587ms, eta: 01h 37m 59s 125ms\n","\u001b[32m2022-04-21T07:00:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0561, train/total_loss: 0.0001, train/total_loss/avg: 0.0561, max mem: 9224.0, experiment: run, epoch: 57, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 721ms, time_since_start: 03h 24m 46s 309ms, eta: 01h 35m 56s 453ms\n","\u001b[32m2022-04-21T07:01:35 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T07:01:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T07:01:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T07:01:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T07:01:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0557, train/total_loss: 0.0001, train/total_loss/avg: 0.0557, max mem: 9224.0, experiment: run, epoch: 57, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 055ms, time_since_start: 03h 26m 15s 364ms, eta: 01h 45m 39s 850ms\n","\u001b[32m2022-04-21T07:01:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T07:01:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T07:01:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T07:01:48 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T07:01:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.6011, val/total_loss: 2.6011, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.5169, val/hateful_memes/roc_auc: 0.7065, num_updates: 15000, epoch: 57, iterations: 15000, max_updates: 22000, val_time: 04s 832ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T07:03:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0553, train/total_loss: 0.0001, train/total_loss/avg: 0.0553, max mem: 9224.0, experiment: run, epoch: 57, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 1.23, time: 01m 21s 255ms, time_since_start: 03h 27m 41s 453ms, eta: 01h 35m 01s 920ms\n","\u001b[32m2022-04-21T07:04:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0550, train/total_loss: 0.0001, train/total_loss/avg: 0.0550, max mem: 9224.0, experiment: run, epoch: 58, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 047ms, time_since_start: 03h 29m 01s 500ms, eta: 01h 32m 15s 742ms\n","\u001b[32m2022-04-21T07:05:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0546, train/total_loss: 0.0001, train/total_loss/avg: 0.0546, max mem: 9224.0, experiment: run, epoch: 58, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 260ms, time_since_start: 03h 30m 21s 761ms, eta: 01h 31m 08s 884ms\n","\u001b[32m2022-04-21T07:07:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0542, train/total_loss: 0.0001, train/total_loss/avg: 0.0542, max mem: 9224.0, experiment: run, epoch: 58, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 251ms, time_since_start: 03h 31m 42s 012ms, eta: 01h 29m 46s 619ms\n","\u001b[32m2022-04-21T07:08:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0539, train/total_loss: 0.0001, train/total_loss/avg: 0.0539, max mem: 9224.0, experiment: run, epoch: 59, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 771ms, time_since_start: 03h 33m 01s 784ms, eta: 01h 27m 53s 325ms\n","\u001b[32m2022-04-21T07:09:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0535, train/total_loss: 0.0001, train/total_loss/avg: 0.0535, max mem: 9224.0, experiment: run, epoch: 59, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 247ms, time_since_start: 03h 34m 22s 032ms, eta: 01h 27m 03s 142ms\n","\u001b[32m2022-04-21T07:11:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0532, train/total_loss: 0.0001, train/total_loss/avg: 0.0532, max mem: 9224.0, experiment: run, epoch: 60, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 112ms, time_since_start: 03h 35m 42s 144ms, eta: 01h 25m 32s 889ms\n","\u001b[32m2022-04-21T07:12:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0529, train/total_loss: 0.0000, train/total_loss/avg: 0.0529, max mem: 9224.0, experiment: run, epoch: 60, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 114ms, time_since_start: 03h 37m 02s 259ms, eta: 01h 24m 11s 539ms\n","\u001b[32m2022-04-21T07:13:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0525, train/total_loss: 0.0000, train/total_loss/avg: 0.0525, max mem: 9224.0, experiment: run, epoch: 60, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 108ms, time_since_start: 03h 38m 22s 367ms, eta: 01h 22m 49s 697ms\n","\u001b[32m2022-04-21T07:15:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T07:15:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T07:15:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T07:15:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T07:15:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0522, train/total_loss: 0.0000, train/total_loss/avg: 0.0522, max mem: 9224.0, experiment: run, epoch: 61, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 974ms, time_since_start: 03h 39m 51s 342ms, eta: 01h 30m 29s 240ms\n","\u001b[32m2022-04-21T07:15:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T07:15:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T07:15:24 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T07:15:24 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T07:15:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 2.9235, val/total_loss: 2.9235, val/hateful_memes/accuracy: 0.7056, val/hateful_memes/binary_f1: 0.5047, val/hateful_memes/roc_auc: 0.7189, num_updates: 16000, epoch: 61, iterations: 16000, max_updates: 22000, val_time: 05s 127ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T07:16:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0519, train/total_loss: 0.0000, train/total_loss/avg: 0.0519, max mem: 9224.0, experiment: run, epoch: 61, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 1.23, time: 01m 21s 388ms, time_since_start: 03h 41m 17s 860ms, eta: 01h 21m 23s 576ms\n","\u001b[32m2022-04-21T07:18:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0516, train/total_loss: 0.0000, train/total_loss/avg: 0.0516, max mem: 9224.0, experiment: run, epoch: 61, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 451ms, time_since_start: 03h 42m 38s 311ms, eta: 01h 19m 05s 498ms\n","\u001b[32m2022-04-21T07:19:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0512, train/total_loss: 0.0000, train/total_loss/avg: 0.0512, max mem: 9224.0, experiment: run, epoch: 62, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 139ms, time_since_start: 03h 43m 58s 450ms, eta: 01h 17m 25s 582ms\n","\u001b[32m2022-04-21T07:20:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0509, train/total_loss: 0.0000, train/total_loss/avg: 0.0509, max mem: 9224.0, experiment: run, epoch: 62, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 378ms, time_since_start: 03h 45m 18s 829ms, eta: 01h 16m 17s 713ms\n","\u001b[32m2022-04-21T07:22:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0506, train/total_loss: 0.0000, train/total_loss/avg: 0.0506, max mem: 9224.0, experiment: run, epoch: 63, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 804ms, time_since_start: 03h 46m 38s 633ms, eta: 01h 14m 23s 879ms\n","\u001b[32m2022-04-21T07:23:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0503, train/total_loss: 0.0000, train/total_loss/avg: 0.0503, max mem: 9224.0, experiment: run, epoch: 63, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 223ms, time_since_start: 03h 47m 58s 857ms, eta: 01h 13m 25s 718ms\n","\u001b[32m2022-04-21T07:24:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0500, train/total_loss: 0.0000, train/total_loss/avg: 0.0500, max mem: 9224.0, experiment: run, epoch: 63, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 317ms, time_since_start: 03h 49m 19s 175ms, eta: 01h 12m 09s 193ms\n","\u001b[32m2022-04-21T07:26:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0497, train/total_loss: 0.0000, train/total_loss/avg: 0.0497, max mem: 9224.0, experiment: run, epoch: 64, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 975ms, time_since_start: 03h 50m 39s 151ms, eta: 01h 10m 29s 448ms\n","\u001b[32m2022-04-21T07:27:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0494, train/total_loss: 0.0000, train/total_loss/avg: 0.0494, max mem: 9224.0, experiment: run, epoch: 64, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 290ms, time_since_start: 03h 51m 59s 441ms, eta: 01h 09m 24s 422ms\n","\u001b[32m2022-04-21T07:28:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T07:28:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T07:28:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T07:28:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T07:28:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0491, train/total_loss: 0.0000, train/total_loss/avg: 0.0491, max mem: 9224.0, experiment: run, epoch: 64, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 921ms, time_since_start: 03h 53m 30s 363ms, eta: 01h 17m 03s 362ms\n","\u001b[32m2022-04-21T07:28:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T07:28:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T07:29:03 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T07:29:03 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T07:29:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 2.6881, val/total_loss: 2.6881, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.4762, val/hateful_memes/roc_auc: 0.7031, num_updates: 17000, epoch: 64, iterations: 17000, max_updates: 22000, val_time: 04s 826ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T07:30:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0489, train/total_loss: 0.0000, train/total_loss/avg: 0.0489, max mem: 9224.0, experiment: run, epoch: 65, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 663ms, time_since_start: 03h 54m 55s 854ms, eta: 01h 06m 59s 700ms\n","\u001b[32m2022-04-21T07:31:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0486, train/total_loss: 0.0000, train/total_loss/avg: 0.0486, max mem: 9224.0, experiment: run, epoch: 65, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 260ms, time_since_start: 03h 56m 16s 114ms, eta: 01h 05m 17s 982ms\n","\u001b[32m2022-04-21T07:33:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0483, train/total_loss: 0.0000, train/total_loss/avg: 0.0483, max mem: 9224.0, experiment: run, epoch: 66, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 939ms, time_since_start: 03h 57m 36s 053ms, eta: 01h 03m 41s 028ms\n","\u001b[32m2022-04-21T07:34:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0480, train/total_loss: 0.0000, train/total_loss/avg: 0.0480, max mem: 9224.0, experiment: run, epoch: 66, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 131ms, time_since_start: 03h 58m 56s 185ms, eta: 01h 02m 28s 702ms\n","\u001b[32m2022-04-21T07:35:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0477, train/total_loss: 0.0000, train/total_loss/avg: 0.0477, max mem: 9224.0, experiment: run, epoch: 66, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 202ms, time_since_start: 04h 16s 387ms, eta: 01h 01m 10s 451ms\n","\u001b[32m2022-04-21T07:37:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0475, train/total_loss: 0.0000, train/total_loss/avg: 0.0475, max mem: 9224.0, experiment: run, epoch: 67, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 602ms, time_since_start: 04h 01m 35s 989ms, eta: 59m 22s 040ms\n","\u001b[32m2022-04-21T07:38:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0472, train/total_loss: 0.0000, train/total_loss/avg: 0.0472, max mem: 9224.0, experiment: run, epoch: 67, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 088ms, time_since_start: 04h 02m 56s 078ms, eta: 58m 22s 351ms\n","\u001b[32m2022-04-21T07:39:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0469, train/total_loss: 0.0000, train/total_loss/avg: 0.0469, max mem: 9224.0, experiment: run, epoch: 67, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 035ms, time_since_start: 04h 04m 16s 113ms, eta: 56m 58s 654ms\n","\u001b[32m2022-04-21T07:41:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0467, train/total_loss: 0.0000, train/total_loss/avg: 0.0467, max mem: 9224.0, experiment: run, epoch: 68, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 907ms, time_since_start: 04h 05m 36s 021ms, eta: 55m 31s 900ms\n","\u001b[32m2022-04-21T07:42:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T07:42:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T07:42:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T07:42:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T07:42:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0464, train/total_loss: 0.0000, train/total_loss/avg: 0.0464, max mem: 9224.0, experiment: run, epoch: 68, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 501ms, time_since_start: 04h 07m 07s 523ms, eta: 01h 02m 02s 300ms\n","\u001b[32m2022-04-21T07:42:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T07:42:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T07:42:41 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T07:42:41 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T07:42:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 2.9410, val/total_loss: 2.9410, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4910, val/hateful_memes/roc_auc: 0.7001, num_updates: 18000, epoch: 68, iterations: 18000, max_updates: 22000, val_time: 05s 279ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T07:44:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0462, train/total_loss: 0.0000, train/total_loss/avg: 0.0462, max mem: 9224.0, experiment: run, epoch: 69, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 710ms, time_since_start: 04h 08m 33s 515ms, eta: 53m 21s 240ms\n","\u001b[32m2022-04-21T07:45:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0468, train/total_loss: 0.0000, train/total_loss/avg: 0.0468, max mem: 9224.0, experiment: run, epoch: 69, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 176ms, time_since_start: 04h 09m 53s 692ms, eta: 51m 38s 515ms\n","\u001b[32m2022-04-21T07:46:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0465, train/total_loss: 0.0000, train/total_loss/avg: 0.0465, max mem: 9224.0, experiment: run, epoch: 69, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 315ms, time_since_start: 04h 11m 14s 008ms, eta: 50m 22s 183ms\n","\u001b[32m2022-04-21T07:48:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0463, train/total_loss: 0.0000, train/total_loss/avg: 0.0463, max mem: 9224.0, experiment: run, epoch: 70, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 863ms, time_since_start: 04h 12m 33s 871ms, eta: 48m 43s 965ms\n","\u001b[32m2022-04-21T07:49:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0460, train/total_loss: 0.0000, train/total_loss/avg: 0.0460, max mem: 9224.0, experiment: run, epoch: 70, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 264ms, time_since_start: 04h 13m 54s 136ms, eta: 47m 37s 025ms\n","\u001b[32m2022-04-21T07:50:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0458, train/total_loss: 0.0000, train/total_loss/avg: 0.0458, max mem: 9224.0, experiment: run, epoch: 70, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 164ms, time_since_start: 04h 15m 14s 301ms, eta: 46m 11s 943ms\n","\u001b[32m2022-04-21T07:52:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0455, train/total_loss: 0.0000, train/total_loss/avg: 0.0455, max mem: 9224.0, experiment: run, epoch: 71, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 737ms, time_since_start: 04h 16m 34s 038ms, eta: 44m 36s 063ms\n","\u001b[32m2022-04-21T07:53:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0453, train/total_loss: 0.0000, train/total_loss/avg: 0.0453, max mem: 9224.0, experiment: run, epoch: 71, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 185ms, time_since_start: 04h 17m 54s 224ms, eta: 43m 29s 566ms\n","\u001b[32m2022-04-21T07:54:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0451, train/total_loss: 0.0000, train/total_loss/avg: 0.0451, max mem: 9224.0, experiment: run, epoch: 72, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 749ms, time_since_start: 04h 19m 13s 974ms, eta: 41m 54s 276ms\n","\u001b[32m2022-04-21T07:56:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T07:56:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T07:56:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T07:56:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T07:56:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0448, train/total_loss: 0.0000, train/total_loss/avg: 0.0448, max mem: 9224.0, experiment: run, epoch: 72, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 849ms, time_since_start: 04h 20m 42s 823ms, eta: 45m 10s 794ms\n","\u001b[32m2022-04-21T07:56:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T07:56:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T07:56:16 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T07:56:16 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T07:56:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.0177, val/total_loss: 3.0177, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.5089, val/hateful_memes/roc_auc: 0.7026, num_updates: 19000, epoch: 72, iterations: 19000, max_updates: 22000, val_time: 05s 079ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T07:57:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0446, train/total_loss: 0.0000, train/total_loss/avg: 0.0446, max mem: 9224.0, experiment: run, epoch: 72, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 1.23, time: 01m 21s 289ms, time_since_start: 04h 22m 09s 194ms, eta: 39m 57s 479ms\n","\u001b[32m2022-04-21T07:58:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0443, train/total_loss: 0.0000, train/total_loss/avg: 0.0443, max mem: 9224.0, experiment: run, epoch: 73, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 018ms, time_since_start: 04h 23m 29s 213ms, eta: 37m 58s 620ms\n","\u001b[32m2022-04-21T08:00:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0441, train/total_loss: 0.0000, train/total_loss/avg: 0.0441, max mem: 9224.0, experiment: run, epoch: 73, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 377ms, time_since_start: 04h 24m 49s 591ms, eta: 36m 47s 083ms\n","\u001b[32m2022-04-21T08:01:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0439, train/total_loss: 0.0000, train/total_loss/avg: 0.0439, max mem: 9224.0, experiment: run, epoch: 73, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 315ms, time_since_start: 04h 26m 09s 906ms, eta: 35m 23s 694ms\n","\u001b[32m2022-04-21T08:02:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0437, train/total_loss: 0.0000, train/total_loss/avg: 0.0437, max mem: 9224.0, experiment: run, epoch: 74, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 943ms, time_since_start: 04h 27m 29s 850ms, eta: 33m 52s 575ms\n","\u001b[32m2022-04-21T08:04:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0435, train/total_loss: 0.0000, train/total_loss/avg: 0.0435, max mem: 9224.0, experiment: run, epoch: 74, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 251ms, time_since_start: 04h 28m 50s 102ms, eta: 32m 38s 784ms\n","\u001b[32m2022-04-21T08:05:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0432, train/total_loss: 0.0000, train/total_loss/avg: 0.0432, max mem: 9224.0, experiment: run, epoch: 75, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 898ms, time_since_start: 04h 30m 10s 000ms, eta: 31m 08s 905ms\n","\u001b[32m2022-04-21T08:06:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0430, train/total_loss: 0.0000, train/total_loss/avg: 0.0430, max mem: 9224.0, experiment: run, epoch: 75, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 201ms, time_since_start: 04h 31m 30s 201ms, eta: 29m 54s 420ms\n","\u001b[32m2022-04-21T08:08:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0428, train/total_loss: 0.0000, train/total_loss/avg: 0.0428, max mem: 9224.0, experiment: run, epoch: 75, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 175ms, time_since_start: 04h 32m 50s 377ms, eta: 28m 32s 313ms\n","\u001b[32m2022-04-21T08:09:38 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T08:09:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T08:09:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T08:09:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T08:09:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0426, train/total_loss: 0.0000, train/total_loss/avg: 0.0426, max mem: 9224.0, experiment: run, epoch: 76, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 963ms, time_since_start: 04h 34m 19s 341ms, eta: 30m 09s 518ms\n","\u001b[32m2022-04-21T08:09:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T08:09:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T08:09:53 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T08:09:53 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T08:09:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 3.1492, val/total_loss: 3.1492, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.5205, val/hateful_memes/roc_auc: 0.7021, num_updates: 20000, epoch: 76, iterations: 20000, max_updates: 22000, val_time: 05s 471ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T08:11:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0424, train/total_loss: 0.0000, train/total_loss/avg: 0.0424, max mem: 9224.0, experiment: run, epoch: 76, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.23, time: 01m 21s 344ms, time_since_start: 04h 35m 46s 159ms, eta: 26m 11s 829ms\n","\u001b[32m2022-04-21T08:12:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0422, train/total_loss: 0.0000, train/total_loss/avg: 0.0422, max mem: 9224.0, experiment: run, epoch: 76, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 388ms, time_since_start: 04h 37m 06s 547ms, eta: 24m 31s 593ms\n","\u001b[32m2022-04-21T08:13:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0419, train/total_loss: 0.0000, train/total_loss/avg: 0.0419, max mem: 9224.0, experiment: run, epoch: 77, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.27, time: 01m 19s 763ms, time_since_start: 04h 38m 26s 311ms, eta: 22m 59s 030ms\n","\u001b[32m2022-04-21T08:15:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0417, train/total_loss: 0.0000, train/total_loss/avg: 0.0417, max mem: 9224.0, experiment: run, epoch: 77, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 274ms, time_since_start: 04h 39m 46s 586ms, eta: 21m 46s 233ms\n","\u001b[32m2022-04-21T08:16:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0415, train/total_loss: 0.0000, train/total_loss/avg: 0.0415, max mem: 9224.0, experiment: run, epoch: 78, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 151ms, time_since_start: 04h 41m 06s 737ms, eta: 20m 22s 705ms\n","\u001b[32m2022-04-21T08:17:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0413, train/total_loss: 0.0000, train/total_loss/avg: 0.0413, max mem: 9224.0, experiment: run, epoch: 78, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 195ms, time_since_start: 04h 42m 26s 932ms, eta: 19m 01s 821ms\n","\u001b[32m2022-04-21T08:19:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0411, train/total_loss: 0.0000, train/total_loss/avg: 0.0411, max mem: 9224.0, experiment: run, epoch: 78, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 234ms, time_since_start: 04h 43m 47s 167ms, eta: 17m 40s 779ms\n","\u001b[32m2022-04-21T08:20:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0409, train/total_loss: 0.0000, train/total_loss/avg: 0.0409, max mem: 9224.0, experiment: run, epoch: 79, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.27, time: 01m 19s 699ms, time_since_start: 04h 45m 06s 866ms, eta: 16m 12s 650ms\n","\u001b[32m2022-04-21T08:21:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0407, train/total_loss: 0.0000, train/total_loss/avg: 0.0407, max mem: 9224.0, experiment: run, epoch: 79, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 244ms, time_since_start: 04h 46m 27s 110ms, eta: 14m 57s 696ms\n","\u001b[32m2022-04-21T08:23:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-21T08:23:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-21T08:23:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-21T08:23:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-21T08:23:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0405, train/total_loss: 0.0000, train/total_loss/avg: 0.0405, max mem: 9224.0, experiment: run, epoch: 79, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 721ms, time_since_start: 04h 47m 55s 832ms, eta: 15m 02s 301ms\n","\u001b[32m2022-04-21T08:23:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-21T08:23:24 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-21T08:23:29 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-21T08:23:29 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T08:23:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 3.2258, val/total_loss: 3.2258, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.5205, val/hateful_memes/roc_auc: 0.7016, num_updates: 21000, epoch: 79, iterations: 21000, max_updates: 22000, val_time: 05s 131ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.721960\n","\u001b[32m2022-04-21T08:24:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0404, train/total_loss: 0.0000, train/total_loss/avg: 0.0404, max mem: 9224.0, experiment: run, epoch: 80, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 853ms, time_since_start: 04h 49m 21s 819ms, eta: 12m 20s 051ms\n","\u001b[32m2022-04-21T08:26:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0402, train/total_loss: 0.0000, train/total_loss/avg: 0.0402, max mem: 9224.0, experiment: run, epoch: 80, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 369ms, time_since_start: 04h 50m 42s 188ms, eta: 10m 53s 884ms\n","\u001b[32m2022-04-21T08:27:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0400, train/total_loss: 0.0000, train/total_loss/avg: 0.0400, max mem: 9224.0, experiment: run, epoch: 81, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.27, time: 01m 19s 847ms, time_since_start: 04h 52m 02s 036ms, eta: 09m 28s 431ms\n","\u001b[32m2022-04-21T08:28:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0398, train/total_loss: 0.0000, train/total_loss/avg: 0.0398, max mem: 9224.0, experiment: run, epoch: 81, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 409ms, time_since_start: 04h 53m 22s 445ms, eta: 08m 10s 656ms\n","\u001b[32m2022-04-21T08:30:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0396, train/total_loss: 0.0000, train/total_loss/avg: 0.0396, max mem: 9224.0, experiment: run, epoch: 81, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 344ms, time_since_start: 04h 54m 42s 789ms, eta: 06m 48s 550ms\n","\u001b[32m2022-04-21T08:31:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0394, train/total_loss: 0.0000, train/total_loss/avg: 0.0394, max mem: 9224.0, experiment: run, epoch: 82, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 151ms, time_since_start: 04h 56m 02s 941ms, eta: 05m 26s 057ms\n","\u001b[32m2022-04-21T08:32:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0392, train/total_loss: 0.0000, train/total_loss/avg: 0.0392, max mem: 9224.0, experiment: run, epoch: 82, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 401ms, time_since_start: 04h 57m 23s 343ms, eta: 04m 05s 306ms\n","\u001b[32m2022-04-21T08:34:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0391, train/total_loss: 0.0000, train/total_loss/avg: 0.0391, max mem: 9224.0, experiment: run, epoch: 82, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.25, time: 01m 20s 330ms, time_since_start: 04h 58m 43s 673ms, eta: 02m 43s 392ms\n","\u001b[32m2022-04-21T08:35:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0389, train/total_loss: 0.0000, train/total_loss/avg: 0.0389, max mem: 9224.0, experiment: run, epoch: 83, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 1.27, time: 01m 19s 711ms, time_since_start: 05h 03s 385ms, eta: 01m 21s 067ms\n"]}],"source":["!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco.yaml \\\n","training.batch_size=32 \\\n","model=visual_bert dataset=hateful_memes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27413,"status":"ok","timestamp":1650549603710,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"NaHdiO5CLt9B","outputId":"d793bb69-019e-4098-8972-5c841a4f3dfc"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-21T13:59:48 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco.yaml\n","\u001b[32m2022-04-21T13:59:48 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-21T13:59:48 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-21T13:59:48 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n","\u001b[32m2022-04-21T13:59:48 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-04-21T13:59:48 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-21T13:59:48 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-21T13:59:48 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False'])\n","\u001b[32m2022-04-21T13:59:48 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-21T13:59:48 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-04-21T13:59:48 | mmf_cli.run: \u001b[0mUsing seed 48425275\n","\u001b[32m2022-04-21T13:59:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-21T13:59:50 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T13:59:50 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T13:59:50 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-21T13:59:50 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-21T13:59:57 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-21T13:59:57 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-21T13:59:57 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T13:59:59 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-21T13:59:59 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-21T13:59:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-21T13:59:59 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n","\u001b[32m2022-04-21T13:59:59 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n","\u001b[32m2022-04-21T13:59:59 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 12\n","\u001b[32m2022-04-21T13:59:59 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-21T13:59:59 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-21T13:59:59 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-21T13:59:59 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2022-04-21T13:59:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 9/9 [00:05<00:00,  1.72it/s]\n","\u001b[32m2022-04-21T14:00:04 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 9\n","\u001b[32m2022-04-21T14:00:04 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-21T14:00:04 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 1.5560, val/total_loss: 1.5560, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4880, val/hateful_memes/roc_auc: 0.7220\n","\u001b[32m2022-04-21T14:00:04 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 07s 393ms\n"]}],"source":["!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco.yaml\" \\\n","model=visual_bert \\\n","dataset=hateful_memes \\\n","run_type=val \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73413,"status":"ok","timestamp":1650906986796,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"tg3RCNkea9Eg","outputId":"72d6fcad-c65f-47f7-9f78-e6e400d3fee0"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-25T17:15:18 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco.yaml\n","\u001b[32m2022-04-25T17:15:18 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-25T17:15:18 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-25T17:15:18 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n","\u001b[32m2022-04-25T17:15:18 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save_visual_bert_coco_bs32/best.ckpt\n","\u001b[32m2022-04-25T17:15:18 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-25T17:15:18 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-25T17:15:18 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save_visual_bert_coco_bs32/best.ckpt', 'checkpoint.resume_pretrained=False'])\n","\u001b[32m2022-04-25T17:15:18 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-25T17:15:18 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n","\u001b[32m2022-04-25T17:15:18 | mmf_cli.run: \u001b[0mUsing seed 18578278\n","\u001b[32m2022-04-25T17:15:18 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-25T17:15:26 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T17:15:26 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T17:15:26 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-25T17:15:26 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.position_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-25T17:15:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-25T17:15:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-25T17:15:33 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-25T17:15:55 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-25T17:15:55 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-25T17:15:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-25T17:15:55 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n","\u001b[32m2022-04-25T17:15:55 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n","\u001b[32m2022-04-25T17:15:55 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 12\n","\u001b[32m2022-04-25T17:15:55 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-25T17:15:55 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-25T17:15:55 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-25T17:15:55 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-04-25T17:15:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 32/32 [00:29<00:00,  1.09it/s]\n","\u001b[32m2022-04-25T17:16:24 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 32\n","\u001b[32m2022-04-25T17:16:24 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-25T17:16:25 | mmf.trainers.callbacks.logistics: \u001b[0mtest/hateful_memes/cross_entropy: 1.4032, test/total_loss: 1.4032, test/hateful_memes/accuracy: 0.7205, test/hateful_memes/binary_f1: 0.5539, test/hateful_memes/roc_auc: 0.7590\n","\u001b[32m2022-04-25T17:16:25 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 51s 641ms\n"]}],"source":["!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco.yaml\" \\\n","model=visual_bert \\\n","dataset=hateful_memes \\\n","run_type=test \\\n","checkpoint.resume_file=save_visual_bert_coco_bs32/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ksv-XPSuFZS"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"4c8odeTj-7BM"},"source":["# **Visual Bert COCO 50 no text + data augmentation**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ma_BpbeQ_EN8","outputId":"388fc4ca-8145-4f45-b72f-d322aa9e4be0"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-27T03:14:46 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yaml\n","\u001b[32m2022-04-27T03:14:46 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n","\u001b[32m2022-04-27T03:14:46 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-27T03:14:46 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-27T03:14:46 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-27T03:14:46 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yaml', 'training.batch_size=32', 'model=visual_bert', 'dataset=hateful_memes'])\n","\u001b[32m2022-04-27T03:14:46 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-27T03:14:46 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-04-27T03:14:46 | mmf_cli.run: \u001b[0mUsing seed 46849637\n","\u001b[32m2022-04-27T03:14:46 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [07:59<00:00, 21.5MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 143kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmporinpdpd\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 24.9kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptbxdy7lp\n","Downloading: 100% 570/570 [00:00<00:00, 478kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpcsxfb8co\n","Downloading: 100% 232k/232k [00:00<00:00, 911kB/s] \n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpu_es3qmq\n","Downloading: 100% 466k/466k [00:00<00:00, 1.10MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-27T03:25:36 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-27T03:25:36 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-27T03:25:36 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-27T03:25:36 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmpneyp9bq9\n","Downloading: 100% 440M/440M [00:05<00:00, 73.7MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.bias', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-27T03:25:51 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-27T03:25:51 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-27T03:25:51 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n","Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:21<00:00, 19.7MB/s]\n","[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n","Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-27T03:26:18 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-27T03:26:18 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-27T03:26:18 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-27T03:26:18 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2022-04-27T03:26:18 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2022-04-27T03:26:19 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-27T03:26:19 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-27T03:26:19 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-27T03:26:19 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-04-27T03:27:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.7078, train/hateful_memes/cross_entropy/avg: 0.7078, train/total_loss: 0.7078, train/total_loss/avg: 0.7078, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 2.04, time: 49s 769ms, time_since_start: 01m 17s 124ms, eta: 03h 04m 44s 866ms\n","\u001b[32m2022-04-27T03:27:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6525, train/hateful_memes/cross_entropy/avg: 0.6802, train/total_loss: 0.6525, train/total_loss/avg: 0.6802, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 369ms, time_since_start: 02m 04s 494ms, eta: 02h 55m 02s 208ms\n","\u001b[32m2022-04-27T03:28:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6525, train/hateful_memes/cross_entropy/avg: 0.5901, train/total_loss: 0.6525, train/total_loss/avg: 0.5901, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 283ms, time_since_start: 02m 51s 778ms, eta: 02h 53m 54s 935ms\n","\u001b[32m2022-04-27T03:29:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6421, train/hateful_memes/cross_entropy/avg: 0.6031, train/total_loss: 0.6421, train/total_loss/avg: 0.6031, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 529ms, time_since_start: 03m 39s 307ms, eta: 02h 54m 913ms\n","\u001b[32m2022-04-27T03:30:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.6421, train/hateful_memes/cross_entropy/avg: 0.5635, train/total_loss: 0.6421, train/total_loss/avg: 0.5635, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 456ms, time_since_start: 04m 26s 764ms, eta: 02h 52m 56s 579ms\n","\u001b[32m2022-04-27T03:31:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.4664, train/hateful_memes/cross_entropy/avg: 0.5473, train/total_loss: 0.4664, train/total_loss/avg: 0.5473, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 085ms, time_since_start: 05m 13s 849ms, eta: 02h 50m 47s 564ms\n","\u001b[32m2022-04-27T03:31:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.5245, train/hateful_memes/cross_entropy/avg: 0.5441, train/total_loss: 0.5245, train/total_loss/avg: 0.5441, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 577ms, time_since_start: 06m 01s 426ms, eta: 02h 51m 46s 252ms\n","\u001b[32m2022-04-27T03:32:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.4664, train/hateful_memes/cross_entropy/avg: 0.5215, train/total_loss: 0.4664, train/total_loss/avg: 0.5215, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 564ms, time_since_start: 06m 48s 991ms, eta: 02h 50m 55s 182ms\n","\u001b[32m2022-04-27T03:33:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.4664, train/hateful_memes/cross_entropy/avg: 0.4896, train/total_loss: 0.4664, train/total_loss/avg: 0.4896, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 731ms, time_since_start: 07m 36s 723ms, eta: 02h 50m 42s 619ms\n","\u001b[32m2022-04-27T03:34:16 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T03:34:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T03:34:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T03:34:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T03:34:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.4101, train/hateful_memes/cross_entropy/avg: 0.4750, train/total_loss: 0.4101, train/total_loss/avg: 0.4750, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.72, time: 58s 541ms, time_since_start: 08m 35s 264ms, eta: 03h 28m 22s 622ms\n","\u001b[32m2022-04-27T03:34:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T03:34:27 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T03:34:32 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T03:34:32 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T03:34:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T03:34:37 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-27T03:34:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T03:34:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T03:34:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.7477, val/total_loss: 0.7477, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4514, val/hateful_memes/roc_auc: 0.6975, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 22s 220ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.697529\n","\u001b[32m2022-04-27T03:35:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.4101, train/hateful_memes/cross_entropy/avg: 0.4610, train/total_loss: 0.4101, train/total_loss/avg: 0.4610, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 567ms, time_since_start: 09m 46s 054ms, eta: 02h 52m 03s 084ms\n","\u001b[32m2022-04-27T03:36:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.4050, train/hateful_memes/cross_entropy/avg: 0.4322, train/total_loss: 0.4050, train/total_loss/avg: 0.4322, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 987ms, time_since_start: 10m 34s 041ms, eta: 02h 49m 11s 117ms\n","\u001b[32m2022-04-27T03:37:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.4050, train/hateful_memes/cross_entropy/avg: 0.4288, train/total_loss: 0.4050, train/total_loss/avg: 0.4288, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 913ms, time_since_start: 11m 21s 955ms, eta: 02h 48m 06s 702ms\n","\u001b[32m2022-04-27T03:38:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.3882, train/hateful_memes/cross_entropy/avg: 0.4098, train/total_loss: 0.3882, train/total_loss/avg: 0.4098, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 712ms, time_since_start: 12m 09s 668ms, eta: 02h 46m 35s 943ms\n","\u001b[32m2022-04-27T03:38:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.3882, train/hateful_memes/cross_entropy/avg: 0.3921, train/total_loss: 0.3882, train/total_loss/avg: 0.3921, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 850ms, time_since_start: 12m 57s 518ms, eta: 02h 46m 16s 059ms\n","\u001b[32m2022-04-27T03:39:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.3632, train/hateful_memes/cross_entropy/avg: 0.3723, train/total_loss: 0.3632, train/total_loss/avg: 0.3723, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 425ms, time_since_start: 13m 44s 943ms, eta: 02h 43m 59s 201ms\n","\u001b[32m2022-04-27T03:40:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.3632, train/hateful_memes/cross_entropy/avg: 0.3632, train/total_loss: 0.3632, train/total_loss/avg: 0.3632, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 873ms, time_since_start: 14m 32s 816ms, eta: 02h 44m 43s 456ms\n","\u001b[32m2022-04-27T03:41:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.3432, train/hateful_memes/cross_entropy/avg: 0.3485, train/total_loss: 0.3432, train/total_loss/avg: 0.3485, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 853ms, time_since_start: 15m 20s 670ms, eta: 02h 43m 50s 841ms\n","\u001b[32m2022-04-27T03:42:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.3432, train/hateful_memes/cross_entropy/avg: 0.3383, train/total_loss: 0.3432, train/total_loss/avg: 0.3383, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 733ms, time_since_start: 16m 08s 404ms, eta: 02h 42m 37s 523ms\n","\u001b[32m2022-04-27T03:42:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T03:42:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T03:42:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T03:42:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T03:42:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.3432, train/hateful_memes/cross_entropy/avg: 0.3391, train/total_loss: 0.3432, train/total_loss/avg: 0.3391, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.79, time: 56s 581ms, time_since_start: 17m 04s 985ms, eta: 03h 11m 48s 614ms\n","\u001b[32m2022-04-27T03:42:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T03:42:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T03:43:02 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T03:43:02 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T03:43:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T03:43:07 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-27T03:43:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T03:43:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T03:43:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.1725, val/total_loss: 1.1725, val/hateful_memes/accuracy: 0.6648, val/hateful_memes/binary_f1: 0.5068, val/hateful_memes/roc_auc: 0.7033, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 21s 981ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.703305\n","\u001b[32m2022-04-27T03:44:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.3215, train/hateful_memes/cross_entropy/avg: 0.3287, train/total_loss: 0.3215, train/total_loss/avg: 0.3287, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 657ms, time_since_start: 18m 15s 625ms, eta: 02h 44m 07s 468ms\n","\u001b[32m2022-04-27T03:44:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.2349, train/hateful_memes/cross_entropy/avg: 0.3149, train/total_loss: 0.2349, train/total_loss/avg: 0.3149, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 536ms, time_since_start: 19m 03s 162ms, eta: 02h 39m 32s 313ms\n","\u001b[32m2022-04-27T03:45:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.2183, train/hateful_memes/cross_entropy/avg: 0.3049, train/total_loss: 0.2183, train/total_loss/avg: 0.3049, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 849ms, time_since_start: 19m 51s 012ms, eta: 02h 39m 46s 576ms\n","\u001b[32m2022-04-27T03:46:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.1626, train/hateful_memes/cross_entropy/avg: 0.2955, train/total_loss: 0.1626, train/total_loss/avg: 0.2955, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 928ms, time_since_start: 20m 38s 941ms, eta: 02h 39m 13s 753ms\n","\u001b[32m2022-04-27T03:47:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.1626, train/hateful_memes/cross_entropy/avg: 0.2912, train/total_loss: 0.1626, train/total_loss/avg: 0.2912, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 943ms, time_since_start: 21m 26s 884ms, eta: 02h 38m 27s 901ms\n","\u001b[32m2022-04-27T03:48:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.1533, train/hateful_memes/cross_entropy/avg: 0.2805, train/total_loss: 0.1533, train/total_loss/avg: 0.2805, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 961ms, time_since_start: 22m 14s 846ms, eta: 02h 37m 42s 739ms\n","\u001b[32m2022-04-27T03:48:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.1437, train/hateful_memes/cross_entropy/avg: 0.2710, train/total_loss: 0.1437, train/total_loss/avg: 0.2710, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 576ms, time_since_start: 23m 02s 423ms, eta: 02h 35m 38s 451ms\n","\u001b[32m2022-04-27T03:49:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.1204, train/hateful_memes/cross_entropy/avg: 0.2626, train/total_loss: 0.1204, train/total_loss/avg: 0.2626, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 992ms, time_since_start: 23m 50s 415ms, eta: 02h 36m 11s 243ms\n","\u001b[32m2022-04-27T03:50:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.1150, train/hateful_memes/cross_entropy/avg: 0.2544, train/total_loss: 0.1150, train/total_loss/avg: 0.2544, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 027ms, time_since_start: 24m 38s 443ms, eta: 02h 35m 29s 144ms\n","\u001b[32m2022-04-27T03:51:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T03:51:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T03:51:24 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T03:51:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T03:51:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.0991, train/hateful_memes/cross_entropy/avg: 0.2463, train/total_loss: 0.0991, train/total_loss/avg: 0.2463, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.64, time: 01m 01s 073ms, time_since_start: 25m 39s 516ms, eta: 03h 16m 41s 250ms\n","\u001b[32m2022-04-27T03:51:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T03:51:31 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T03:51:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T03:51:34 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T03:51:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T03:51:40 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-27T03:51:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T03:51:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T03:51:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.7026, val/total_loss: 1.7026, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.4865, val/hateful_memes/roc_auc: 0.7067, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 19s 232ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.706684\n","\u001b[32m2022-04-27T03:52:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.0857, train/hateful_memes/cross_entropy/avg: 0.2391, train/total_loss: 0.0857, train/total_loss/avg: 0.2391, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 926ms, time_since_start: 26m 47s 677ms, eta: 02h 36m 44s 269ms\n","\u001b[32m2022-04-27T03:53:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.0781, train/hateful_memes/cross_entropy/avg: 0.2318, train/total_loss: 0.0781, train/total_loss/avg: 0.2318, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 926ms, time_since_start: 27m 35s 603ms, eta: 02h 32m 43s 417ms\n","\u001b[32m2022-04-27T03:54:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0781, train/hateful_memes/cross_entropy/avg: 0.2279, train/total_loss: 0.0781, train/total_loss/avg: 0.2279, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 929ms, time_since_start: 28m 23s 533ms, eta: 02h 31m 55s 261ms\n","\u001b[32m2022-04-27T03:55:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0756, train/hateful_memes/cross_entropy/avg: 0.2222, train/total_loss: 0.0756, train/total_loss/avg: 0.2222, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 077ms, time_since_start: 29m 11s 611ms, eta: 02h 31m 34s 516ms\n","\u001b[32m2022-04-27T03:55:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0354, train/hateful_memes/cross_entropy/avg: 0.2161, train/total_loss: 0.0354, train/total_loss/avg: 0.2161, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 963ms, time_since_start: 29m 59s 575ms, eta: 02h 30m 24s 071ms\n","\u001b[32m2022-04-27T03:56:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0354, train/hateful_memes/cross_entropy/avg: 0.2159, train/total_loss: 0.0354, train/total_loss/avg: 0.2159, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 188ms, time_since_start: 30m 47s 763ms, eta: 02h 30m 17s 398ms\n","\u001b[32m2022-04-27T03:57:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0354, train/hateful_memes/cross_entropy/avg: 0.2118, train/total_loss: 0.0354, train/total_loss/avg: 0.2118, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 099ms, time_since_start: 31m 35s 862ms, eta: 02h 29m 11s 802ms\n","\u001b[32m2022-04-27T03:58:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0345, train/hateful_memes/cross_entropy/avg: 0.2063, train/total_loss: 0.0345, train/total_loss/avg: 0.2063, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 510ms, time_since_start: 32m 23s 373ms, eta: 02h 26m 33s 868ms\n","\u001b[32m2022-04-27T03:59:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0345, train/hateful_memes/cross_entropy/avg: 0.2019, train/total_loss: 0.0345, train/total_loss/avg: 0.2019, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 159ms, time_since_start: 33m 11s 532ms, eta: 02h 27m 45s 008ms\n","\u001b[32m2022-04-27T03:59:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T03:59:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T03:59:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:00:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:00:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0258, train/hateful_memes/cross_entropy/avg: 0.1971, train/total_loss: 0.0258, train/total_loss/avg: 0.1971, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.75, time: 57s 982ms, time_since_start: 34m 09s 514ms, eta: 02h 56m 54s 242ms\n","\u001b[32m2022-04-27T04:00:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T04:00:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T04:00:04 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T04:00:04 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T04:00:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:00:09 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-27T04:00:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:00:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:00:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.4548, val/total_loss: 1.4548, val/hateful_memes/accuracy: 0.7167, val/hateful_memes/binary_f1: 0.5204, val/hateful_memes/roc_auc: 0.7123, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 20s 552ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.712316\n","\u001b[32m2022-04-27T04:01:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0258, train/hateful_memes/cross_entropy/avg: 0.1971, train/total_loss: 0.0258, train/total_loss/avg: 0.1971, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 799ms, time_since_start: 35m 18s 867ms, eta: 02h 28m 03s 573ms\n","\u001b[32m2022-04-27T04:01:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0345, train/hateful_memes/cross_entropy/avg: 0.1943, train/total_loss: 0.0345, train/total_loss/avg: 0.1943, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 430ms, time_since_start: 36m 07s 297ms, eta: 02h 26m 07s 157ms\n","\u001b[32m2022-04-27T04:02:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0250, train/hateful_memes/cross_entropy/avg: 0.1899, train/total_loss: 0.0250, train/total_loss/avg: 0.1899, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 744ms, time_since_start: 36m 55s 042ms, eta: 02h 23m 14s 514ms\n","\u001b[32m2022-04-27T04:03:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0238, train/hateful_memes/cross_entropy/avg: 0.1857, train/total_loss: 0.0238, train/total_loss/avg: 0.1857, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 289ms, time_since_start: 37m 43s 332ms, eta: 02h 24m 03s 462ms\n","\u001b[32m2022-04-27T04:04:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0223, train/hateful_memes/cross_entropy/avg: 0.1817, train/total_loss: 0.0223, train/total_loss/avg: 0.1817, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 203ms, time_since_start: 38m 31s 535ms, eta: 02h 22m 58s 941ms\n","\u001b[32m2022-04-27T04:05:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0223, train/hateful_memes/cross_entropy/avg: 0.1780, train/total_loss: 0.0223, train/total_loss/avg: 0.1780, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 164ms, time_since_start: 39m 19s 699ms, eta: 02h 22m 03s 047ms\n","\u001b[32m2022-04-27T04:05:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0145, train/hateful_memes/cross_entropy/avg: 0.1745, train/total_loss: 0.0145, train/total_loss/avg: 0.1745, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 211ms, time_since_start: 40m 07s 911ms, eta: 02h 21m 22s 363ms\n","\u001b[32m2022-04-27T04:06:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0143, train/hateful_memes/cross_entropy/avg: 0.1710, train/total_loss: 0.0143, train/total_loss/avg: 0.1710, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 602ms, time_since_start: 40m 55s 513ms, eta: 02h 18m 46s 786ms\n","\u001b[32m2022-04-27T04:07:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0131, train/hateful_memes/cross_entropy/avg: 0.1675, train/total_loss: 0.0131, train/total_loss/avg: 0.1675, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 252ms, time_since_start: 41m 43s 766ms, eta: 02h 19m 51s 483ms\n","\u001b[32m2022-04-27T04:08:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T04:08:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:08:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:08:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:08:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0127, train/hateful_memes/cross_entropy/avg: 0.1645, train/total_loss: 0.0127, train/total_loss/avg: 0.1645, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.75, time: 57s 024ms, time_since_start: 42m 40s 790ms, eta: 02h 44m 18s 925ms\n","\u001b[32m2022-04-27T04:08:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T04:08:32 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T04:08:35 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T04:08:35 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T04:08:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:08:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:08:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:08:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.5413, val/total_loss: 1.5413, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.5251, val/hateful_memes/roc_auc: 0.6950, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 15s 902ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.712316\n","\u001b[32m2022-04-27T04:09:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1614, train/total_loss: 0.0088, train/total_loss/avg: 0.1614, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 850ms, time_since_start: 43m 45s 545ms, eta: 02h 19m 56s 101ms\n","\u001b[32m2022-04-27T04:10:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1583, train/total_loss: 0.0088, train/total_loss/avg: 0.1583, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 335ms, time_since_start: 44m 33s 880ms, eta: 02h 17m 38s 354ms\n","\u001b[32m2022-04-27T04:11:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1556, train/total_loss: 0.0088, train/total_loss/avg: 0.1556, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 101ms, time_since_start: 45m 21s 982ms, eta: 02h 16m 09s 554ms\n","\u001b[32m2022-04-27T04:12:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1528, train/total_loss: 0.0073, train/total_loss/avg: 0.1528, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 796ms, time_since_start: 46m 09s 778ms, eta: 02h 14m 29s 058ms\n","\u001b[32m2022-04-27T04:12:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1506, train/total_loss: 0.0088, train/total_loss/avg: 0.1506, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 251ms, time_since_start: 46m 58s 030ms, eta: 02h 14m 56s 898ms\n","\u001b[32m2022-04-27T04:13:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1515, train/total_loss: 0.0088, train/total_loss/avg: 0.1515, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 105ms, time_since_start: 47m 46s 136ms, eta: 02h 13m 43s 475ms\n","\u001b[32m2022-04-27T04:14:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1489, train/total_loss: 0.0065, train/total_loss/avg: 0.1489, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 267ms, time_since_start: 48m 34s 403ms, eta: 02h 13m 21s 339ms\n","\u001b[32m2022-04-27T04:15:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1475, train/total_loss: 0.0088, train/total_loss/avg: 0.1475, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 529ms, time_since_start: 49m 22s 933ms, eta: 02h 13m 15s 384ms\n","\u001b[32m2022-04-27T04:16:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1451, train/total_loss: 0.0065, train/total_loss/avg: 0.1451, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 100ms, time_since_start: 50m 11s 033ms, eta: 02h 11m 15s 847ms\n","\u001b[32m2022-04-27T04:16:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T04:16:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:16:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:17:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:17:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1429, train/total_loss: 0.0065, train/total_loss/avg: 0.1429, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.72, time: 58s 939ms, time_since_start: 51m 09s 973ms, eta: 02h 39m 50s 705ms\n","\u001b[32m2022-04-27T04:17:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T04:17:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T04:17:05 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T04:17:05 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T04:17:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:17:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:17:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:17:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.2931, val/total_loss: 2.2931, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4247, val/hateful_memes/roc_auc: 0.6967, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 15s 872ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.712316\n","\u001b[32m2022-04-27T04:18:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0054, train/hateful_memes/cross_entropy/avg: 0.1406, train/total_loss: 0.0054, train/total_loss/avg: 0.1406, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 849ms, time_since_start: 52m 14s 698ms, eta: 02h 11m 39s 192ms\n","\u001b[32m2022-04-27T04:18:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0053, train/hateful_memes/cross_entropy/avg: 0.1384, train/total_loss: 0.0053, train/total_loss/avg: 0.1384, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 262ms, time_since_start: 53m 02s 960ms, eta: 02h 09m 15s 069ms\n","\u001b[32m2022-04-27T04:19:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0054, train/hateful_memes/cross_entropy/avg: 0.1363, train/total_loss: 0.0054, train/total_loss/avg: 0.1363, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 387ms, time_since_start: 53m 51s 347ms, eta: 02h 08m 45s 936ms\n","\u001b[32m2022-04-27T04:20:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0054, train/hateful_memes/cross_entropy/avg: 0.1342, train/total_loss: 0.0054, train/total_loss/avg: 0.1342, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 042ms, time_since_start: 54m 39s 390ms, eta: 02h 07m 02s 025ms\n","\u001b[32m2022-04-27T04:21:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.1322, train/total_loss: 0.0041, train/total_loss/avg: 0.1322, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 438ms, time_since_start: 55m 27s 828ms, eta: 02h 07m 15s 564ms\n","\u001b[32m2022-04-27T04:22:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.1302, train/total_loss: 0.0031, train/total_loss/avg: 0.1302, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 556ms, time_since_start: 56m 16s 385ms, eta: 02h 06m 44s 825ms\n","\u001b[32m2022-04-27T04:22:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.1283, train/total_loss: 0.0026, train/total_loss/avg: 0.1283, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 274ms, time_since_start: 57m 04s 659ms, eta: 02h 05m 11s 582ms\n","\u001b[32m2022-04-27T04:23:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.1264, train/total_loss: 0.0026, train/total_loss/avg: 0.1264, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 487ms, time_since_start: 57m 53s 147ms, eta: 02h 04m 55s 388ms\n","\u001b[32m2022-04-27T04:24:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.1246, train/total_loss: 0.0022, train/total_loss/avg: 0.1246, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 367ms, time_since_start: 58m 41s 515ms, eta: 02h 03m 47s 720ms\n","\u001b[32m2022-04-27T04:25:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T04:25:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:25:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:25:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:25:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.1235, train/total_loss: 0.0022, train/total_loss/avg: 0.1235, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.72, time: 58s 721ms, time_since_start: 59m 40s 236ms, eta: 02h 29m 18s 003ms\n","\u001b[32m2022-04-27T04:25:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T04:25:32 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T04:25:35 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T04:25:35 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T04:25:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:25:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:25:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:25:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 1.7108, val/total_loss: 1.7108, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.5670, val/hateful_memes/roc_auc: 0.6970, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 11s 998ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.712316\n","\u001b[32m2022-04-27T04:26:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.1222, train/total_loss: 0.0022, train/total_loss/avg: 0.1222, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 2.04, time: 49s 067ms, time_since_start: 01h 41s 304ms, eta: 02h 03m 55s 326ms\n","\u001b[32m2022-04-27T04:27:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.1206, train/total_loss: 0.0026, train/total_loss/avg: 0.1206, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 550ms, time_since_start: 01h 01m 29s 854ms, eta: 02h 01m 47s 587ms\n","\u001b[32m2022-04-27T04:28:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.1190, train/total_loss: 0.0022, train/total_loss/avg: 0.1190, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 441ms, time_since_start: 01h 02m 18s 295ms, eta: 02h 41s 885ms\n","\u001b[32m2022-04-27T04:28:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1174, train/total_loss: 0.0020, train/total_loss/avg: 0.1174, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 386ms, time_since_start: 01h 03m 06s 681ms, eta: 01h 59m 44s 466ms\n","\u001b[32m2022-04-27T04:29:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1158, train/total_loss: 0.0019, train/total_loss/avg: 0.1158, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 954ms, time_since_start: 01h 03m 54s 635ms, eta: 01h 57m 51s 542ms\n","\u001b[32m2022-04-27T04:30:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1143, train/total_loss: 0.0019, train/total_loss/avg: 0.1143, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 474ms, time_since_start: 01h 04m 43s 110ms, eta: 01h 58m 19s 047ms\n","\u001b[32m2022-04-27T04:31:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1129, train/total_loss: 0.0018, train/total_loss/avg: 0.1129, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 512ms, time_since_start: 01h 05m 31s 623ms, eta: 01h 57m 35s 235ms\n","\u001b[32m2022-04-27T04:32:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1115, train/total_loss: 0.0018, train/total_loss/avg: 0.1115, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 524ms, time_since_start: 01h 06m 20s 147ms, eta: 01h 56m 47s 665ms\n","\u001b[32m2022-04-27T04:33:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1129, train/total_loss: 0.0019, train/total_loss/avg: 0.1129, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 503ms, time_since_start: 01h 07m 08s 651ms, eta: 01h 55m 55s 265ms\n","\u001b[32m2022-04-27T04:33:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T04:33:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:33:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:33:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:33:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1115, train/total_loss: 0.0019, train/total_loss/avg: 0.1115, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.75, time: 57s 840ms, time_since_start: 01h 08m 06s 491ms, eta: 02h 17m 15s 310ms\n","\u001b[32m2022-04-27T04:33:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T04:33:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T04:34:01 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T04:34:01 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T04:34:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:34:07 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-27T04:34:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:34:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:34:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.0923, val/total_loss: 2.0923, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.4724, val/hateful_memes/roc_auc: 0.7125, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 24s 098ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.712482\n","\u001b[32m2022-04-27T04:35:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1102, train/total_loss: 0.0018, train/total_loss/avg: 0.1102, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 2.04, time: 49s 102ms, time_since_start: 01h 09m 19s 694ms, eta: 01h 55m 41s 321ms\n","\u001b[32m2022-04-27T04:36:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1088, train/total_loss: 0.0020, train/total_loss/avg: 0.1088, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 710ms, time_since_start: 01h 10m 08s 405ms, eta: 01h 53m 56s 314ms\n","\u001b[32m2022-04-27T04:36:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1075, train/total_loss: 0.0020, train/total_loss/avg: 0.1075, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 389ms, time_since_start: 01h 10m 56s 794ms, eta: 01h 52m 22s 024ms\n","\u001b[32m2022-04-27T04:37:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1063, train/total_loss: 0.0020, train/total_loss/avg: 0.1063, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 466ms, time_since_start: 01h 11m 45s 261ms, eta: 01h 51m 43s 551ms\n","\u001b[32m2022-04-27T04:38:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1051, train/total_loss: 0.0020, train/total_loss/avg: 0.1051, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 474ms, time_since_start: 01h 12m 33s 735ms, eta: 01h 50m 55s 258ms\n","\u001b[32m2022-04-27T04:39:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1039, train/total_loss: 0.0020, train/total_loss/avg: 0.1039, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 006ms, time_since_start: 01h 13m 21s 742ms, eta: 01h 49m 02s 266ms\n","\u001b[32m2022-04-27T04:40:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1027, train/total_loss: 0.0018, train/total_loss/avg: 0.1027, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 419ms, time_since_start: 01h 14m 10s 161ms, eta: 01h 49m 09s 205ms\n","\u001b[32m2022-04-27T04:40:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1016, train/total_loss: 0.0020, train/total_loss/avg: 0.1016, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 250ms, time_since_start: 01h 14m 58s 411ms, eta: 01h 47m 57s 348ms\n","\u001b[32m2022-04-27T04:41:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1005, train/total_loss: 0.0020, train/total_loss/avg: 0.1005, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 252ms, time_since_start: 01h 15m 46s 663ms, eta: 01h 47m 08s 491ms\n","\u001b[32m2022-04-27T04:42:26 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T04:42:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:42:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:42:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:42:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.0994, train/total_loss: 0.0020, train/total_loss/avg: 0.0994, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 1.69, time: 59s 072ms, time_since_start: 01h 16m 45s 736ms, eta: 02h 10m 09s 997ms\n","\u001b[32m2022-04-27T04:42:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T04:42:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T04:42:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T04:42:40 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T04:42:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:42:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:42:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:42:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 1.7071, val/total_loss: 1.7071, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.5411, val/hateful_memes/roc_auc: 0.6941, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 22000, val_time: 16s 307ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.712482\n","\u001b[32m2022-04-27T04:43:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.0984, train/total_loss: 0.0019, train/total_loss/avg: 0.0984, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 832ms, time_since_start: 01h 17m 50s 878ms, eta: 01h 46m 46s 544ms\n","\u001b[32m2022-04-27T04:44:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.0973, train/total_loss: 0.0018, train/total_loss/avg: 0.0973, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 368ms, time_since_start: 01h 18m 39s 246ms, eta: 01h 44m 56s 366ms\n","\u001b[32m2022-04-27T04:45:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.0963, train/total_loss: 0.0018, train/total_loss/avg: 0.0963, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 582ms, time_since_start: 01h 19m 27s 829ms, eta: 01h 44m 34s 897ms\n","\u001b[32m2022-04-27T04:46:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.0952, train/total_loss: 0.0015, train/total_loss/avg: 0.0952, max mem: 9224.0, experiment: run, epoch: 36, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 438ms, time_since_start: 01h 20m 16s 267ms, eta: 01h 43m 27s 026ms\n","\u001b[32m2022-04-27T04:46:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.0943, train/total_loss: 0.0019, train/total_loss/avg: 0.0943, max mem: 9224.0, experiment: run, epoch: 36, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 544ms, time_since_start: 01h 21m 04s 812ms, eta: 01h 42m 51s 254ms\n","\u001b[32m2022-04-27T04:47:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.0934, train/total_loss: 0.0019, train/total_loss/avg: 0.0934, max mem: 9224.0, experiment: run, epoch: 37, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 090ms, time_since_start: 01h 21m 52s 903ms, eta: 01h 41m 04s 633ms\n","\u001b[32m2022-04-27T04:48:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.0924, train/total_loss: 0.0019, train/total_loss/avg: 0.0924, max mem: 9224.0, experiment: run, epoch: 37, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 243ms, time_since_start: 01h 22m 41s 146ms, eta: 01h 40m 34s 808ms\n","\u001b[32m2022-04-27T04:49:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.0916, train/total_loss: 0.0019, train/total_loss/avg: 0.0916, max mem: 9224.0, experiment: run, epoch: 37, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 346ms, time_since_start: 01h 23m 29s 493ms, eta: 01h 39m 58s 513ms\n","\u001b[32m2022-04-27T04:50:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.0907, train/total_loss: 0.0015, train/total_loss/avg: 0.0907, max mem: 9224.0, experiment: run, epoch: 38, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 004ms, time_since_start: 01h 24m 17s 497ms, eta: 01h 38m 27s 264ms\n","\u001b[32m2022-04-27T04:50:57 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T04:50:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:51:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:51:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:51:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0898, train/total_loss: 0.0013, train/total_loss/avg: 0.0898, max mem: 9224.0, experiment: run, epoch: 38, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 035ms, time_since_start: 01h 25m 19s 532ms, eta: 02h 06m 10s 806ms\n","\u001b[32m2022-04-27T04:51:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T04:51:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T04:51:14 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T04:51:14 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T04:51:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:51:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:51:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:51:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.4075, val/total_loss: 2.4075, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.5143, val/hateful_memes/roc_auc: 0.6976, num_updates: 10000, epoch: 38, iterations: 10000, max_updates: 22000, val_time: 15s 502ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.712482\n","\u001b[32m2022-04-27T04:52:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0889, train/total_loss: 0.0013, train/total_loss/avg: 0.0889, max mem: 9224.0, experiment: run, epoch: 38, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 2.04, time: 49s 112ms, time_since_start: 01h 26m 24s 149ms, eta: 01h 39m 03s 781ms\n","\u001b[32m2022-04-27T04:53:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0881, train/total_loss: 0.0013, train/total_loss/avg: 0.0881, max mem: 9224.0, experiment: run, epoch: 39, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 076ms, time_since_start: 01h 27m 12s 225ms, eta: 01h 36m 09s 441ms\n","\u001b[32m2022-04-27T04:53:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0872, train/total_loss: 0.0006, train/total_loss/avg: 0.0872, max mem: 9224.0, experiment: run, epoch: 39, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 433ms, time_since_start: 01h 28m 659ms, eta: 01h 36m 03s 047ms\n","\u001b[32m2022-04-27T04:54:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0868, train/total_loss: 0.0013, train/total_loss/avg: 0.0868, max mem: 9224.0, experiment: run, epoch: 40, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 352ms, time_since_start: 01h 28m 49s 011ms, eta: 01h 35m 04s 196ms\n","\u001b[32m2022-04-27T04:55:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0866, train/total_loss: 0.0013, train/total_loss/avg: 0.0866, max mem: 9224.0, experiment: run, epoch: 40, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 389ms, time_since_start: 01h 29m 37s 400ms, eta: 01h 34m 19s 351ms\n","\u001b[32m2022-04-27T04:56:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0858, train/total_loss: 0.0006, train/total_loss/avg: 0.0858, max mem: 9224.0, experiment: run, epoch: 40, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 488ms, time_since_start: 01h 30m 25s 889ms, eta: 01h 33m 41s 707ms\n","\u001b[32m2022-04-27T04:57:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0850, train/total_loss: 0.0006, train/total_loss/avg: 0.0850, max mem: 9224.0, experiment: run, epoch: 41, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 064ms, time_since_start: 01h 31m 13s 954ms, eta: 01h 32m 03s 668ms\n","\u001b[32m2022-04-27T04:57:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0842, train/total_loss: 0.0005, train/total_loss/avg: 0.0842, max mem: 9224.0, experiment: run, epoch: 41, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 378ms, time_since_start: 01h 32m 02s 332ms, eta: 01h 31m 50s 492ms\n","\u001b[32m2022-04-27T04:58:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0835, train/total_loss: 0.0005, train/total_loss/avg: 0.0835, max mem: 9224.0, experiment: run, epoch: 41, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 424ms, time_since_start: 01h 32m 50s 757ms, eta: 01h 31m 06s 553ms\n","\u001b[32m2022-04-27T04:59:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T04:59:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:59:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:59:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:59:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0828, train/total_loss: 0.0005, train/total_loss/avg: 0.0828, max mem: 9224.0, experiment: run, epoch: 42, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 1.72, time: 58s 143ms, time_since_start: 01h 33m 48s 900ms, eta: 01h 48m 24s 464ms\n","\u001b[32m2022-04-27T04:59:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T04:59:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T04:59:43 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T04:59:43 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T04:59:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T04:59:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T04:59:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T04:59:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.4911, val/total_loss: 2.4911, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.5015, val/hateful_memes/roc_auc: 0.7084, num_updates: 11000, epoch: 42, iterations: 11000, max_updates: 22000, val_time: 15s 093ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.712482\n","\u001b[32m2022-04-27T05:00:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0821, train/total_loss: 0.0005, train/total_loss/avg: 0.0821, max mem: 9224.0, experiment: run, epoch: 42, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 2.04, time: 49s 170ms, time_since_start: 01h 34m 53s 166ms, eta: 01h 30m 50s 682ms\n","\u001b[32m2022-04-27T05:01:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0813, train/total_loss: 0.0005, train/total_loss/avg: 0.0813, max mem: 9224.0, experiment: run, epoch: 43, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 096ms, time_since_start: 01h 35m 41s 262ms, eta: 01h 28m 02s 707ms\n","\u001b[32m2022-04-27T05:02:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0806, train/total_loss: 0.0002, train/total_loss/avg: 0.0806, max mem: 9224.0, experiment: run, epoch: 43, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 337ms, time_since_start: 01h 36m 29s 600ms, eta: 01h 27m 40s 030ms\n","\u001b[32m2022-04-27T05:03:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0799, train/total_loss: 0.0005, train/total_loss/avg: 0.0799, max mem: 9224.0, experiment: run, epoch: 43, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 365ms, time_since_start: 01h 37m 17s 965ms, eta: 01h 26m 53s 865ms\n","\u001b[32m2022-04-27T05:03:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0792, train/total_loss: 0.0005, train/total_loss/avg: 0.0792, max mem: 9224.0, experiment: run, epoch: 44, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 393ms, time_since_start: 01h 38m 06s 358ms, eta: 01h 26m 07s 678ms\n","\u001b[32m2022-04-27T05:04:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0785, train/total_loss: 0.0002, train/total_loss/avg: 0.0785, max mem: 9224.0, experiment: run, epoch: 44, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 418ms, time_since_start: 01h 38m 54s 777ms, eta: 01h 25m 21s 111ms\n","\u001b[32m2022-04-27T05:05:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0779, train/total_loss: 0.0002, train/total_loss/avg: 0.0779, max mem: 9224.0, experiment: run, epoch: 44, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 233ms, time_since_start: 01h 39m 43s 010ms, eta: 01h 24m 12s 539ms\n","\u001b[32m2022-04-27T05:06:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0772, train/total_loss: 0.0002, train/total_loss/avg: 0.0772, max mem: 9224.0, experiment: run, epoch: 45, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 064ms, time_since_start: 01h 40m 31s 075ms, eta: 01h 23m 05s 945ms\n","\u001b[32m2022-04-27T05:07:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0766, train/total_loss: 0.0002, train/total_loss/avg: 0.0766, max mem: 9224.0, experiment: run, epoch: 45, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 331ms, time_since_start: 01h 41m 19s 407ms, eta: 01h 22m 44s 505ms\n","\u001b[32m2022-04-27T05:07:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T05:07:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T05:08:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T05:08:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T05:08:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0759, train/total_loss: 0.0002, train/total_loss/avg: 0.0759, max mem: 9224.0, experiment: run, epoch: 46, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 1.69, time: 59s 719ms, time_since_start: 01h 42m 19s 127ms, eta: 01h 41m 13s 512ms\n","\u001b[32m2022-04-27T05:08:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T05:08:10 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T05:08:14 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T05:08:14 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T05:08:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T05:08:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T05:08:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T05:08:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.1447, val/total_loss: 2.1447, val/hateful_memes/accuracy: 0.7037, val/hateful_memes/binary_f1: 0.5238, val/hateful_memes/roc_auc: 0.6937, num_updates: 12000, epoch: 46, iterations: 12000, max_updates: 22000, val_time: 15s 536ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.712482\n","\u001b[32m2022-04-27T05:09:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0753, train/total_loss: 0.0003, train/total_loss/avg: 0.0753, max mem: 9224.0, experiment: run, epoch: 46, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 2.04, time: 49s 273ms, time_since_start: 01h 43m 23s 938ms, eta: 01h 22m 40s 963ms\n","\u001b[32m2022-04-27T05:10:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0747, train/total_loss: 0.0003, train/total_loss/avg: 0.0747, max mem: 9224.0, experiment: run, epoch: 46, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 362ms, time_since_start: 01h 44m 12s 300ms, eta: 01h 20m 20s 079ms\n","\u001b[32m2022-04-27T05:10:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0741, train/total_loss: 0.0003, train/total_loss/avg: 0.0741, max mem: 9224.0, experiment: run, epoch: 47, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 044ms, time_since_start: 01h 45m 345ms, eta: 01h 18m 59s 545ms\n","\u001b[32m2022-04-27T05:11:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0735, train/total_loss: 0.0002, train/total_loss/avg: 0.0735, max mem: 9224.0, experiment: run, epoch: 47, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 558ms, time_since_start: 01h 45m 48s 903ms, eta: 01h 19m 890ms\n","\u001b[32m2022-04-27T05:12:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0731, train/total_loss: 0.0002, train/total_loss/avg: 0.0731, max mem: 9224.0, experiment: run, epoch: 47, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 307ms, time_since_start: 01h 46m 37s 211ms, eta: 01h 17m 47s 237ms\n","\u001b[32m2022-04-27T05:13:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0725, train/total_loss: 0.0003, train/total_loss/avg: 0.0725, max mem: 9224.0, experiment: run, epoch: 48, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 413ms, time_since_start: 01h 47m 25s 625ms, eta: 01h 17m 08s 254ms\n","\u001b[32m2022-04-27T05:14:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0725, train/total_loss: 0.0004, train/total_loss/avg: 0.0725, max mem: 9224.0, experiment: run, epoch: 48, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 401ms, time_since_start: 01h 48m 14s 027ms, eta: 01h 16m 17s 906ms\n","\u001b[32m2022-04-27T05:14:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0719, train/total_loss: 0.0004, train/total_loss/avg: 0.0719, max mem: 9224.0, experiment: run, epoch: 49, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 876ms, time_since_start: 01h 49m 01s 903ms, eta: 01h 14m 39s 472ms\n","\u001b[32m2022-04-27T05:15:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0714, train/total_loss: 0.0003, train/total_loss/avg: 0.0714, max mem: 9224.0, experiment: run, epoch: 49, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 222ms, time_since_start: 01h 49m 50s 125ms, eta: 01h 14m 22s 828ms\n","\u001b[32m2022-04-27T05:16:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T05:16:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T05:16:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T05:16:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T05:16:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0708, train/total_loss: 0.0003, train/total_loss/avg: 0.0708, max mem: 9224.0, experiment: run, epoch: 49, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 1.69, time: 59s 861ms, time_since_start: 01h 50m 49s 987ms, eta: 01h 31m 19s 139ms\n","\u001b[32m2022-04-27T05:16:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T05:16:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T05:16:44 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T05:16:44 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T05:16:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.2305, val/total_loss: 2.2305, val/hateful_memes/accuracy: 0.7037, val/hateful_memes/binary_f1: 0.5122, val/hateful_memes/roc_auc: 0.7015, num_updates: 13000, epoch: 49, iterations: 13000, max_updates: 22000, val_time: 04s 796ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.712482\n","\u001b[32m2022-04-27T05:17:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0703, train/total_loss: 0.0003, train/total_loss/avg: 0.0703, max mem: 9224.0, experiment: run, epoch: 50, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 2.04, time: 49s 097ms, time_since_start: 01h 51m 43s 883ms, eta: 01h 14m 03s 965ms\n","\u001b[32m2022-04-27T05:18:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0698, train/total_loss: 0.0003, train/total_loss/avg: 0.0698, max mem: 9224.0, experiment: run, epoch: 50, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 446ms, time_since_start: 01h 52m 32s 329ms, eta: 01h 12m 15s 795ms\n","\u001b[32m2022-04-27T05:19:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0694, train/total_loss: 0.0003, train/total_loss/avg: 0.0694, max mem: 9224.0, experiment: run, epoch: 50, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 904ms, time_since_start: 01h 53m 20s 234ms, eta: 01h 10m 38s 544ms\n","\u001b[32m2022-04-27T05:20:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0689, train/total_loss: 0.0003, train/total_loss/avg: 0.0689, max mem: 9224.0, experiment: run, epoch: 51, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 599ms, time_since_start: 01h 54m 08s 834ms, eta: 01h 10m 50s 647ms\n","\u001b[32m2022-04-27T05:20:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0684, train/total_loss: 0.0002, train/total_loss/avg: 0.0684, max mem: 9224.0, experiment: run, epoch: 51, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 422ms, time_since_start: 01h 54m 57s 256ms, eta: 01h 09m 45s 892ms\n","\u001b[32m2022-04-27T05:21:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0679, train/total_loss: 0.0002, train/total_loss/avg: 0.0679, max mem: 9224.0, experiment: run, epoch: 52, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 282ms, time_since_start: 01h 55m 45s 538ms, eta: 01h 08m 44s 637ms\n","\u001b[32m2022-04-27T05:22:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0674, train/total_loss: 0.0002, train/total_loss/avg: 0.0674, max mem: 9224.0, experiment: run, epoch: 52, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 496ms, time_since_start: 01h 56m 34s 035ms, eta: 01h 08m 13s 597ms\n","\u001b[32m2022-04-27T05:23:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0672, train/total_loss: 0.0003, train/total_loss/avg: 0.0672, max mem: 9224.0, experiment: run, epoch: 52, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 436ms, time_since_start: 01h 57m 22s 471ms, eta: 01h 07m 19s 303ms\n","\u001b[32m2022-04-27T05:24:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0667, train/total_loss: 0.0003, train/total_loss/avg: 0.0667, max mem: 9224.0, experiment: run, epoch: 53, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 941ms, time_since_start: 01h 58m 10s 412ms, eta: 01h 05m 49s 282ms\n","\u001b[32m2022-04-27T05:24:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T05:24:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T05:24:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T05:25:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T05:25:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0663, train/total_loss: 0.0003, train/total_loss/avg: 0.0663, max mem: 9224.0, experiment: run, epoch: 53, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 1.69, time: 59s 816ms, time_since_start: 01h 59m 10s 229ms, eta: 01h 21m 06s 633ms\n","\u001b[32m2022-04-27T05:25:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T05:25:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T05:25:05 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T05:25:05 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T05:25:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.3641, val/total_loss: 2.3641, val/hateful_memes/accuracy: 0.7000, val/hateful_memes/binary_f1: 0.4969, val/hateful_memes/roc_auc: 0.6880, num_updates: 14000, epoch: 53, iterations: 14000, max_updates: 22000, val_time: 03s 109ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.712482\n","\u001b[32m2022-04-27T05:25:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0658, train/total_loss: 0.0003, train/total_loss/avg: 0.0658, max mem: 9224.0, experiment: run, epoch: 54, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 2.04, time: 49s 091ms, time_since_start: 02h 02s 431ms, eta: 01h 05m 44s 145ms\n","\u001b[32m2022-04-27T05:26:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0654, train/total_loss: 0.0003, train/total_loss/avg: 0.0654, max mem: 9224.0, experiment: run, epoch: 54, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 573ms, time_since_start: 02h 51s 004ms, eta: 01h 04m 13s 121ms\n","\u001b[32m2022-04-27T05:27:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0649, train/total_loss: 0.0003, train/total_loss/avg: 0.0649, max mem: 9224.0, experiment: run, epoch: 54, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 790ms, time_since_start: 02h 01m 39s 795ms, eta: 01h 03m 40s 726ms\n","\u001b[32m2022-04-27T05:28:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0645, train/total_loss: 0.0003, train/total_loss/avg: 0.0645, max mem: 9224.0, experiment: run, epoch: 55, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 413ms, time_since_start: 02h 02m 28s 208ms, eta: 01h 02m 22s 005ms\n","\u001b[32m2022-04-27T05:29:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0640, train/total_loss: 0.0003, train/total_loss/avg: 0.0640, max mem: 9224.0, experiment: run, epoch: 55, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 549ms, time_since_start: 02h 03m 16s 758ms, eta: 01h 01m 43s 124ms\n","\u001b[32m2022-04-27T05:29:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0636, train/total_loss: 0.0003, train/total_loss/avg: 0.0636, max mem: 9224.0, experiment: run, epoch: 55, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 517ms, time_since_start: 02h 04m 05s 275ms, eta: 01h 51s 297ms\n","\u001b[32m2022-04-27T05:30:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0632, train/total_loss: 0.0002, train/total_loss/avg: 0.0632, max mem: 9224.0, experiment: run, epoch: 56, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 365ms, time_since_start: 02h 04m 53s 641ms, eta: 59m 50s 741ms\n","\u001b[32m2022-04-27T05:31:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0627, train/total_loss: 0.0002, train/total_loss/avg: 0.0627, max mem: 9224.0, experiment: run, epoch: 56, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 335ms, time_since_start: 02h 05m 41s 977ms, eta: 58m 59s 331ms\n","\u001b[32m2022-04-27T05:32:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0623, train/total_loss: 0.0002, train/total_loss/avg: 0.0623, max mem: 9224.0, experiment: run, epoch: 57, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 821ms, time_since_start: 02h 06m 29s 799ms, eta: 57m 33s 078ms\n","\u001b[32m2022-04-27T05:33:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T05:33:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T05:33:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T05:33:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T05:33:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0619, train/total_loss: 0.0002, train/total_loss/avg: 0.0619, max mem: 9224.0, experiment: run, epoch: 57, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 1.75, time: 57s 514ms, time_since_start: 02h 07m 27s 314ms, eta: 01h 08m 14s 483ms\n","\u001b[32m2022-04-27T05:33:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T05:33:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T05:33:22 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T05:33:22 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T05:33:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.5530, val/total_loss: 2.5530, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.4985, val/hateful_memes/roc_auc: 0.6927, num_updates: 15000, epoch: 57, iterations: 15000, max_updates: 22000, val_time: 03s 205ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.712482\n","\u001b[32m2022-04-27T05:34:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0615, train/total_loss: 0.0002, train/total_loss/avg: 0.0615, max mem: 9224.0, experiment: run, epoch: 57, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 2.04, time: 49s 373ms, time_since_start: 02h 08m 19s 895ms, eta: 57m 44s 702ms\n","\u001b[32m2022-04-27T05:35:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0611, train/total_loss: 0.0002, train/total_loss/avg: 0.0611, max mem: 9224.0, experiment: run, epoch: 58, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 545ms, time_since_start: 02h 09m 08s 441ms, eta: 55m 57s 248ms\n","\u001b[32m2022-04-27T05:35:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0607, train/total_loss: 0.0002, train/total_loss/avg: 0.0607, max mem: 9224.0, experiment: run, epoch: 58, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 630ms, time_since_start: 02h 09m 57s 071ms, eta: 55m 13s 611ms\n","\u001b[32m2022-04-27T05:36:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0603, train/total_loss: 0.0002, train/total_loss/avg: 0.0603, max mem: 9224.0, experiment: run, epoch: 58, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 503ms, time_since_start: 02h 10m 45s 574ms, eta: 54m 15s 639ms\n","\u001b[32m2022-04-27T05:37:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0599, train/total_loss: 0.0001, train/total_loss/avg: 0.0599, max mem: 9224.0, experiment: run, epoch: 59, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 090ms, time_since_start: 02h 11m 33s 665ms, eta: 52m 59s 035ms\n","\u001b[32m2022-04-27T05:38:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0595, train/total_loss: 0.0001, train/total_loss/avg: 0.0595, max mem: 9224.0, experiment: run, epoch: 59, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 438ms, time_since_start: 02h 12m 22s 104ms, eta: 52m 32s 794ms\n","\u001b[32m2022-04-27T05:39:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0591, train/total_loss: 0.0001, train/total_loss/avg: 0.0591, max mem: 9224.0, experiment: run, epoch: 60, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 164ms, time_since_start: 02h 13m 10s 268ms, eta: 51m 25s 920ms\n","\u001b[32m2022-04-27T05:39:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0588, train/total_loss: 0.0001, train/total_loss/avg: 0.0588, max mem: 9224.0, experiment: run, epoch: 60, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 456ms, time_since_start: 02h 13m 58s 724ms, eta: 50m 55s 381ms\n","\u001b[32m2022-04-27T05:40:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0584, train/total_loss: 0.0001, train/total_loss/avg: 0.0584, max mem: 9224.0, experiment: run, epoch: 60, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 437ms, time_since_start: 02h 14m 47s 162ms, eta: 50m 04s 900ms\n","\u001b[32m2022-04-27T05:41:27 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T05:41:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T05:41:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T05:41:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T05:41:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0580, train/total_loss: 0.0001, train/total_loss/avg: 0.0580, max mem: 9224.0, experiment: run, epoch: 61, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 1.72, time: 58s 215ms, time_since_start: 02h 15m 45s 377ms, eta: 59m 12s 318ms\n","\u001b[32m2022-04-27T05:41:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T05:41:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T05:41:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T05:41:40 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T05:41:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 2.4477, val/total_loss: 2.4477, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4970, val/hateful_memes/roc_auc: 0.6924, num_updates: 16000, epoch: 61, iterations: 16000, max_updates: 22000, val_time: 03s 638ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.712482\n","\u001b[32m2022-04-27T05:42:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0577, train/total_loss: 0.0001, train/total_loss/avg: 0.0577, max mem: 9224.0, experiment: run, epoch: 61, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 2.04, time: 49s 202ms, time_since_start: 02h 16m 38s 219ms, eta: 49m 12s 270ms\n","\u001b[32m2022-04-27T05:43:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0573, train/total_loss: 0.0001, train/total_loss/avg: 0.0573, max mem: 9224.0, experiment: run, epoch: 61, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 502ms, time_since_start: 02h 17m 26s 721ms, eta: 47m 40s 944ms\n","\u001b[32m2022-04-27T05:44:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0570, train/total_loss: 0.0001, train/total_loss/avg: 0.0570, max mem: 9224.0, experiment: run, epoch: 62, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 230ms, time_since_start: 02h 18m 14s 952ms, eta: 46m 35s 873ms\n","\u001b[32m2022-04-27T05:44:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0566, train/total_loss: 0.0001, train/total_loss/avg: 0.0566, max mem: 9224.0, experiment: run, epoch: 62, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 381ms, time_since_start: 02h 19m 03s 333ms, eta: 45m 55s 443ms\n","\u001b[32m2022-04-27T05:45:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0563, train/total_loss: 0.0001, train/total_loss/avg: 0.0563, max mem: 9224.0, experiment: run, epoch: 63, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 052ms, time_since_start: 02h 19m 51s 386ms, eta: 44m 47s 815ms\n","\u001b[32m2022-04-27T05:46:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0559, train/total_loss: 0.0000, train/total_loss/avg: 0.0559, max mem: 9224.0, experiment: run, epoch: 63, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 418ms, time_since_start: 02h 20m 39s 805ms, eta: 44m 19s 071ms\n","\u001b[32m2022-04-27T05:47:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0556, train/total_loss: 0.0000, train/total_loss/avg: 0.0556, max mem: 9224.0, experiment: run, epoch: 63, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 462ms, time_since_start: 02h 21m 28s 267ms, eta: 43m 32s 174ms\n","\u001b[32m2022-04-27T05:48:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0553, train/total_loss: 0.0000, train/total_loss/avg: 0.0553, max mem: 9224.0, experiment: run, epoch: 64, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 169ms, time_since_start: 02h 22m 16s 437ms, eta: 42m 27s 393ms\n","\u001b[32m2022-04-27T05:48:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0549, train/total_loss: 0.0000, train/total_loss/avg: 0.0549, max mem: 9224.0, experiment: run, epoch: 64, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 380ms, time_since_start: 02h 23m 04s 817ms, eta: 41m 49s 338ms\n"]}],"source":["!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yaml \\\n","training.batch_size=32 \\\n","model=visual_bert dataset=hateful_memes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":715358,"status":"ok","timestamp":1651056410575,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"RLgQJfQEAtHC","outputId":"50e29ace-421a-4b02-8e7f-0935f25be6ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-27T10:35:05 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yaml\n","\u001b[32m2022-04-27T10:35:05 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-27T10:35:05 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-27T10:35:05 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n","\u001b[32m2022-04-27T10:35:05 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-04-27T10:35:05 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-27T10:35:05 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-27T10:35:05 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False'])\n","\u001b[32m2022-04-27T10:35:05 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-27T10:35:05 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-04-27T10:35:05 | mmf_cli.run: \u001b[0mUsing seed 5138063\n","\u001b[32m2022-04-27T10:35:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [08:18<00:00, 20.7MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 171kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppx43m66r\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 24.3kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphpkimnod\n","Downloading: 100% 570/570 [00:00<00:00, 440kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4rzpzdnz\n","Downloading: 100% 232k/232k [00:00<00:00, 915kB/s] \n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmply4dq4r_\n","Downloading: 100% 466k/466k [00:00<00:00, 1.10MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-27T10:46:18 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-27T10:46:18 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-27T10:46:18 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-27T10:46:18 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmp72ueup5w\n","Downloading: 100% 440M/440M [00:06<00:00, 67.2MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.position_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-27T10:46:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-27T10:46:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-27T10:46:37 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-27T10:46:45 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-27T10:46:45 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-27T10:46:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-27T10:46:46 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 8000\n","\u001b[32m2022-04-27T10:46:46 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 8000\n","\u001b[32m2022-04-27T10:46:46 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 31\n","\u001b[32m2022-04-27T10:46:46 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-27T10:46:46 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-27T10:46:46 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-27T10:46:46 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2022-04-27T10:46:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 9/9 [00:03<00:00,  2.49it/s]\n","\u001b[32m2022-04-27T10:46:49 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 9\n","\u001b[32m2022-04-27T10:46:49 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T10:46:49 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 2.0923, val/total_loss: 2.0923, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.4724, val/hateful_memes/roc_auc: 0.7125\n","\u001b[32m2022-04-27T10:46:49 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 12s 328ms\n"]}],"source":["!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yaml\" \\\n","model=visual_bert \\\n","dataset=hateful_memes \\\n","run_type=val \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32725,"status":"ok","timestamp":1651056443292,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"Wcyrw9ThjFRM","outputId":"daca117a-0a7f-4e94-c631-249dbbd3a691"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-27T10:46:59 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yaml\n","\u001b[32m2022-04-27T10:46:59 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-27T10:46:59 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-04-27T10:46:59 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n","\u001b[32m2022-04-27T10:46:59 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n","\u001b[32m2022-04-27T10:46:59 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-27T10:46:59 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-27T10:46:59 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False'])\n","\u001b[32m2022-04-27T10:46:59 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-27T10:46:59 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-04-27T10:46:59 | mmf_cli.run: \u001b[0mUsing seed 59980262\n","\u001b[32m2022-04-27T10:46:59 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-27T10:47:03 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-27T10:47:03 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-27T10:47:03 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-27T10:47:03 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-27T10:47:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-27T10:47:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-27T10:47:10 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-27T10:47:11 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-27T10:47:11 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-27T10:47:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-27T10:47:11 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 8000\n","\u001b[32m2022-04-27T10:47:11 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 8000\n","\u001b[32m2022-04-27T10:47:11 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 31\n","\u001b[32m2022-04-27T10:47:11 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-27T10:47:11 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-27T10:47:11 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-27T10:47:11 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-04-27T10:47:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 32/32 [00:11<00:00,  2.89it/s]\n","\u001b[32m2022-04-27T10:47:22 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 32\n","\u001b[32m2022-04-27T10:47:22 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T10:47:23 | mmf.trainers.callbacks.logistics: \u001b[0mtest/hateful_memes/cross_entropy: 1.8807, test/total_loss: 1.8807, test/hateful_memes/accuracy: 0.7180, test/hateful_memes/binary_f1: 0.5316, test/hateful_memes/roc_auc: 0.7581\n","\u001b[32m2022-04-27T10:47:23 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 12s 913ms\n"]}],"source":["!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yaml\" \\\n","model=visual_bert \\\n","dataset=hateful_memes \\\n","run_type=test \\\n","checkpoint.resume_file=save/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"markdown","metadata":{"id":"8IvqaT6itY6N"},"source":["# **Visual Bert Coco + data augmentation**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":748462,"status":"ok","timestamp":1651068764248,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"fFfJ3dZ6lpey","outputId":"7e09d236-c6d0-445b-9360-b03f2fb1ee96"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-27T11:10:28 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yaml\n","\u001b[32m2022-04-27T11:10:28 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n","\u001b[32m2022-04-27T11:10:28 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-27T11:10:28 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-27T11:10:28 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-27T11:10:28 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yaml', 'training.batch_size=32', 'model=visual_bert', 'dataset=hateful_memes'])\n","\u001b[32m2022-04-27T11:10:28 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-27T11:10:28 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-04-27T11:10:28 | mmf_cli.run: \u001b[0mUsing seed 28321162\n","\u001b[32m2022-04-27T11:10:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-27T11:10:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-27T11:10:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-27T11:10:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-27T11:10:32 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-27T11:10:38 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-27T11:10:38 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-27T11:10:38 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n","Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:18<00:00, 22.0MB/s]\n","[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n","Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-27T11:11:03 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-27T11:11:03 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-27T11:11:03 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-27T11:11:03 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2022-04-27T11:11:03 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-27T11:11:03 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-27T11:11:03 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-27T11:11:03 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-04-27T11:11:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.5927, train/hateful_memes/cross_entropy/avg: 0.5927, train/total_loss: 0.5927, train/total_loss/avg: 0.5927, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 2.04, time: 49s 860ms, time_since_start: 01m 15s 163ms, eta: 03h 05m 04s 994ms\n","\u001b[32m2022-04-27T11:12:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.4485, train/hateful_memes/cross_entropy/avg: 0.5206, train/total_loss: 0.4485, train/total_loss/avg: 0.5206, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 882ms, time_since_start: 02m 03s 045ms, eta: 02h 56m 55s 747ms\n","\u001b[32m2022-04-27T11:13:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.5927, train/hateful_memes/cross_entropy/avg: 0.5757, train/total_loss: 0.5927, train/total_loss/avg: 0.5757, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 569ms, time_since_start: 02m 50s 615ms, eta: 02h 54m 58s 008ms\n","\u001b[32m2022-04-27T11:14:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.5761, train/hateful_memes/cross_entropy/avg: 0.5758, train/total_loss: 0.5761, train/total_loss/avg: 0.5758, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 364ms, time_since_start: 03m 37s 980ms, eta: 02h 53m 24s 719ms\n","\u001b[32m2022-04-27T11:15:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.5761, train/hateful_memes/cross_entropy/avg: 0.5602, train/total_loss: 0.5761, train/total_loss/avg: 0.5602, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 597ms, time_since_start: 04m 25s 578ms, eta: 02h 53m 27s 541ms\n","\u001b[32m2022-04-27T11:15:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.4976, train/hateful_memes/cross_entropy/avg: 0.5179, train/total_loss: 0.4976, train/total_loss/avg: 0.5179, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 164ms, time_since_start: 05m 12s 742ms, eta: 02h 51m 04s 804ms\n","\u001b[32m2022-04-27T11:16:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.4976, train/hateful_memes/cross_entropy/avg: 0.4887, train/total_loss: 0.4976, train/total_loss/avg: 0.4887, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 433ms, time_since_start: 06m 176ms, eta: 02h 51m 15s 131ms\n","\u001b[32m2022-04-27T11:17:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.4485, train/hateful_memes/cross_entropy/avg: 0.4683, train/total_loss: 0.4485, train/total_loss/avg: 0.4683, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 249ms, time_since_start: 06m 47s 426ms, eta: 02h 49m 47s 225ms\n","\u001b[32m2022-04-27T11:18:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.4485, train/hateful_memes/cross_entropy/avg: 0.4364, train/total_loss: 0.4485, train/total_loss/avg: 0.4364, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 463ms, time_since_start: 07m 34s 890ms, eta: 02h 49m 45s 158ms\n","\u001b[32m2022-04-27T11:19:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T11:19:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T11:19:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T11:19:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T11:19:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.3258, train/hateful_memes/cross_entropy/avg: 0.4118, train/total_loss: 0.3258, train/total_loss/avg: 0.4118, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.79, time: 56s 338ms, time_since_start: 08m 31s 228ms, eta: 03h 20m 32s 201ms\n","\u001b[32m2022-04-27T11:19:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T11:19:10 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T11:19:16 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T11:19:16 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T11:19:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T11:19:22 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-27T11:19:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T11:19:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T11:19:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.7502, val/total_loss: 0.7502, val/hateful_memes/accuracy: 0.7000, val/hateful_memes/binary_f1: 0.5235, val/hateful_memes/roc_auc: 0.7218, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 24s 708ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.721750\n","\u001b[32m2022-04-27T11:20:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.3258, train/hateful_memes/cross_entropy/avg: 0.4019, train/total_loss: 0.3258, train/total_loss/avg: 0.4019, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 047ms, time_since_start: 09m 43s 986ms, eta: 02h 50m 12s 654ms\n","\u001b[32m2022-04-27T11:21:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.3258, train/hateful_memes/cross_entropy/avg: 0.3992, train/total_loss: 0.3258, train/total_loss/avg: 0.3992, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 511ms, time_since_start: 10m 31s 498ms, eta: 02h 47m 30s 415ms\n","\u001b[32m2022-04-27T11:21:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.3258, train/hateful_memes/cross_entropy/avg: 0.3812, train/total_loss: 0.3258, train/total_loss/avg: 0.3812, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 503ms, time_since_start: 11m 19s 002ms, eta: 02h 46m 40s 483ms\n","\u001b[32m2022-04-27T11:22:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.3258, train/hateful_memes/cross_entropy/avg: 0.3836, train/total_loss: 0.3258, train/total_loss/avg: 0.3836, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 278ms, time_since_start: 12m 06s 280ms, eta: 02h 45m 04s 940ms\n","\u001b[32m2022-04-27T11:23:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.3258, train/hateful_memes/cross_entropy/avg: 0.3732, train/total_loss: 0.3258, train/total_loss/avg: 0.3732, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 404ms, time_since_start: 12m 53s 685ms, eta: 02h 44m 43s 181ms\n","\u001b[32m2022-04-27T11:24:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.3130, train/hateful_memes/cross_entropy/avg: 0.3576, train/total_loss: 0.3130, train/total_loss/avg: 0.3576, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 067ms, time_since_start: 13m 40s 753ms, eta: 02h 42m 45s 040ms\n","\u001b[32m2022-04-27T11:25:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.3130, train/hateful_memes/cross_entropy/avg: 0.3383, train/total_loss: 0.3130, train/total_loss/avg: 0.3383, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 439ms, time_since_start: 14m 28s 192ms, eta: 02h 43m 13s 909ms\n","\u001b[32m2022-04-27T11:25:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.3068, train/hateful_memes/cross_entropy/avg: 0.3247, train/total_loss: 0.3068, train/total_loss/avg: 0.3247, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 451ms, time_since_start: 15m 15s 643ms, eta: 02h 42m 28s 111ms\n","\u001b[32m2022-04-27T11:26:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.3068, train/hateful_memes/cross_entropy/avg: 0.3116, train/total_loss: 0.3068, train/total_loss/avg: 0.3116, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 398ms, time_since_start: 16m 03s 042ms, eta: 02h 41m 29s 088ms\n","\u001b[32m2022-04-27T11:27:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T11:27:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T11:27:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T11:27:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T11:27:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.3033, train/hateful_memes/cross_entropy/avg: 0.2987, train/total_loss: 0.3033, train/total_loss/avg: 0.2987, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.72, time: 58s 690ms, time_since_start: 17m 01s 733ms, eta: 03h 18m 57s 725ms\n","\u001b[32m2022-04-27T11:27:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T11:27:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T11:27:43 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T11:27:43 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T11:27:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T11:27:48 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-27T11:27:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T11:28:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T11:28:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.1066, val/total_loss: 1.1066, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.4970, val/hateful_memes/roc_auc: 0.7246, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 20s 848ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T11:28:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.2277, train/hateful_memes/cross_entropy/avg: 0.2903, train/total_loss: 0.2277, train/total_loss/avg: 0.2903, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 236ms, time_since_start: 18m 10s 819ms, eta: 02h 42m 42s 230ms\n","\u001b[32m2022-04-27T11:29:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.1909, train/hateful_memes/cross_entropy/avg: 0.2776, train/total_loss: 0.1909, train/total_loss/avg: 0.2776, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 255ms, time_since_start: 18m 58s 075ms, eta: 02h 38m 35s 717ms\n","\u001b[32m2022-04-27T11:30:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.1809, train/hateful_memes/cross_entropy/avg: 0.2721, train/total_loss: 0.1809, train/total_loss/avg: 0.2721, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 532ms, time_since_start: 19m 45s 608ms, eta: 02h 38m 43s 130ms\n","\u001b[32m2022-04-27T11:31:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.1659, train/hateful_memes/cross_entropy/avg: 0.2641, train/total_loss: 0.1659, train/total_loss/avg: 0.2641, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 440ms, time_since_start: 20m 33s 048ms, eta: 02h 37m 36s 404ms\n","\u001b[32m2022-04-27T11:31:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.1502, train/hateful_memes/cross_entropy/avg: 0.2569, train/total_loss: 0.1502, train/total_loss/avg: 0.2569, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 603ms, time_since_start: 21m 20s 652ms, eta: 02h 37m 20s 500ms\n","\u001b[32m2022-04-27T11:32:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.1502, train/hateful_memes/cross_entropy/avg: 0.2564, train/total_loss: 0.1502, train/total_loss/avg: 0.2564, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 462ms, time_since_start: 22m 08s 114ms, eta: 02h 36m 04s 177ms\n","\u001b[32m2022-04-27T11:33:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.1236, train/hateful_memes/cross_entropy/avg: 0.2481, train/total_loss: 0.1236, train/total_loss/avg: 0.2481, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 043ms, time_since_start: 22m 55s 158ms, eta: 02h 33m 53s 743ms\n","\u001b[32m2022-04-27T11:34:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.1228, train/hateful_memes/cross_entropy/avg: 0.2401, train/total_loss: 0.1228, train/total_loss/avg: 0.2401, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 378ms, time_since_start: 23m 42s 537ms, eta: 02h 34m 11s 404ms\n","\u001b[32m2022-04-27T11:35:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.1228, train/hateful_memes/cross_entropy/avg: 0.2363, train/total_loss: 0.1228, train/total_loss/avg: 0.2363, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 525ms, time_since_start: 24m 30s 062ms, eta: 02h 33m 51s 743ms\n","\u001b[32m2022-04-27T11:35:55 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T11:35:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T11:35:59 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T11:36:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T11:36:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.0940, train/hateful_memes/cross_entropy/avg: 0.2288, train/total_loss: 0.0940, train/total_loss/avg: 0.2288, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.82, time: 55s 241ms, time_since_start: 25m 25s 304ms, eta: 02h 57m 54s 324ms\n","\u001b[32m2022-04-27T11:36:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T11:36:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T11:36:06 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T11:36:06 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T11:36:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T11:36:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T11:36:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T11:36:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.4743, val/total_loss: 1.4743, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.4315, val/hateful_memes/roc_auc: 0.7059, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 11s 601ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T11:37:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.0848, train/hateful_memes/cross_entropy/avg: 0.2237, train/total_loss: 0.0848, train/total_loss/avg: 0.2237, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 819ms, time_since_start: 26m 24s 727ms, eta: 02h 33m 11s 484ms\n","\u001b[32m2022-04-27T11:37:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.0795, train/hateful_memes/cross_entropy/avg: 0.2169, train/total_loss: 0.0795, train/total_loss/avg: 0.2169, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 175ms, time_since_start: 27m 11s 902ms, eta: 02h 30m 19s 758ms\n","\u001b[32m2022-04-27T11:38:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0753, train/hateful_memes/cross_entropy/avg: 0.2105, train/total_loss: 0.0753, train/total_loss/avg: 0.2105, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 583ms, time_since_start: 27m 59s 486ms, eta: 02h 30m 49s 419ms\n","\u001b[32m2022-04-27T11:39:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0715, train/hateful_memes/cross_entropy/avg: 0.2044, train/total_loss: 0.0715, train/total_loss/avg: 0.2044, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 310ms, time_since_start: 28m 46s 797ms, eta: 02h 29m 09s 440ms\n","\u001b[32m2022-04-27T11:40:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0533, train/hateful_memes/cross_entropy/avg: 0.1993, train/total_loss: 0.0533, train/total_loss/avg: 0.1993, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 218ms, time_since_start: 29m 34s 016ms, eta: 02h 28m 04s 014ms\n","\u001b[32m2022-04-27T11:41:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0533, train/hateful_memes/cross_entropy/avg: 0.1955, train/total_loss: 0.0533, train/total_loss/avg: 0.1955, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 839ms, time_since_start: 30m 21s 855ms, eta: 02h 29m 12s 095ms\n","\u001b[32m2022-04-27T11:41:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0631, train/hateful_memes/cross_entropy/avg: 0.1962, train/total_loss: 0.0631, train/total_loss/avg: 0.1962, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 946ms, time_since_start: 31m 09s 802ms, eta: 02h 28m 43s 364ms\n","\u001b[32m2022-04-27T11:42:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0533, train/hateful_memes/cross_entropy/avg: 0.1912, train/total_loss: 0.0533, train/total_loss/avg: 0.1912, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 657ms, time_since_start: 31m 57s 459ms, eta: 02h 27m 01s 080ms\n","\u001b[32m2022-04-27T11:43:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0316, train/hateful_memes/cross_entropy/avg: 0.1864, train/total_loss: 0.0316, train/total_loss/avg: 0.1864, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 073ms, time_since_start: 32m 45s 532ms, eta: 02h 27m 29s 222ms\n","\u001b[32m2022-04-27T11:44:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T11:44:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T11:44:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T11:44:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T11:44:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0262, train/hateful_memes/cross_entropy/avg: 0.1817, train/total_loss: 0.0262, train/total_loss/avg: 0.1817, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.82, time: 55s 521ms, time_since_start: 33m 41s 054ms, eta: 02h 49m 23s 733ms\n","\u001b[32m2022-04-27T11:44:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T11:44:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T11:44:22 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T11:44:22 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T11:44:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T11:44:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T11:44:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T11:44:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 2.1434, val/total_loss: 2.1434, val/hateful_memes/accuracy: 0.6648, val/hateful_memes/binary_f1: 0.3693, val/hateful_memes/roc_auc: 0.6901, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 14s 485ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T11:45:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0262, train/hateful_memes/cross_entropy/avg: 0.1787, train/total_loss: 0.0262, train/total_loss/avg: 0.1787, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 996ms, time_since_start: 34m 44s 537ms, eta: 02h 28m 39s 408ms\n","\u001b[32m2022-04-27T11:46:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0262, train/hateful_memes/cross_entropy/avg: 0.1746, train/total_loss: 0.0262, train/total_loss/avg: 0.1746, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 557ms, time_since_start: 35m 33s 094ms, eta: 02h 26m 30s 114ms\n","\u001b[32m2022-04-27T11:46:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0262, train/hateful_memes/cross_entropy/avg: 0.1732, train/total_loss: 0.0262, train/total_loss/avg: 0.1732, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 957ms, time_since_start: 36m 21s 052ms, eta: 02h 23m 52s 863ms\n","\u001b[32m2022-04-27T11:47:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0234, train/hateful_memes/cross_entropy/avg: 0.1693, train/total_loss: 0.0234, train/total_loss/avg: 0.1693, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 198ms, time_since_start: 37m 09s 251ms, eta: 02h 23m 47s 089ms\n","\u001b[32m2022-04-27T11:48:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0095, train/hateful_memes/cross_entropy/avg: 0.1656, train/total_loss: 0.0095, train/total_loss/avg: 0.1656, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 554ms, time_since_start: 37m 57s 806ms, eta: 02h 24m 01s 578ms\n","\u001b[32m2022-04-27T11:49:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0095, train/hateful_memes/cross_entropy/avg: 0.1622, train/total_loss: 0.0095, train/total_loss/avg: 0.1622, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 143ms, time_since_start: 38m 45s 949ms, eta: 02h 21m 59s 329ms\n","\u001b[32m2022-04-27T11:50:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0095, train/hateful_memes/cross_entropy/avg: 0.1590, train/total_loss: 0.0095, train/total_loss/avg: 0.1590, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 092ms, time_since_start: 39m 34s 042ms, eta: 02h 21m 01s 471ms\n","\u001b[32m2022-04-27T11:51:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0077, train/hateful_memes/cross_entropy/avg: 0.1557, train/total_loss: 0.0077, train/total_loss/avg: 0.1557, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 807ms, time_since_start: 40m 21s 849ms, eta: 02h 19m 22s 662ms\n","\u001b[32m2022-04-27T11:51:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0074, train/hateful_memes/cross_entropy/avg: 0.1525, train/total_loss: 0.0074, train/total_loss/avg: 0.1525, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 200ms, time_since_start: 41m 10s 050ms, eta: 02h 19m 42s 478ms\n","\u001b[32m2022-04-27T11:52:36 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T11:52:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T11:52:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T11:52:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T11:52:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0074, train/hateful_memes/cross_entropy/avg: 0.1535, train/total_loss: 0.0074, train/total_loss/avg: 0.1535, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.69, time: 59s 038ms, time_since_start: 42m 09s 089ms, eta: 02h 50m 07s 195ms\n","\u001b[32m2022-04-27T11:52:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T11:52:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T11:52:50 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T11:52:50 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T11:52:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T11:52:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T11:53:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T11:53:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.5204, val/total_loss: 1.5204, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.3871, val/hateful_memes/roc_auc: 0.7000, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 12s 849ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T11:53:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1505, train/total_loss: 0.0062, train/total_loss/avg: 0.1505, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 921ms, time_since_start: 43m 10s 860ms, eta: 02h 20m 08s 264ms\n","\u001b[32m2022-04-27T11:54:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1479, train/total_loss: 0.0062, train/total_loss/avg: 0.1479, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 202ms, time_since_start: 43m 59s 063ms, eta: 02h 17m 15s 755ms\n","\u001b[32m2022-04-27T11:55:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0064, train/hateful_memes/cross_entropy/avg: 0.1453, train/total_loss: 0.0064, train/total_loss/avg: 0.1453, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 214ms, time_since_start: 44m 47s 278ms, eta: 02h 16m 28s 733ms\n","\u001b[32m2022-04-27T11:56:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0064, train/hateful_memes/cross_entropy/avg: 0.1426, train/total_loss: 0.0064, train/total_loss/avg: 0.1426, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 759ms, time_since_start: 45m 35s 038ms, eta: 02h 14m 22s 908ms\n","\u001b[32m2022-04-27T11:57:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1401, train/total_loss: 0.0062, train/total_loss/avg: 0.1401, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 302ms, time_since_start: 46m 23s 340ms, eta: 02h 15m 05s 378ms\n","\u001b[32m2022-04-27T11:57:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1382, train/total_loss: 0.0062, train/total_loss/avg: 0.1382, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 139ms, time_since_start: 47m 11s 480ms, eta: 02h 13m 49s 098ms\n","\u001b[32m2022-04-27T11:58:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0054, train/hateful_memes/cross_entropy/avg: 0.1359, train/total_loss: 0.0054, train/total_loss/avg: 0.1359, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 029ms, time_since_start: 47m 59s 510ms, eta: 02h 12m 41s 926ms\n","\u001b[32m2022-04-27T11:59:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1336, train/total_loss: 0.0033, train/total_loss/avg: 0.1336, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 037ms, time_since_start: 48m 47s 547ms, eta: 02h 11m 54s 355ms\n","\u001b[32m2022-04-27T12:00:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1313, train/total_loss: 0.0033, train/total_loss/avg: 0.1313, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 575ms, time_since_start: 49m 35s 122ms, eta: 02h 09m 49s 791ms\n","\u001b[32m2022-04-27T12:01:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T12:01:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T12:01:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T12:01:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T12:01:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0054, train/hateful_memes/cross_entropy/avg: 0.1292, train/total_loss: 0.0054, train/total_loss/avg: 0.1292, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.72, time: 58s 495ms, time_since_start: 50m 33s 617ms, eta: 02h 38m 38s 343ms\n","\u001b[32m2022-04-27T12:01:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T12:01:12 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T12:01:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T12:01:15 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T12:01:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T12:01:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T12:01:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T12:01:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 1.2879, val/total_loss: 1.2879, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.5585, val/hateful_memes/roc_auc: 0.7015, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 16s 557ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T12:02:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1272, train/total_loss: 0.0033, train/total_loss/avg: 0.1272, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 690ms, time_since_start: 51m 38s 867ms, eta: 02h 11m 13s 422ms\n","\u001b[32m2022-04-27T12:03:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1257, train/total_loss: 0.0033, train/total_loss/avg: 0.1257, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 837ms, time_since_start: 52m 26s 704ms, eta: 02h 08m 06s 782ms\n","\u001b[32m2022-04-27T12:03:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.1238, train/total_loss: 0.0026, train/total_loss/avg: 0.1238, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 645ms, time_since_start: 53m 14s 350ms, eta: 02h 06m 47s 575ms\n","\u001b[32m2022-04-27T12:04:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1237, train/total_loss: 0.0033, train/total_loss/avg: 0.1237, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 133ms, time_since_start: 54m 01s 483ms, eta: 02h 04m 37s 787ms\n","\u001b[32m2022-04-27T12:05:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0040, train/hateful_memes/cross_entropy/avg: 0.1219, train/total_loss: 0.0040, train/total_loss/avg: 0.1219, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 589ms, time_since_start: 54m 49s 073ms, eta: 02h 05m 01s 794ms\n","\u001b[32m2022-04-27T12:06:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1201, train/total_loss: 0.0033, train/total_loss/avg: 0.1201, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 582ms, time_since_start: 55m 36s 655ms, eta: 02h 04m 12s 224ms\n","\u001b[32m2022-04-27T12:07:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1184, train/total_loss: 0.0033, train/total_loss/avg: 0.1184, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 412ms, time_since_start: 56m 24s 068ms, eta: 02h 02m 57s 511ms\n","\u001b[32m2022-04-27T12:07:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1166, train/total_loss: 0.0033, train/total_loss/avg: 0.1166, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 931ms, time_since_start: 57m 12s 000ms, eta: 02h 03m 29s 512ms\n","\u001b[32m2022-04-27T12:08:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0040, train/hateful_memes/cross_entropy/avg: 0.1150, train/total_loss: 0.0040, train/total_loss/avg: 0.1150, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 575ms, time_since_start: 57m 59s 576ms, eta: 02h 01m 46s 103ms\n","\u001b[32m2022-04-27T12:09:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T12:09:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T12:09:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T12:09:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T12:09:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1134, train/total_loss: 0.0033, train/total_loss/avg: 0.1134, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.85, time: 54s 345ms, time_since_start: 58m 53s 921ms, eta: 02h 18m 10s 352ms\n","\u001b[32m2022-04-27T12:09:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T12:09:32 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T12:09:35 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T12:09:35 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T12:09:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 1.6661, val/total_loss: 1.6661, val/hateful_memes/accuracy: 0.7167, val/hateful_memes/binary_f1: 0.5016, val/hateful_memes/roc_auc: 0.6973, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 03s 164ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T12:10:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1118, train/total_loss: 0.0033, train/total_loss/avg: 0.1118, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 255ms, time_since_start: 59m 45s 342ms, eta: 02h 01m 52s 249ms\n","\u001b[32m2022-04-27T12:11:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1103, train/total_loss: 0.0028, train/total_loss/avg: 0.1103, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 324ms, time_since_start: 01h 32s 666ms, eta: 01h 58m 43s 080ms\n","\u001b[32m2022-04-27T12:11:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1093, train/total_loss: 0.0028, train/total_loss/avg: 0.1093, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 686ms, time_since_start: 01h 01m 20s 353ms, eta: 01h 58m 49s 038ms\n","\u001b[32m2022-04-27T12:12:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1079, train/total_loss: 0.0033, train/total_loss/avg: 0.1079, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 727ms, time_since_start: 01h 02m 08s 080ms, eta: 01h 58m 06s 695ms\n","\u001b[32m2022-04-27T12:13:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1065, train/total_loss: 0.0028, train/total_loss/avg: 0.1065, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 330ms, time_since_start: 01h 02m 55s 411ms, eta: 01h 56m 19s 562ms\n","\u001b[32m2022-04-27T12:14:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1052, train/total_loss: 0.0028, train/total_loss/avg: 0.1052, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 645ms, time_since_start: 01h 03m 43s 056ms, eta: 01h 56m 17s 552ms\n","\u001b[32m2022-04-27T12:15:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.1038, train/total_loss: 0.0026, train/total_loss/avg: 0.1038, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 499ms, time_since_start: 01h 04m 30s 555ms, eta: 01h 55m 07s 908ms\n","\u001b[32m2022-04-27T12:15:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1025, train/total_loss: 0.0018, train/total_loss/avg: 0.1025, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 246ms, time_since_start: 01h 05m 17s 802ms, eta: 01h 53m 43s 030ms\n","\u001b[32m2022-04-27T12:16:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1012, train/total_loss: 0.0018, train/total_loss/avg: 0.1012, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 284ms, time_since_start: 01h 06m 05s 086ms, eta: 01h 53m 418ms\n","\u001b[32m2022-04-27T12:17:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T12:17:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T12:17:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T12:17:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T12:17:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.0999, train/total_loss: 0.0016, train/total_loss/avg: 0.0999, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.85, time: 54s 865ms, time_since_start: 01h 06m 59s 952ms, eta: 02h 10m 11s 775ms\n","\u001b[32m2022-04-27T12:17:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T12:17:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T12:17:41 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T12:17:41 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T12:17:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.2295, val/total_loss: 2.2295, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4615, val/hateful_memes/roc_auc: 0.6914, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 04s 741ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T12:18:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.0987, train/total_loss: 0.0018, train/total_loss/avg: 0.0987, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 224ms, time_since_start: 01h 07m 52s 920ms, eta: 01h 53m 37s 164ms\n","\u001b[32m2022-04-27T12:19:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0975, train/total_loss: 0.0013, train/total_loss/avg: 0.0975, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 519ms, time_since_start: 01h 08m 40s 439ms, eta: 01h 51m 09s 209ms\n","\u001b[32m2022-04-27T12:20:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0964, train/total_loss: 0.0013, train/total_loss/avg: 0.0964, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 312ms, time_since_start: 01h 09m 27s 752ms, eta: 01h 49m 52s 052ms\n","\u001b[32m2022-04-27T12:20:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0954, train/total_loss: 0.0013, train/total_loss/avg: 0.0954, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 645ms, time_since_start: 01h 10m 15s 398ms, eta: 01h 49m 49s 918ms\n","\u001b[32m2022-04-27T12:21:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0943, train/total_loss: 0.0013, train/total_loss/avg: 0.0943, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 642ms, time_since_start: 01h 11m 03s 040ms, eta: 01h 49m 01s 115ms\n","\u001b[32m2022-04-27T12:22:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0932, train/total_loss: 0.0007, train/total_loss/avg: 0.0932, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 360ms, time_since_start: 01h 11m 50s 401ms, eta: 01h 47m 34s 230ms\n","\u001b[32m2022-04-27T12:23:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0921, train/total_loss: 0.0007, train/total_loss/avg: 0.0921, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 513ms, time_since_start: 01h 12m 37s 914ms, eta: 01h 47m 06s 702ms\n","\u001b[32m2022-04-27T12:24:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0911, train/total_loss: 0.0007, train/total_loss/avg: 0.0911, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 227ms, time_since_start: 01h 13m 25s 142ms, eta: 01h 45m 40s 043ms\n","\u001b[32m2022-04-27T12:24:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0901, train/total_loss: 0.0006, train/total_loss/avg: 0.0901, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 565ms, time_since_start: 01h 14m 12s 708ms, eta: 01h 45m 37s 034ms\n","\u001b[32m2022-04-27T12:25:38 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T12:25:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T12:25:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T12:25:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T12:25:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0891, train/total_loss: 0.0007, train/total_loss/avg: 0.0891, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 1.79, time: 56s 672ms, time_since_start: 01h 15m 09s 380ms, eta: 02h 04m 52s 639ms\n","\u001b[32m2022-04-27T12:25:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T12:25:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T12:25:50 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T12:25:50 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T12:25:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 1.5957, val/total_loss: 1.5957, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.5015, val/hateful_memes/roc_auc: 0.7049, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 22000, val_time: 03s 219ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T12:26:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0899, train/total_loss: 0.0010, train/total_loss/avg: 0.0899, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 981ms, time_since_start: 01h 16m 582ms, eta: 01h 44m 54s 805ms\n","\u001b[32m2022-04-27T12:27:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.0890, train/total_loss: 0.0011, train/total_loss/avg: 0.0890, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 732ms, time_since_start: 01h 16m 48s 315ms, eta: 01h 43m 33s 590ms\n","\u001b[32m2022-04-27T12:28:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0880, train/total_loss: 0.0010, train/total_loss/avg: 0.0880, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 160ms, time_since_start: 01h 17m 36s 475ms, eta: 01h 43m 40s 378ms\n","\u001b[32m2022-04-27T12:29:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0871, train/total_loss: 0.0007, train/total_loss/avg: 0.0871, max mem: 9224.0, experiment: run, epoch: 36, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 802ms, time_since_start: 01h 18m 24s 277ms, eta: 01h 42m 05s 450ms\n","\u001b[32m2022-04-27T12:29:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0862, train/total_loss: 0.0007, train/total_loss/avg: 0.0862, max mem: 9224.0, experiment: run, epoch: 36, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 803ms, time_since_start: 01h 19m 12s 081ms, eta: 01h 41m 17s 070ms\n","\u001b[32m2022-04-27T12:30:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0853, train/total_loss: 0.0006, train/total_loss/avg: 0.0853, max mem: 9224.0, experiment: run, epoch: 37, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 243ms, time_since_start: 01h 19m 59s 325ms, eta: 01h 39m 17s 838ms\n","\u001b[32m2022-04-27T12:31:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0844, train/total_loss: 0.0006, train/total_loss/avg: 0.0844, max mem: 9224.0, experiment: run, epoch: 37, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 724ms, time_since_start: 01h 20m 47s 050ms, eta: 01h 39m 29s 883ms\n","\u001b[32m2022-04-27T12:32:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0835, train/total_loss: 0.0006, train/total_loss/avg: 0.0835, max mem: 9224.0, experiment: run, epoch: 37, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 723ms, time_since_start: 01h 21m 34s 773ms, eta: 01h 38m 41s 238ms\n","\u001b[32m2022-04-27T12:33:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0827, train/total_loss: 0.0006, train/total_loss/avg: 0.0827, max mem: 9224.0, experiment: run, epoch: 38, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 332ms, time_since_start: 01h 22m 22s 105ms, eta: 01h 37m 04s 578ms\n","\u001b[32m2022-04-27T12:33:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T12:33:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T12:33:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T12:33:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T12:33:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0820, train/total_loss: 0.0009, train/total_loss/avg: 0.0820, max mem: 9224.0, experiment: run, epoch: 38, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 1.79, time: 56s 985ms, time_since_start: 01h 23m 19s 091ms, eta: 01h 55m 54s 499ms\n","\u001b[32m2022-04-27T12:33:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T12:33:57 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T12:34:01 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T12:34:01 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T12:34:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.0393, val/total_loss: 2.0393, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.4407, val/hateful_memes/roc_auc: 0.6907, num_updates: 10000, epoch: 38, iterations: 10000, max_updates: 22000, val_time: 03s 851ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T12:34:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0812, train/total_loss: 0.0007, train/total_loss/avg: 0.0812, max mem: 9224.0, experiment: run, epoch: 38, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 690ms, time_since_start: 01h 24m 11s 635ms, eta: 01h 38m 12s 695ms\n","\u001b[32m2022-04-27T12:35:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0805, train/total_loss: 0.0007, train/total_loss/avg: 0.0805, max mem: 9224.0, experiment: run, epoch: 39, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 375ms, time_since_start: 01h 24m 59s 011ms, eta: 01h 34m 45s 395ms\n","\u001b[32m2022-04-27T12:36:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0799, train/total_loss: 0.0007, train/total_loss/avg: 0.0799, max mem: 9224.0, experiment: run, epoch: 39, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 736ms, time_since_start: 01h 25m 46s 747ms, eta: 01h 34m 40s 084ms\n","\u001b[32m2022-04-27T12:37:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0792, train/total_loss: 0.0006, train/total_loss/avg: 0.0792, max mem: 9224.0, experiment: run, epoch: 40, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 560ms, time_since_start: 01h 26m 34s 307ms, eta: 01h 33m 30s 773ms\n","\u001b[32m2022-04-27T12:38:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0784, train/total_loss: 0.0006, train/total_loss/avg: 0.0784, max mem: 9224.0, experiment: run, epoch: 40, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 582ms, time_since_start: 01h 27m 21s 890ms, eta: 01h 32m 44s 999ms\n","\u001b[32m2022-04-27T12:38:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0777, train/total_loss: 0.0006, train/total_loss/avg: 0.0777, max mem: 9224.0, experiment: run, epoch: 40, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 739ms, time_since_start: 01h 28m 09s 629ms, eta: 01h 32m 14s 839ms\n","\u001b[32m2022-04-27T12:39:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0770, train/total_loss: 0.0006, train/total_loss/avg: 0.0770, max mem: 9224.0, experiment: run, epoch: 41, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 421ms, time_since_start: 01h 28m 57s 051ms, eta: 01h 30m 49s 736ms\n","\u001b[32m2022-04-27T12:40:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0763, train/total_loss: 0.0006, train/total_loss/avg: 0.0763, max mem: 9224.0, experiment: run, epoch: 41, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 673ms, time_since_start: 01h 29m 44s 724ms, eta: 01h 30m 30s 180ms\n","\u001b[32m2022-04-27T12:41:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0756, train/total_loss: 0.0006, train/total_loss/avg: 0.0756, max mem: 9224.0, experiment: run, epoch: 41, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 060ms, time_since_start: 01h 30m 32s 785ms, eta: 01h 30m 25s 403ms\n","\u001b[32m2022-04-27T12:41:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T12:41:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T12:42:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T12:42:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T12:42:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0749, train/total_loss: 0.0006, train/total_loss/avg: 0.0749, max mem: 9224.0, experiment: run, epoch: 42, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 1.75, time: 57s 502ms, time_since_start: 01h 31m 30s 288ms, eta: 01h 47m 12s 861ms\n","\u001b[32m2022-04-27T12:42:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T12:42:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T12:42:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T12:42:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T12:42:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.3337, val/total_loss: 2.3337, val/hateful_memes/accuracy: 0.6667, val/hateful_memes/binary_f1: 0.4915, val/hateful_memes/roc_auc: 0.6978, num_updates: 11000, epoch: 42, iterations: 11000, max_updates: 22000, val_time: 03s 909ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T12:43:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0742, train/total_loss: 0.0005, train/total_loss/avg: 0.0742, max mem: 9224.0, experiment: run, epoch: 42, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 508ms, time_since_start: 01h 32m 22s 707ms, eta: 01h 29m 37s 264ms\n","\u001b[32m2022-04-27T12:43:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0736, train/total_loss: 0.0005, train/total_loss/avg: 0.0736, max mem: 9224.0, experiment: run, epoch: 43, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 236ms, time_since_start: 01h 33m 09s 944ms, eta: 01h 26m 28s 285ms\n","\u001b[32m2022-04-27T12:44:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0729, train/total_loss: 0.0005, train/total_loss/avg: 0.0729, max mem: 9224.0, experiment: run, epoch: 43, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 571ms, time_since_start: 01h 33m 57s 515ms, eta: 01h 26m 16s 647ms\n","\u001b[32m2022-04-27T12:45:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0723, train/total_loss: 0.0005, train/total_loss/avg: 0.0723, max mem: 9224.0, experiment: run, epoch: 43, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 552ms, time_since_start: 01h 34m 45s 068ms, eta: 01h 25m 26s 297ms\n","\u001b[32m2022-04-27T12:46:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0717, train/total_loss: 0.0005, train/total_loss/avg: 0.0717, max mem: 9224.0, experiment: run, epoch: 44, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 350ms, time_since_start: 01h 35m 32s 419ms, eta: 01h 24m 16s 377ms\n","\u001b[32m2022-04-27T12:46:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0711, train/total_loss: 0.0004, train/total_loss/avg: 0.0711, max mem: 9224.0, experiment: run, epoch: 44, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 606ms, time_since_start: 01h 36m 20s 025ms, eta: 01h 23m 55s 230ms\n","\u001b[32m2022-04-27T12:47:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0705, train/total_loss: 0.0007, train/total_loss/avg: 0.0705, max mem: 9224.0, experiment: run, epoch: 44, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 528ms, time_since_start: 01h 37m 07s 554ms, eta: 01h 22m 58s 676ms\n","\u001b[32m2022-04-27T12:48:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0699, train/total_loss: 0.0007, train/total_loss/avg: 0.0699, max mem: 9224.0, experiment: run, epoch: 45, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 340ms, time_since_start: 01h 37m 54s 895ms, eta: 01h 21m 50s 870ms\n","\u001b[32m2022-04-27T12:49:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0693, train/total_loss: 0.0005, train/total_loss/avg: 0.0693, max mem: 9224.0, experiment: run, epoch: 45, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 707ms, time_since_start: 01h 38m 42s 603ms, eta: 01h 21m 40s 392ms\n","\u001b[32m2022-04-27T12:50:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T12:50:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T12:50:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T12:50:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T12:50:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0687, train/total_loss: 0.0005, train/total_loss/avg: 0.0687, max mem: 9224.0, experiment: run, epoch: 46, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 1.67, time: 01m 846ms, time_since_start: 01h 39m 43s 449ms, eta: 01h 43m 08s 081ms\n","\u001b[32m2022-04-27T12:50:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T12:50:22 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T12:50:27 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T12:50:27 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T12:50:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.0284, val/total_loss: 2.0284, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.4985, val/hateful_memes/roc_auc: 0.6751, num_updates: 12000, epoch: 46, iterations: 12000, max_updates: 22000, val_time: 05s 710ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T12:51:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0682, train/total_loss: 0.0005, train/total_loss/avg: 0.0682, max mem: 9224.0, experiment: run, epoch: 46, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 570ms, time_since_start: 01h 40m 37s 732ms, eta: 01h 21m 30s 236ms\n","\u001b[32m2022-04-27T12:52:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0676, train/total_loss: 0.0004, train/total_loss/avg: 0.0676, max mem: 9224.0, experiment: run, epoch: 46, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 708ms, time_since_start: 01h 41m 25s 440ms, eta: 01h 19m 14s 912ms\n","\u001b[32m2022-04-27T12:52:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0671, train/total_loss: 0.0004, train/total_loss/avg: 0.0671, max mem: 9224.0, experiment: run, epoch: 47, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 414ms, time_since_start: 01h 42m 12s 855ms, eta: 01h 17m 57s 399ms\n","\u001b[32m2022-04-27T12:53:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0665, train/total_loss: 0.0004, train/total_loss/avg: 0.0665, max mem: 9224.0, experiment: run, epoch: 47, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 739ms, time_since_start: 01h 43m 595ms, eta: 01h 17m 40s 946ms\n","\u001b[32m2022-04-27T12:54:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0660, train/total_loss: 0.0004, train/total_loss/avg: 0.0660, max mem: 9224.0, experiment: run, epoch: 47, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 660ms, time_since_start: 01h 43m 48s 255ms, eta: 01h 16m 44s 723ms\n","\u001b[32m2022-04-27T12:55:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0655, train/total_loss: 0.0004, train/total_loss/avg: 0.0655, max mem: 9224.0, experiment: run, epoch: 48, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 802ms, time_since_start: 01h 44m 36s 058ms, eta: 01h 16m 09s 827ms\n","\u001b[32m2022-04-27T12:56:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0650, train/total_loss: 0.0003, train/total_loss/avg: 0.0650, max mem: 9224.0, experiment: run, epoch: 48, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 671ms, time_since_start: 01h 45m 23s 730ms, eta: 01h 15m 08s 828ms\n","\u001b[32m2022-04-27T12:56:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0645, train/total_loss: 0.0003, train/total_loss/avg: 0.0645, max mem: 9224.0, experiment: run, epoch: 49, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 199ms, time_since_start: 01h 46m 10s 929ms, eta: 01h 13m 36s 137ms\n","\u001b[32m2022-04-27T12:57:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0640, train/total_loss: 0.0003, train/total_loss/avg: 0.0640, max mem: 9224.0, experiment: run, epoch: 49, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 655ms, time_since_start: 01h 46m 58s 584ms, eta: 01h 13m 30s 344ms\n","\u001b[32m2022-04-27T12:58:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T12:58:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T12:58:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T12:58:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T12:58:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0635, train/total_loss: 0.0002, train/total_loss/avg: 0.0635, max mem: 9224.0, experiment: run, epoch: 49, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 1.79, time: 56s 661ms, time_since_start: 01h 47m 55s 246ms, eta: 01h 26m 26s 242ms\n","\u001b[32m2022-04-27T12:58:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T12:58:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T12:58:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T12:58:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T12:58:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.1004, val/total_loss: 2.1004, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.5469, val/hateful_memes/roc_auc: 0.6964, num_updates: 13000, epoch: 49, iterations: 13000, max_updates: 22000, val_time: 03s 467ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T12:59:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0630, train/total_loss: 0.0002, train/total_loss/avg: 0.0630, max mem: 9224.0, experiment: run, epoch: 50, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 350ms, time_since_start: 01h 48m 47s 065ms, eta: 01h 12m 56s 339ms\n","\u001b[32m2022-04-27T13:00:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0625, train/total_loss: 0.0002, train/total_loss/avg: 0.0625, max mem: 9224.0, experiment: run, epoch: 50, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 714ms, time_since_start: 01h 49m 34s 780ms, eta: 01h 11m 10s 249ms\n","\u001b[32m2022-04-27T13:01:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0623, train/total_loss: 0.0002, train/total_loss/avg: 0.0623, max mem: 9224.0, experiment: run, epoch: 50, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 225ms, time_since_start: 01h 50m 22s 006ms, eta: 01h 09m 38s 492ms\n","\u001b[32m2022-04-27T13:01:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0619, train/total_loss: 0.0002, train/total_loss/avg: 0.0619, max mem: 9224.0, experiment: run, epoch: 51, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 648ms, time_since_start: 01h 51m 09s 655ms, eta: 01h 09m 27s 464ms\n","\u001b[32m2022-04-27T13:02:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0614, train/total_loss: 0.0002, train/total_loss/avg: 0.0614, max mem: 9224.0, experiment: run, epoch: 51, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 788ms, time_since_start: 01h 51m 57s 443ms, eta: 01h 08m 51s 045ms\n","\u001b[32m2022-04-27T13:03:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0610, train/total_loss: 0.0002, train/total_loss/avg: 0.0610, max mem: 9224.0, experiment: run, epoch: 52, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 398ms, time_since_start: 01h 52m 44s 841ms, eta: 01h 07m 29s 163ms\n","\u001b[32m2022-04-27T13:04:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0605, train/total_loss: 0.0002, train/total_loss/avg: 0.0605, max mem: 9224.0, experiment: run, epoch: 52, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 638ms, time_since_start: 01h 53m 32s 480ms, eta: 01h 07m 01s 234ms\n","\u001b[32m2022-04-27T13:04:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0601, train/total_loss: 0.0002, train/total_loss/avg: 0.0601, max mem: 9224.0, experiment: run, epoch: 52, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 643ms, time_since_start: 01h 54m 20s 124ms, eta: 01h 06m 13s 217ms\n","\u001b[32m2022-04-27T13:05:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0597, train/total_loss: 0.0002, train/total_loss/avg: 0.0597, max mem: 9224.0, experiment: run, epoch: 53, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 360ms, time_since_start: 01h 55m 07s 484ms, eta: 01h 05m 01s 386ms\n","\u001b[32m2022-04-27T13:06:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T13:06:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T13:06:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T13:06:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T13:06:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0592, train/total_loss: 0.0002, train/total_loss/avg: 0.0592, max mem: 9224.0, experiment: run, epoch: 53, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 1.72, time: 58s 100ms, time_since_start: 01h 56m 05s 585ms, eta: 01h 18m 47s 075ms\n","\u001b[32m2022-04-27T13:06:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T13:06:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T13:06:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T13:06:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T13:06:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.0972, val/total_loss: 2.0972, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.5272, val/hateful_memes/roc_auc: 0.6814, num_updates: 14000, epoch: 53, iterations: 14000, max_updates: 22000, val_time: 04s 000ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T13:07:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0588, train/total_loss: 0.0002, train/total_loss/avg: 0.0588, max mem: 9224.0, experiment: run, epoch: 54, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 396ms, time_since_start: 01h 56m 57s 983ms, eta: 01h 04m 48s 298ms\n","\u001b[32m2022-04-27T13:08:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0584, train/total_loss: 0.0002, train/total_loss/avg: 0.0584, max mem: 9224.0, experiment: run, epoch: 54, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 766ms, time_since_start: 01h 57m 45s 749ms, eta: 01h 03m 09s 134ms\n","\u001b[32m2022-04-27T13:09:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0584, train/total_loss: 0.0002, train/total_loss/avg: 0.0584, max mem: 9224.0, experiment: run, epoch: 54, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 582ms, time_since_start: 01h 58m 33s 332ms, eta: 01h 02m 06s 173ms\n","\u001b[32m2022-04-27T13:09:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0580, train/total_loss: 0.0002, train/total_loss/avg: 0.0580, max mem: 9224.0, experiment: run, epoch: 55, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 142ms, time_since_start: 01h 59m 20s 475ms, eta: 01h 43s 731ms\n","\u001b[32m2022-04-27T13:10:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0576, train/total_loss: 0.0002, train/total_loss/avg: 0.0576, max mem: 9224.0, experiment: run, epoch: 55, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 601ms, time_since_start: 02h 08s 076ms, eta: 01h 30s 774ms\n","\u001b[32m2022-04-27T13:11:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0572, train/total_loss: 0.0002, train/total_loss/avg: 0.0572, max mem: 9224.0, experiment: run, epoch: 55, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 949ms, time_since_start: 02h 56s 025ms, eta: 01h 08s 555ms\n","\u001b[32m2022-04-27T13:12:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0568, train/total_loss: 0.0002, train/total_loss/avg: 0.0568, max mem: 9224.0, experiment: run, epoch: 56, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 732ms, time_since_start: 02h 01m 43s 758ms, eta: 59m 03s 742ms\n","\u001b[32m2022-04-27T13:13:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0564, train/total_loss: 0.0002, train/total_loss/avg: 0.0564, max mem: 9224.0, experiment: run, epoch: 56, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 894ms, time_since_start: 02h 02m 31s 652ms, eta: 58m 26s 992ms\n","\u001b[32m2022-04-27T13:13:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0560, train/total_loss: 0.0002, train/total_loss/avg: 0.0560, max mem: 9224.0, experiment: run, epoch: 57, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 307ms, time_since_start: 02h 03m 18s 959ms, eta: 56m 55s 932ms\n","\u001b[32m2022-04-27T13:14:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T13:14:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T13:14:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T13:14:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T13:14:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0556, train/total_loss: 0.0002, train/total_loss/avg: 0.0556, max mem: 9224.0, experiment: run, epoch: 57, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 1.79, time: 56s 774ms, time_since_start: 02h 04m 15s 733ms, eta: 01h 07m 21s 743ms\n","\u001b[32m2022-04-27T13:14:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T13:14:54 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T13:14:57 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T13:14:57 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T13:14:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.5916, val/total_loss: 2.5916, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.5233, val/hateful_memes/roc_auc: 0.6834, num_updates: 15000, epoch: 57, iterations: 15000, max_updates: 22000, val_time: 03s 382ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T13:15:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0553, train/total_loss: 0.0002, train/total_loss/avg: 0.0553, max mem: 9224.0, experiment: run, epoch: 57, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 343ms, time_since_start: 02h 05m 07s 460ms, eta: 56m 32s 375ms\n","\u001b[32m2022-04-27T13:16:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0549, train/total_loss: 0.0002, train/total_loss/avg: 0.0549, max mem: 9224.0, experiment: run, epoch: 58, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 461ms, time_since_start: 02h 05m 54s 921ms, eta: 54m 42s 254ms\n","\u001b[32m2022-04-27T13:17:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0546, train/total_loss: 0.0002, train/total_loss/avg: 0.0546, max mem: 9224.0, experiment: run, epoch: 58, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 692ms, time_since_start: 02h 06m 42s 614ms, eta: 54m 09s 691ms\n","\u001b[32m2022-04-27T13:18:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0543, train/total_loss: 0.0002, train/total_loss/avg: 0.0543, max mem: 9224.0, experiment: run, epoch: 58, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 576ms, time_since_start: 02h 07m 30s 190ms, eta: 53m 13s 433ms\n","\u001b[32m2022-04-27T13:18:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0539, train/total_loss: 0.0001, train/total_loss/avg: 0.0539, max mem: 9224.0, experiment: run, epoch: 59, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 130ms, time_since_start: 02h 08m 17s 321ms, eta: 51m 55s 562ms\n","\u001b[32m2022-04-27T13:19:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0536, train/total_loss: 0.0001, train/total_loss/avg: 0.0536, max mem: 9224.0, experiment: run, epoch: 59, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 519ms, time_since_start: 02h 09m 04s 840ms, eta: 51m 32s 920ms\n","\u001b[32m2022-04-27T13:20:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0532, train/total_loss: 0.0001, train/total_loss/avg: 0.0532, max mem: 9224.0, experiment: run, epoch: 60, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 411ms, time_since_start: 02h 09m 52s 251ms, eta: 50m 37s 696ms\n","\u001b[32m2022-04-27T13:21:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0529, train/total_loss: 0.0001, train/total_loss/avg: 0.0529, max mem: 9224.0, experiment: run, epoch: 60, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 569ms, time_since_start: 02h 10m 39s 821ms, eta: 49m 59s 469ms\n","\u001b[32m2022-04-27T13:22:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0526, train/total_loss: 0.0001, train/total_loss/avg: 0.0526, max mem: 9224.0, experiment: run, epoch: 60, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 492ms, time_since_start: 02h 11m 27s 313ms, eta: 49m 06s 271ms\n","\u001b[32m2022-04-27T13:22:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T13:22:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T13:22:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T13:23:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T13:23:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0523, train/total_loss: 0.0001, train/total_loss/avg: 0.0523, max mem: 9224.0, experiment: run, epoch: 61, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 1.82, time: 55s 989ms, time_since_start: 02h 12m 23s 302ms, eta: 56m 56s 461ms\n","\u001b[32m2022-04-27T13:23:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T13:23:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T13:23:04 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T13:23:04 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T13:23:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 2.8825, val/total_loss: 2.8825, val/hateful_memes/accuracy: 0.7056, val/hateful_memes/binary_f1: 0.5282, val/hateful_memes/roc_auc: 0.6574, num_updates: 16000, epoch: 61, iterations: 16000, max_updates: 22000, val_time: 03s 176ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T13:23:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0520, train/total_loss: 0.0001, train/total_loss/avg: 0.0520, max mem: 9224.0, experiment: run, epoch: 61, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 425ms, time_since_start: 02h 13m 14s 906ms, eta: 48m 25s 649ms\n","\u001b[32m2022-04-27T13:24:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0516, train/total_loss: 0.0001, train/total_loss/avg: 0.0516, max mem: 9224.0, experiment: run, epoch: 61, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 799ms, time_since_start: 02h 14m 02s 705ms, eta: 46m 59s 501ms\n","\u001b[32m2022-04-27T13:25:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0513, train/total_loss: 0.0001, train/total_loss/avg: 0.0513, max mem: 9224.0, experiment: run, epoch: 62, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 825ms, time_since_start: 02h 14m 50s 531ms, eta: 46m 12s 424ms\n","\u001b[32m2022-04-27T13:26:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0510, train/total_loss: 0.0001, train/total_loss/avg: 0.0510, max mem: 9224.0, experiment: run, epoch: 62, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 829ms, time_since_start: 02h 15m 38s 361ms, eta: 45m 23s 991ms\n","\u001b[32m2022-04-27T13:27:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0507, train/total_loss: 0.0000, train/total_loss/avg: 0.0507, max mem: 9224.0, experiment: run, epoch: 63, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 388ms, time_since_start: 02h 16m 25s 749ms, eta: 44m 10s 662ms\n","\u001b[32m2022-04-27T13:27:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0504, train/total_loss: 0.0000, train/total_loss/avg: 0.0504, max mem: 9224.0, experiment: run, epoch: 63, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 852ms, time_since_start: 02h 17m 13s 601ms, eta: 43m 47s 952ms\n","\u001b[32m2022-04-27T13:28:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0501, train/total_loss: 0.0000, train/total_loss/avg: 0.0501, max mem: 9224.0, experiment: run, epoch: 63, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 639ms, time_since_start: 02h 18m 01s 241ms, eta: 42m 47s 814ms\n","\u001b[32m2022-04-27T13:29:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0498, train/total_loss: 0.0000, train/total_loss/avg: 0.0498, max mem: 9224.0, experiment: run, epoch: 64, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 470ms, time_since_start: 02h 18m 48s 711ms, eta: 41m 50s 436ms\n","\u001b[32m2022-04-27T13:30:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0496, train/total_loss: 0.0000, train/total_loss/avg: 0.0496, max mem: 9224.0, experiment: run, epoch: 64, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 498ms, time_since_start: 02h 19m 36s 210ms, eta: 41m 03s 627ms\n","\u001b[32m2022-04-27T13:31:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T13:31:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T13:31:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T13:31:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T13:31:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0493, train/total_loss: 0.0000, train/total_loss/avg: 0.0493, max mem: 9224.0, experiment: run, epoch: 64, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 1.72, time: 58s 777ms, time_since_start: 02h 20m 34s 988ms, eta: 49m 48s 858ms\n","\u001b[32m2022-04-27T13:31:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T13:31:13 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T13:31:17 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T13:31:17 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T13:31:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 2.8296, val/total_loss: 2.8296, val/hateful_memes/accuracy: 0.7130, val/hateful_memes/binary_f1: 0.5507, val/hateful_memes/roc_auc: 0.6816, num_updates: 17000, epoch: 64, iterations: 17000, max_updates: 22000, val_time: 03s 728ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T13:32:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0490, train/total_loss: 0.0000, train/total_loss/avg: 0.0490, max mem: 9224.0, experiment: run, epoch: 65, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 775ms, time_since_start: 02h 21m 26s 494ms, eta: 39m 40s 789ms\n","\u001b[32m2022-04-27T13:32:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0487, train/total_loss: 0.0000, train/total_loss/avg: 0.0487, max mem: 9224.0, experiment: run, epoch: 65, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 590ms, time_since_start: 02h 22m 14s 084ms, eta: 38m 43s 186ms\n","\u001b[32m2022-04-27T13:33:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0484, train/total_loss: 0.0000, train/total_loss/avg: 0.0484, max mem: 9224.0, experiment: run, epoch: 66, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 594ms, time_since_start: 02h 23m 01s 679ms, eta: 37m 54s 947ms\n","\u001b[32m2022-04-27T13:34:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0481, train/total_loss: 0.0000, train/total_loss/avg: 0.0481, max mem: 9224.0, experiment: run, epoch: 66, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 587ms, time_since_start: 02h 23m 49s 266ms, eta: 37m 06s 220ms\n","\u001b[32m2022-04-27T13:35:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0479, train/total_loss: 0.0000, train/total_loss/avg: 0.0479, max mem: 9224.0, experiment: run, epoch: 66, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 605ms, time_since_start: 02h 24m 36s 871ms, eta: 36m 18s 662ms\n","\u001b[32m2022-04-27T13:36:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0476, train/total_loss: 0.0000, train/total_loss/avg: 0.0476, max mem: 9224.0, experiment: run, epoch: 67, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 268ms, time_since_start: 02h 25m 24s 139ms, eta: 35m 15s 159ms\n","\u001b[32m2022-04-27T13:36:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0473, train/total_loss: 0.0000, train/total_loss/avg: 0.0473, max mem: 9224.0, experiment: run, epoch: 67, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 625ms, time_since_start: 02h 26m 11s 765ms, eta: 34m 42s 717ms\n","\u001b[32m2022-04-27T13:37:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0471, train/total_loss: 0.0000, train/total_loss/avg: 0.0471, max mem: 9224.0, experiment: run, epoch: 67, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 647ms, time_since_start: 02h 26m 59s 413ms, eta: 33m 55s 219ms\n","\u001b[32m2022-04-27T13:38:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0468, train/total_loss: 0.0000, train/total_loss/avg: 0.0468, max mem: 9224.0, experiment: run, epoch: 68, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 665ms, time_since_start: 02h 27m 47s 078ms, eta: 33m 07s 499ms\n","\u001b[32m2022-04-27T13:39:13 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T13:39:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T13:39:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T13:39:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T13:39:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0465, train/total_loss: 0.0000, train/total_loss/avg: 0.0465, max mem: 9224.0, experiment: run, epoch: 68, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 1.75, time: 57s 023ms, time_since_start: 02h 28m 44s 101ms, eta: 38m 39s 699ms\n","\u001b[32m2022-04-27T13:39:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T13:39:22 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T13:39:26 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T13:39:26 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T13:39:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 2.8796, val/total_loss: 2.8796, val/hateful_memes/accuracy: 0.7074, val/hateful_memes/binary_f1: 0.5123, val/hateful_memes/roc_auc: 0.6930, num_updates: 18000, epoch: 68, iterations: 18000, max_updates: 22000, val_time: 03s 540ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T13:40:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0463, train/total_loss: 0.0000, train/total_loss/avg: 0.0463, max mem: 9224.0, experiment: run, epoch: 69, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 579ms, time_since_start: 02h 29m 36s 223ms, eta: 32m 06s 801ms\n","\u001b[32m2022-04-27T13:41:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0460, train/total_loss: 0.0000, train/total_loss/avg: 0.0460, max mem: 9224.0, experiment: run, epoch: 69, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 109ms, time_since_start: 02h 30m 24s 332ms, eta: 30m 59s 234ms\n","\u001b[32m2022-04-27T13:41:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0458, train/total_loss: 0.0000, train/total_loss/avg: 0.0458, max mem: 9224.0, experiment: run, epoch: 69, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 894ms, time_since_start: 02h 31m 12s 227ms, eta: 30m 02s 226ms\n","\u001b[32m2022-04-27T13:42:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0455, train/total_loss: 0.0000, train/total_loss/avg: 0.0455, max mem: 9224.0, experiment: run, epoch: 70, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 742ms, time_since_start: 02h 31m 59s 969ms, eta: 29m 07s 943ms\n","\u001b[32m2022-04-27T13:43:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0453, train/total_loss: 0.0000, train/total_loss/avg: 0.0453, max mem: 9224.0, experiment: run, epoch: 70, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 276ms, time_since_start: 02h 32m 48s 246ms, eta: 28m 38s 408ms\n","\u001b[32m2022-04-27T13:44:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0450, train/total_loss: 0.0000, train/total_loss/avg: 0.0450, max mem: 9224.0, experiment: run, epoch: 70, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 191ms, time_since_start: 02h 33m 36s 437ms, eta: 27m 46s 349ms\n","\u001b[32m2022-04-27T13:45:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0448, train/total_loss: 0.0000, train/total_loss/avg: 0.0448, max mem: 9224.0, experiment: run, epoch: 71, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 835ms, time_since_start: 02h 34m 24s 273ms, eta: 26m 45s 404ms\n","\u001b[32m2022-04-27T13:45:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0446, train/total_loss: 0.0000, train/total_loss/avg: 0.0446, max mem: 9224.0, experiment: run, epoch: 71, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 812ms, time_since_start: 02h 35m 12s 086ms, eta: 25m 56s 025ms\n","\u001b[32m2022-04-27T13:46:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0443, train/total_loss: 0.0000, train/total_loss/avg: 0.0443, max mem: 9224.0, experiment: run, epoch: 72, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 332ms, time_since_start: 02h 35m 59s 418ms, eta: 24m 52s 243ms\n","\u001b[32m2022-04-27T13:47:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T13:47:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T13:47:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T13:47:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T13:47:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0441, train/total_loss: 0.0000, train/total_loss/avg: 0.0441, max mem: 9224.0, experiment: run, epoch: 72, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 1.79, time: 56s 429ms, time_since_start: 02h 36m 55s 848ms, eta: 28m 41s 671ms\n","\u001b[32m2022-04-27T13:47:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T13:47:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T13:47:38 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T13:47:38 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T13:47:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.1189, val/total_loss: 3.1189, val/hateful_memes/accuracy: 0.7074, val/hateful_memes/binary_f1: 0.5031, val/hateful_memes/roc_auc: 0.6886, num_updates: 19000, epoch: 72, iterations: 19000, max_updates: 22000, val_time: 03s 769ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T13:48:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0439, train/total_loss: 0.0000, train/total_loss/avg: 0.0439, max mem: 9224.0, experiment: run, epoch: 72, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 626ms, time_since_start: 02h 37m 48s 245ms, eta: 23m 54s 143ms\n","\u001b[32m2022-04-27T13:49:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0436, train/total_loss: 0.0000, train/total_loss/avg: 0.0436, max mem: 9224.0, experiment: run, epoch: 73, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 371ms, time_since_start: 02h 38m 35s 617ms, eta: 22m 28s 962ms\n","\u001b[32m2022-04-27T13:50:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0434, train/total_loss: 0.0000, train/total_loss/avg: 0.0434, max mem: 9224.0, experiment: run, epoch: 73, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 694ms, time_since_start: 02h 39m 23s 311ms, eta: 21m 49s 642ms\n","\u001b[32m2022-04-27T13:50:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0432, train/total_loss: 0.0000, train/total_loss/avg: 0.0432, max mem: 9224.0, experiment: run, epoch: 73, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 530ms, time_since_start: 02h 40m 10s 842ms, eta: 20m 56s 804ms\n","\u001b[32m2022-04-27T13:51:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0430, train/total_loss: 0.0000, train/total_loss/avg: 0.0430, max mem: 9224.0, experiment: run, epoch: 74, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 245ms, time_since_start: 02h 40m 58s 087ms, eta: 20m 01s 207ms\n","\u001b[32m2022-04-27T13:52:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0427, train/total_loss: 0.0000, train/total_loss/avg: 0.0427, max mem: 9224.0, experiment: run, epoch: 74, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 562ms, time_since_start: 02h 41m 45s 650ms, eta: 19m 20s 905ms\n","\u001b[32m2022-04-27T13:53:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0425, train/total_loss: 0.0000, train/total_loss/avg: 0.0425, max mem: 9224.0, experiment: run, epoch: 75, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 229ms, time_since_start: 02h 42m 32s 879ms, eta: 18m 24s 753ms\n","\u001b[32m2022-04-27T13:53:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0423, train/total_loss: 0.0000, train/total_loss/avg: 0.0423, max mem: 9224.0, experiment: run, epoch: 75, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 479ms, time_since_start: 02h 43m 20s 359ms, eta: 17m 42s 306ms\n","\u001b[32m2022-04-27T13:54:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0421, train/total_loss: 0.0000, train/total_loss/avg: 0.0421, max mem: 9224.0, experiment: run, epoch: 75, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 542ms, time_since_start: 02h 44m 07s 902ms, eta: 16m 55s 367ms\n","\u001b[32m2022-04-27T13:55:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T13:55:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T13:55:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T13:55:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T13:55:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0419, train/total_loss: 0.0000, train/total_loss/avg: 0.0419, max mem: 9224.0, experiment: run, epoch: 76, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 1.69, time: 59s 022ms, time_since_start: 02h 45m 06s 924ms, eta: 20m 516ms\n","\u001b[32m2022-04-27T13:55:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T13:55:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T13:55:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T13:55:48 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T13:55:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 3.1955, val/total_loss: 3.1955, val/hateful_memes/accuracy: 0.7093, val/hateful_memes/binary_f1: 0.5169, val/hateful_memes/roc_auc: 0.6900, num_updates: 20000, epoch: 76, iterations: 20000, max_updates: 22000, val_time: 03s 548ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T13:56:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0417, train/total_loss: 0.0000, train/total_loss/avg: 0.0417, max mem: 9224.0, experiment: run, epoch: 76, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 2.08, time: 48s 623ms, time_since_start: 02h 45m 59s 098ms, eta: 15m 39s 557ms\n","\u001b[32m2022-04-27T13:57:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0415, train/total_loss: 0.0000, train/total_loss/avg: 0.0415, max mem: 9224.0, experiment: run, epoch: 76, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 870ms, time_since_start: 02h 46m 46s 968ms, eta: 14m 36s 316ms\n","\u001b[32m2022-04-27T13:58:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0413, train/total_loss: 0.0000, train/total_loss/avg: 0.0413, max mem: 9224.0, experiment: run, epoch: 77, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 554ms, time_since_start: 02h 47m 34s 523ms, eta: 13m 42s 168ms\n","\u001b[32m2022-04-27T13:59:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0411, train/total_loss: 0.0000, train/total_loss/avg: 0.0411, max mem: 9224.0, experiment: run, epoch: 77, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 833ms, time_since_start: 02h 48m 22s 357ms, eta: 12m 58s 353ms\n","\u001b[32m2022-04-27T13:59:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0409, train/total_loss: 0.0000, train/total_loss/avg: 0.0409, max mem: 9224.0, experiment: run, epoch: 78, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 755ms, time_since_start: 02h 49m 10s 112ms, eta: 12m 08s 515ms\n","\u001b[32m2022-04-27T14:00:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0407, train/total_loss: 0.0000, train/total_loss/avg: 0.0407, max mem: 9224.0, experiment: run, epoch: 78, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 781ms, time_since_start: 02h 49m 57s 894ms, eta: 11m 20s 311ms\n","\u001b[32m2022-04-27T14:01:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0405, train/total_loss: 0.0000, train/total_loss/avg: 0.0405, max mem: 9224.0, experiment: run, epoch: 78, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 688ms, time_since_start: 02h 50m 45s 582ms, eta: 10m 30s 484ms\n","\u001b[32m2022-04-27T14:02:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0403, train/total_loss: 0.0000, train/total_loss/avg: 0.0403, max mem: 9224.0, experiment: run, epoch: 79, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 398ms, time_since_start: 02h 51m 32s 981ms, eta: 09m 38s 452ms\n","\u001b[32m2022-04-27T14:02:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0401, train/total_loss: 0.0000, train/total_loss/avg: 0.0401, max mem: 9224.0, experiment: run, epoch: 79, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 705ms, time_since_start: 02h 52m 20s 686ms, eta: 08m 53s 680ms\n","\u001b[32m2022-04-27T14:03:46 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T14:03:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T14:03:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T14:03:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T14:03:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0399, train/total_loss: 0.0000, train/total_loss/avg: 0.0399, max mem: 9224.0, experiment: run, epoch: 79, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 1.75, time: 57s 958ms, time_since_start: 02h 53m 18s 645ms, eta: 09m 49s 439ms\n","\u001b[32m2022-04-27T14:03:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T14:03:57 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T14:04:00 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T14:04:00 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T14:04:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 3.3030, val/total_loss: 3.3030, val/hateful_memes/accuracy: 0.7185, val/hateful_memes/binary_f1: 0.5422, val/hateful_memes/roc_auc: 0.6921, num_updates: 21000, epoch: 79, iterations: 21000, max_updates: 22000, val_time: 03s 554ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T14:04:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0397, train/total_loss: 0.0000, train/total_loss/avg: 0.0397, max mem: 9224.0, experiment: run, epoch: 80, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 2.08, time: 48s 388ms, time_since_start: 02h 54m 10s 589ms, eta: 07m 22s 896ms\n","\u001b[32m2022-04-27T14:05:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0395, train/total_loss: 0.0000, train/total_loss/avg: 0.0395, max mem: 9224.0, experiment: run, epoch: 80, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 995ms, time_since_start: 02h 54m 58s 584ms, eta: 06m 30s 492ms\n","\u001b[32m2022-04-27T14:06:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0393, train/total_loss: 0.0000, train/total_loss/avg: 0.0393, max mem: 9224.0, experiment: run, epoch: 81, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 638ms, time_since_start: 02h 55m 46s 223ms, eta: 05m 39s 135ms\n","\u001b[32m2022-04-27T14:07:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0391, train/total_loss: 0.0000, train/total_loss/avg: 0.0391, max mem: 9224.0, experiment: run, epoch: 81, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 910ms, time_since_start: 02h 56m 34s 133ms, eta: 04m 52s 348ms\n","\u001b[32m2022-04-27T14:08:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0389, train/total_loss: 0.0000, train/total_loss/avg: 0.0389, max mem: 9224.0, experiment: run, epoch: 81, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 793ms, time_since_start: 02h 57m 21s 926ms, eta: 04m 03s 027ms\n","\u001b[32m2022-04-27T14:08:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0388, train/total_loss: 0.0000, train/total_loss/avg: 0.0388, max mem: 9224.0, experiment: run, epoch: 82, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 900ms, time_since_start: 02h 58m 09s 827ms, eta: 03m 14s 860ms\n","\u001b[32m2022-04-27T14:09:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0386, train/total_loss: 0.0000, train/total_loss/avg: 0.0386, max mem: 9224.0, experiment: run, epoch: 82, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 759ms, time_since_start: 02h 58m 57s 586ms, eta: 02m 25s 713ms\n","\u001b[32m2022-04-27T14:10:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0384, train/total_loss: 0.0000, train/total_loss/avg: 0.0384, max mem: 9224.0, experiment: run, epoch: 82, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 922ms, time_since_start: 02h 59m 45s 509ms, eta: 01m 37s 475ms\n","\u001b[32m2022-04-27T14:11:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0382, train/total_loss: 0.0000, train/total_loss/avg: 0.0382, max mem: 9224.0, experiment: run, epoch: 83, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 474ms, time_since_start: 03h 32s 984ms, eta: 48s 281ms\n","\u001b[32m2022-04-27T14:11:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-27T14:11:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-27T14:12:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-27T14:12:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-27T14:12:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0381, train/total_loss: 0.0000, train/total_loss/avg: 0.0381, max mem: 9224.0, experiment: run, epoch: 83, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 1.79, time: 56s 186ms, time_since_start: 03h 01m 29s 171ms, eta: 0ms\n","\u001b[32m2022-04-27T14:12:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-27T14:12:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-27T14:12:10 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n","\u001b[32m2022-04-27T14:12:10 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T14:12:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/hateful_memes/cross_entropy: 3.3269, val/total_loss: 3.3269, val/hateful_memes/accuracy: 0.7130, val/hateful_memes/binary_f1: 0.5289, val/hateful_memes/roc_auc: 0.6947, num_updates: 22000, epoch: 83, iterations: 22000, max_updates: 22000, val_time: 04s 848ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.724632\n","\u001b[32m2022-04-27T14:12:12 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2022-04-27T14:12:12 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-04-27T14:12:12 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-04-27T14:12:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-27T14:12:22 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n","\u001b[32m2022-04-27T14:12:22 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n","\u001b[32m2022-04-27T14:12:22 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 8\n","\u001b[32m2022-04-27T14:12:23 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-04-27T14:12:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:16<00:00,  3.89it/s]\n","\u001b[32m2022-04-27T14:12:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 63\n","\u001b[32m2022-04-27T14:12:39 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-27T14:12:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, test/hateful_memes/cross_entropy: 1.0581, test/total_loss: 1.0581, test/hateful_memes/accuracy: 0.7135, test/hateful_memes/binary_f1: 0.5315, test/hateful_memes/roc_auc: 0.7564\n","\u001b[32m2022-04-27T14:12:39 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 03h 02m 01s 157ms\n"]}],"source":["!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yaml \\\n","training.batch_size=32 \\\n","model=visual_bert dataset=hateful_memes"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Fbn7-wv7tlVa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651532564499,"user_tz":300,"elapsed":723895,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"}},"outputId":"5777aff8-75fb-40c1-a7c3-733409d1e53c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-02T22:50:47 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yml\n","\u001b[32m2022-05-02T22:50:47 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-05-02T22:50:47 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-05-02T22:50:47 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n","\u001b[32m2022-05-02T22:50:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save_visual_bert_coco_bs32_da/best.ckpt\n","\u001b[32m2022-05-02T22:50:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","\u001b[32m2022-05-02T22:50:47 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-02T22:50:47 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-02T22:50:47 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save_visual_bert_coco_bs32_da/best.ckpt', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n","\u001b[32m2022-05-02T22:50:47 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-02T22:50:47 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-05-02T22:50:47 | mmf_cli.run: \u001b[0mUsing seed 48425275\n","\u001b[32m2022-05-02T22:50:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [08:40<00:00, 19.8MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 164kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn25r7995\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 24.1kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi2owl8n3\n","Downloading: 100% 570/570 [00:00<00:00, 481kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwdre4ca4\n","Downloading: 100% 232k/232k [00:00<00:00, 687kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpr1xiyo85\n","Downloading: 100% 466k/466k [00:00<00:00, 1.11MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-02T23:02:12 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T23:02:12 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T23:02:12 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T23:02:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmpwe1l2ss4\n","Downloading: 100% 440M/440M [00:05<00:00, 81.5MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-05-02T23:02:26 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-02T23:02:26 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-02T23:02:27 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T23:02:36 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T23:02:36 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-05-02T23:02:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-02T23:02:36 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n","\u001b[32m2022-05-02T23:02:36 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n","\u001b[32m2022-05-02T23:02:36 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 8\n","\u001b[32m2022-05-02T23:02:36 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n","\u001b[32m2022-05-02T23:02:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 9/9 [00:07<00:00,  1.16it/s]\n","\u001b[32m2022-05-02T23:02:44 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes/save/hateful_memes_visual_bert_48425275/reports/hateful_memes_run_val_2022-05-02T23:02:44.csv\n","\u001b[32m2022-05-02T23:02:44 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 9\n","\u001b[32m2022-05-02T23:02:44 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"]}],"source":["!mmf_predict config=\"mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yml\" \\\n","model=visual_bert \\\n","dataset=hateful_memes \\\n","run_type=val \\\n","checkpoint.resume_file=save_visual_bert_coco_bs32_da/best.ckpt \\\n","checkpoint.resume_pretrained=False"]},{"cell_type":"code","source":["!mmf_predict config=\"mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yml\" \\\n","model=visual_bert \\\n","dataset=hateful_memes \\\n","run_type=test \\\n","checkpoint.resume_file=save_visual_bert_coco_bs32_da/best.ckpt \\\n","checkpoint.resume_pretrained=False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOGC3xffsG09","executionInfo":{"status":"ok","timestamp":1651532603230,"user_tz":300,"elapsed":38738,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"}},"outputId":"bc259892-101b-4e6d-8dfc-6efc9f4fb03a"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-02T23:02:52 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yml\n","\u001b[32m2022-05-02T23:02:52 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-05-02T23:02:52 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2022-05-02T23:02:52 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n","\u001b[32m2022-05-02T23:02:52 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save_visual_bert_coco_bs32_da/best.ckpt\n","\u001b[32m2022-05-02T23:02:52 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","\u001b[32m2022-05-02T23:02:52 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-02T23:02:52 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-02T23:02:52 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_50_da.yml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save_visual_bert_coco_bs32_da/best.ckpt', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n","\u001b[32m2022-05-02T23:02:52 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-02T23:02:52 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-05-02T23:02:52 | mmf_cli.run: \u001b[0mUsing seed 48425275\n","\u001b[32m2022-05-02T23:02:52 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-02T23:02:56 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T23:02:56 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T23:02:56 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T23:02:56 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.bias', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.position_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-05-02T23:03:01 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-02T23:03:01 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-02T23:03:01 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T23:03:03 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T23:03:03 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-05-02T23:03:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-02T23:03:03 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n","\u001b[32m2022-05-02T23:03:03 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n","\u001b[32m2022-05-02T23:03:03 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 8\n","\u001b[32m2022-05-02T23:03:03 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n","\u001b[32m2022-05-02T23:03:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 32/32 [00:20<00:00,  1.58it/s]\n","\u001b[32m2022-05-02T23:03:23 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes/save/hateful_memes_visual_bert_48425275/reports/hateful_memes_run_test_2022-05-02T23:03:23.csv\n","\u001b[32m2022-05-02T23:03:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 32\n","\u001b[32m2022-05-02T23:03:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"]}]},{"cell_type":"markdown","metadata":{"id":"sj4Wv30LICg9"},"source":["# **Visual Bert Coco + lr1 + default**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60lkHoEAIArr","outputId":"6a1f4275-9094-4518-af90-bb5567a4c693"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-29T06:38:23 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_lr1.yml\n","\u001b[32m2022-04-29T06:38:23 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n","\u001b[32m2022-04-29T06:38:23 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-29T06:38:23 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-29T06:38:23 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-29T06:38:23 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_lr1.yml', 'training.batch_size=32', 'model=visual_bert', 'dataset=hateful_memes'])\n","\u001b[32m2022-04-29T06:38:23 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-29T06:38:23 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-04-29T06:38:23 | mmf_cli.run: \u001b[0mUsing seed 48425275\n","\u001b[32m2022-04-29T06:38:23 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [06:42<00:00, 25.6MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 145kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp38t7ibuz\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 25.7kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzuypyu0v\n","Downloading: 100% 570/570 [00:00<00:00, 497kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphuj4uk91\n","Downloading: 100% 232k/232k [00:00<00:00, 688kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpoz3otwah\n","Downloading: 100% 466k/466k [00:00<00:00, 1.10MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-29T06:47:58 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-29T06:47:58 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-29T06:47:58 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-29T06:47:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmpb9rv2ejz\n","Downloading: 100% 440M/440M [00:05<00:00, 78.4MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-29T06:48:15 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-29T06:48:15 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-29T06:48:15 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n","Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:21<00:00, 19.2MB/s]\n","[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n","Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-29T06:48:42 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-29T06:48:42 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-29T06:48:42 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-29T06:48:42 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2022-04-29T06:48:42 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2022-04-29T06:48:43 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-29T06:48:43 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-29T06:48:43 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-29T06:48:43 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-04-29T06:50:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.7064, train/hateful_memes/cross_entropy/avg: 0.7064, train/total_loss: 0.7064, train/total_loss/avg: 0.7064, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.27, time: 01m 19s 059ms, time_since_start: 01m 47s 058ms, eta: 04h 53m 28s 444ms\n","\u001b[32m2022-04-29T06:51:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.7064, train/hateful_memes/cross_entropy/avg: 0.7238, train/total_loss: 0.7064, train/total_loss/avg: 0.7238, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 1.28, time: 01m 18s 353ms, time_since_start: 03m 05s 412ms, eta: 04h 49m 31s 445ms\n","\u001b[32m2022-04-29T06:52:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.7064, train/hateful_memes/cross_entropy/avg: 0.7005, train/total_loss: 0.7064, train/total_loss/avg: 0.7005, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 1.28, time: 01m 18s 171ms, time_since_start: 04m 23s 583ms, eta: 04h 47m 31s 495ms\n","\u001b[32m2022-04-29T06:53:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6540, train/hateful_memes/cross_entropy/avg: 0.6773, train/total_loss: 0.6540, train/total_loss/avg: 0.6773, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 1.28, time: 01m 18s 438ms, time_since_start: 05m 42s 022ms, eta: 04h 47m 10s 815ms\n","\u001b[32m2022-04-29T06:55:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.6755, train/hateful_memes/cross_entropy/avg: 0.6769, train/total_loss: 0.6755, train/total_loss/avg: 0.6769, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 1.28, time: 01m 18s 287ms, time_since_start: 07m 309ms, eta: 04h 45m 17s 871ms\n","\u001b[32m2022-04-29T06:56:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.6540, train/hateful_memes/cross_entropy/avg: 0.6577, train/total_loss: 0.6540, train/total_loss/avg: 0.6577, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 1.30, time: 01m 17s 814ms, time_since_start: 08m 18s 123ms, eta: 04h 42m 15s 355ms\n","\u001b[32m2022-04-29T06:57:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6540, train/hateful_memes/cross_entropy/avg: 0.6304, train/total_loss: 0.6540, train/total_loss/avg: 0.6304, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 1.28, time: 01m 18s 397ms, time_since_start: 09m 36s 521ms, eta: 04h 43m 02s 548ms\n","\u001b[32m2022-04-29T06:59:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.6074, train/hateful_memes/cross_entropy/avg: 0.6075, train/total_loss: 0.6074, train/total_loss/avg: 0.6075, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 1.28, time: 01m 18s 048ms, time_since_start: 10m 54s 570ms, eta: 04h 40m 27s 666ms\n","\u001b[32m2022-04-29T07:00:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.6540, train/hateful_memes/cross_entropy/avg: 0.6130, train/total_loss: 0.6540, train/total_loss/avg: 0.6130, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 1.28, time: 01m 18s 521ms, time_since_start: 12m 13s 091ms, eta: 04h 40m 49s 646ms\n","\u001b[32m2022-04-29T07:01:46 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T07:01:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T07:01:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T07:01:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T07:01:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.6074, train/hateful_memes/cross_entropy/avg: 0.6096, train/total_loss: 0.6074, train/total_loss/avg: 0.6096, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 025ms, time_since_start: 13m 41s 117ms, eta: 05h 13m 19s 607ms\n","\u001b[32m2022-04-29T07:01:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T07:01:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T07:02:03 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T07:02:03 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T07:02:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T07:02:09 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-29T07:02:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T07:02:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T07:02:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.7959, val/total_loss: 0.7959, val/hateful_memes/accuracy: 0.6593, val/hateful_memes/binary_f1: 0.2069, val/hateful_memes/roc_auc: 0.6358, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 28s 784ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.635765\n","\u001b[32m2022-04-29T07:03:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.6074, train/hateful_memes/cross_entropy/avg: 0.5789, train/total_loss: 0.6074, train/total_loss/avg: 0.5789, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 868ms, time_since_start: 15m 28s 772ms, eta: 04h 39m 23s 840ms\n","\u001b[32m2022-04-29T07:05:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.5791, train/hateful_memes/cross_entropy/avg: 0.5685, train/total_loss: 0.5791, train/total_loss/avg: 0.5685, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 498ms, time_since_start: 16m 47s 271ms, eta: 04h 36m 45s 280ms\n","\u001b[32m2022-04-29T07:06:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.5791, train/hateful_memes/cross_entropy/avg: 0.5654, train/total_loss: 0.5791, train/total_loss/avg: 0.5654, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 491ms, time_since_start: 18m 05s 763ms, eta: 04h 35m 24s 055ms\n","\u001b[32m2022-04-29T07:07:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.5618, train/hateful_memes/cross_entropy/avg: 0.5487, train/total_loss: 0.5618, train/total_loss/avg: 0.5487, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 292ms, time_since_start: 19m 24s 055ms, eta: 04h 33m 22s 468ms\n","\u001b[32m2022-04-29T07:08:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.5618, train/hateful_memes/cross_entropy/avg: 0.5318, train/total_loss: 0.5618, train/total_loss/avg: 0.5318, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 697ms, time_since_start: 20m 42s 753ms, eta: 04h 33m 27s 316ms\n","\u001b[32m2022-04-29T07:10:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.5284, train/hateful_memes/cross_entropy/avg: 0.5138, train/total_loss: 0.5284, train/total_loss/avg: 0.5138, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 125ms, time_since_start: 22m 879ms, eta: 04h 30m 08s 549ms\n","\u001b[32m2022-04-29T07:11:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.5284, train/hateful_memes/cross_entropy/avg: 0.5045, train/total_loss: 0.5284, train/total_loss/avg: 0.5045, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 677ms, time_since_start: 23m 19s 557ms, eta: 04h 30m 43s 143ms\n","\u001b[32m2022-04-29T07:12:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.4664, train/hateful_memes/cross_entropy/avg: 0.4974, train/total_loss: 0.4664, train/total_loss/avg: 0.4974, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 689ms, time_since_start: 24m 38s 246ms, eta: 04h 29m 25s 545ms\n","\u001b[32m2022-04-29T07:14:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.4664, train/hateful_memes/cross_entropy/avg: 0.4760, train/total_loss: 0.4664, train/total_loss/avg: 0.4760, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 436ms, time_since_start: 25m 56s 683ms, eta: 04h 27m 13s 807ms\n","\u001b[32m2022-04-29T07:15:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T07:15:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T07:15:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T07:15:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T07:15:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.4542, train/hateful_memes/cross_entropy/avg: 0.4615, train/total_loss: 0.4542, train/total_loss/avg: 0.4615, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 051ms, time_since_start: 27m 24s 735ms, eta: 04h 58m 29s 765ms\n","\u001b[32m2022-04-29T07:15:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T07:15:39 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T07:15:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T07:15:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T07:15:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T07:15:50 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-29T07:15:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T07:16:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T07:16:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 0.8928, val/total_loss: 0.8928, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.4865, val/hateful_memes/roc_auc: 0.7028, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 25s 998ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.702794\n","\u001b[32m2022-04-29T07:17:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.4476, train/hateful_memes/cross_entropy/avg: 0.4482, train/total_loss: 0.4476, train/total_loss/avg: 0.4482, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 564ms, time_since_start: 29m 10s 300ms, eta: 04h 28m 22s 478ms\n","\u001b[32m2022-04-29T07:18:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.3758, train/hateful_memes/cross_entropy/avg: 0.4307, train/total_loss: 0.3758, train/total_loss/avg: 0.4307, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 085ms, time_since_start: 30m 28s 385ms, eta: 04h 22m 03s 764ms\n","\u001b[32m2022-04-29T07:20:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.3568, train/hateful_memes/cross_entropy/avg: 0.4237, train/total_loss: 0.3568, train/total_loss/avg: 0.4237, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 671ms, time_since_start: 31m 47s 057ms, eta: 04h 22m 41s 734ms\n","\u001b[32m2022-04-29T07:21:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.3309, train/hateful_memes/cross_entropy/avg: 0.4071, train/total_loss: 0.3309, train/total_loss/avg: 0.4071, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 238ms, time_since_start: 33m 05s 295ms, eta: 04h 19m 55s 433ms\n","\u001b[32m2022-04-29T07:22:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.2948, train/hateful_memes/cross_entropy/avg: 0.3914, train/total_loss: 0.2948, train/total_loss/avg: 0.3914, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 719ms, time_since_start: 34m 24s 015ms, eta: 04h 20m 11s 320ms\n","\u001b[32m2022-04-29T07:23:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.2722, train/hateful_memes/cross_entropy/avg: 0.3768, train/total_loss: 0.2722, train/total_loss/avg: 0.3768, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 723ms, time_since_start: 35m 42s 739ms, eta: 04h 18m 52s 071ms\n","\u001b[32m2022-04-29T07:25:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.2700, train/hateful_memes/cross_entropy/avg: 0.3669, train/total_loss: 0.2700, train/total_loss/avg: 0.3669, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 342ms, time_since_start: 37m 01s 082ms, eta: 04h 16m 17s 184ms\n","\u001b[32m2022-04-29T07:26:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.2439, train/hateful_memes/cross_entropy/avg: 0.3540, train/total_loss: 0.2439, train/total_loss/avg: 0.3540, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 763ms, time_since_start: 38m 19s 845ms, eta: 04h 16m 19s 660ms\n","\u001b[32m2022-04-29T07:27:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.1852, train/hateful_memes/cross_entropy/avg: 0.3449, train/total_loss: 0.1852, train/total_loss/avg: 0.3449, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 845ms, time_since_start: 39m 38s 691ms, eta: 04h 15m 15s 599ms\n","\u001b[32m2022-04-29T07:29:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T07:29:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T07:29:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T07:29:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T07:29:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.1825, train/hateful_memes/cross_entropy/avg: 0.3344, train/total_loss: 0.1825, train/total_loss/avg: 0.3344, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 258ms, time_since_start: 41m 05s 950ms, eta: 04h 41m 990ms\n","\u001b[32m2022-04-29T07:29:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T07:29:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T07:29:26 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T07:29:26 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T07:29:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T07:29:33 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-29T07:29:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T07:29:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T07:29:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.5972, val/total_loss: 1.5972, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.5115, val/hateful_memes/roc_auc: 0.7142, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 21s 475ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.714217\n","\u001b[32m2022-04-29T07:31:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.1282, train/hateful_memes/cross_entropy/avg: 0.3277, train/total_loss: 0.1282, train/total_loss/avg: 0.3277, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 569ms, time_since_start: 42m 46s 996ms, eta: 04h 14m 54s 204ms\n","\u001b[32m2022-04-29T07:32:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.1093, train/hateful_memes/cross_entropy/avg: 0.3177, train/total_loss: 0.1093, train/total_loss/avg: 0.3177, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 376ms, time_since_start: 44m 05s 373ms, eta: 04h 09m 45s 340ms\n","\u001b[32m2022-04-29T07:33:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0921, train/hateful_memes/cross_entropy/avg: 0.3082, train/total_loss: 0.0921, train/total_loss/avg: 0.3082, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 814ms, time_since_start: 45m 24s 187ms, eta: 04h 09m 48s 794ms\n","\u001b[32m2022-04-29T07:34:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0897, train/hateful_memes/cross_entropy/avg: 0.3016, train/total_loss: 0.0897, train/total_loss/avg: 0.3016, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 772ms, time_since_start: 46m 42s 960ms, eta: 04h 08m 20s 833ms\n","\u001b[32m2022-04-29T07:36:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0856, train/hateful_memes/cross_entropy/avg: 0.2945, train/total_loss: 0.0856, train/total_loss/avg: 0.2945, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 530ms, time_since_start: 48m 01s 490ms, eta: 04h 06m 15s 067ms\n","\u001b[32m2022-04-29T07:37:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0630, train/hateful_memes/cross_entropy/avg: 0.2864, train/total_loss: 0.0630, train/total_loss/avg: 0.2864, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 838ms, time_since_start: 49m 20s 329ms, eta: 04h 05m 52s 986ms\n","\u001b[32m2022-04-29T07:38:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0515, train/hateful_memes/cross_entropy/avg: 0.2788, train/total_loss: 0.0515, train/total_loss/avg: 0.2788, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 737ms, time_since_start: 50m 39s 066ms, eta: 04h 04m 13s 888ms\n","\u001b[32m2022-04-29T07:40:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0297, train/hateful_memes/cross_entropy/avg: 0.2716, train/total_loss: 0.0297, train/total_loss/avg: 0.2716, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 082ms, time_since_start: 51m 57s 148ms, eta: 04h 52s 535ms\n","\u001b[32m2022-04-29T07:41:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0235, train/hateful_memes/cross_entropy/avg: 0.2651, train/total_loss: 0.0235, train/total_loss/avg: 0.2651, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 484ms, time_since_start: 53m 15s 633ms, eta: 04h 47s 236ms\n","\u001b[32m2022-04-29T07:42:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T07:42:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T07:42:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T07:42:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T07:42:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0235, train/hateful_memes/cross_entropy/avg: 0.2598, train/total_loss: 0.0235, train/total_loss/avg: 0.2598, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 1.16, time: 01m 26s 302ms, time_since_start: 54m 41s 936ms, eta: 04h 23m 18s 568ms\n","\u001b[32m2022-04-29T07:42:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T07:42:57 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T07:43:01 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T07:43:01 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T07:43:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T07:43:05 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-29T07:43:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T07:43:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T07:43:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.8344, val/total_loss: 1.8344, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.4444, val/hateful_memes/roc_auc: 0.7168, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 22s 210ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.716849\n","\u001b[32m2022-04-29T07:44:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0193, train/hateful_memes/cross_entropy/avg: 0.2538, train/total_loss: 0.0193, train/total_loss/avg: 0.2538, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 374ms, time_since_start: 56m 23s 523ms, eta: 04h 49s 634ms\n","\u001b[32m2022-04-29T07:45:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0148, train/hateful_memes/cross_entropy/avg: 0.2481, train/total_loss: 0.0148, train/total_loss/avg: 0.2481, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 572ms, time_since_start: 57m 42s 095ms, eta: 03h 57m 03s 645ms\n","\u001b[32m2022-04-29T07:47:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0148, train/hateful_memes/cross_entropy/avg: 0.2428, train/total_loss: 0.0148, train/total_loss/avg: 0.2428, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 104ms, time_since_start: 59m 200ms, eta: 03h 54m 19s 463ms\n","\u001b[32m2022-04-29T07:48:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0147, train/hateful_memes/cross_entropy/avg: 0.2374, train/total_loss: 0.0147, train/total_loss/avg: 0.2374, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 585ms, time_since_start: 01h 18s 786ms, eta: 03h 54m 26s 264ms\n","\u001b[32m2022-04-29T07:49:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0148, train/hateful_memes/cross_entropy/avg: 0.2362, train/total_loss: 0.0148, train/total_loss/avg: 0.2362, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 463ms, time_since_start: 01h 01m 37s 250ms, eta: 03h 52m 44s 614ms\n","\u001b[32m2022-04-29T07:51:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0189, train/hateful_memes/cross_entropy/avg: 0.2319, train/total_loss: 0.0189, train/total_loss/avg: 0.2319, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 299ms, time_since_start: 01h 02m 55s 549ms, eta: 03h 50m 55s 695ms\n","\u001b[32m2022-04-29T07:52:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0148, train/hateful_memes/cross_entropy/avg: 0.2270, train/total_loss: 0.0148, train/total_loss/avg: 0.2270, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 542ms, time_since_start: 01h 04m 14s 091ms, eta: 03h 50m 18s 840ms\n","\u001b[32m2022-04-29T07:53:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0148, train/hateful_memes/cross_entropy/avg: 0.2224, train/total_loss: 0.0148, train/total_loss/avg: 0.2224, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 1.30, time: 01m 17s 925ms, time_since_start: 01h 05m 32s 017ms, eta: 03h 47m 11s 035ms\n","\u001b[32m2022-04-29T07:55:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0148, train/hateful_memes/cross_entropy/avg: 0.2184, train/total_loss: 0.0148, train/total_loss/avg: 0.2184, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 546ms, time_since_start: 01h 06m 50s 564ms, eta: 03h 47m 39s 832ms\n","\u001b[32m2022-04-29T07:56:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T07:56:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T07:56:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T07:56:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T07:56:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0147, train/hateful_memes/cross_entropy/avg: 0.2141, train/total_loss: 0.0147, train/total_loss/avg: 0.2141, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 599ms, time_since_start: 01h 08m 19s 164ms, eta: 04h 15m 18s 024ms\n","\u001b[32m2022-04-29T07:56:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T07:56:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T07:56:38 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T07:56:38 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T07:56:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T07:56:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T07:56:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T07:56:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.8486, val/total_loss: 1.8486, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.5307, val/hateful_memes/roc_auc: 0.7146, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 15s 555ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.716849\n","\u001b[32m2022-04-29T07:58:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0069, train/hateful_memes/cross_entropy/avg: 0.2099, train/total_loss: 0.0069, train/total_loss/avg: 0.2099, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 156ms, time_since_start: 01h 09m 53s 877ms, eta: 03h 46m 44s 880ms\n","\u001b[32m2022-04-29T07:59:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0069, train/hateful_memes/cross_entropy/avg: 0.2059, train/total_loss: 0.0069, train/total_loss/avg: 0.2059, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 743ms, time_since_start: 01h 11m 12s 621ms, eta: 03h 44m 13s 858ms\n","\u001b[32m2022-04-29T08:00:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0069, train/hateful_memes/cross_entropy/avg: 0.2020, train/total_loss: 0.0069, train/total_loss/avg: 0.2020, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 859ms, time_since_start: 01h 12m 31s 480ms, eta: 03h 43m 13s 346ms\n","\u001b[32m2022-04-29T08:02:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0055, train/hateful_memes/cross_entropy/avg: 0.1983, train/total_loss: 0.0055, train/total_loss/avg: 0.1983, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 351ms, time_since_start: 01h 13m 49s 831ms, eta: 03h 40m 27s 385ms\n","\u001b[32m2022-04-29T08:03:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0055, train/hateful_memes/cross_entropy/avg: 0.1947, train/total_loss: 0.0055, train/total_loss/avg: 0.1947, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 694ms, time_since_start: 01h 15m 08s 525ms, eta: 03h 40m 05s 289ms\n","\u001b[32m2022-04-29T08:04:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0055, train/hateful_memes/cross_entropy/avg: 0.1913, train/total_loss: 0.0055, train/total_loss/avg: 0.1913, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 331ms, time_since_start: 01h 16m 26s 856ms, eta: 03h 37m 44s 691ms\n","\u001b[32m2022-04-29T08:06:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0055, train/hateful_memes/cross_entropy/avg: 0.1893, train/total_loss: 0.0055, train/total_loss/avg: 0.1893, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 629ms, time_since_start: 01h 17m 45s 486ms, eta: 03h 37m 14s 463ms\n","\u001b[32m2022-04-29T08:07:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0069, train/hateful_memes/cross_entropy/avg: 0.1869, train/total_loss: 0.0069, train/total_loss/avg: 0.1869, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 749ms, time_since_start: 01h 19m 04s 236ms, eta: 03h 36m 14s 372ms\n","\u001b[32m2022-04-29T08:08:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0052, train/hateful_memes/cross_entropy/avg: 0.1838, train/total_loss: 0.0052, train/total_loss/avg: 0.1838, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 441ms, time_since_start: 01h 20m 22s 677ms, eta: 03h 34m 03s 759ms\n","\u001b[32m2022-04-29T08:09:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T08:09:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T08:10:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T08:10:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T08:10:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0052, train/hateful_memes/cross_entropy/avg: 0.1808, train/total_loss: 0.0052, train/total_loss/avg: 0.1808, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 686ms, time_since_start: 01h 21m 54s 364ms, eta: 04h 08m 39s 224ms\n","\u001b[32m2022-04-29T08:10:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T08:10:09 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T08:10:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T08:10:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T08:10:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T08:10:17 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-29T08:10:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T08:10:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T08:10:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 1.9432, val/total_loss: 1.9432, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.5474, val/hateful_memes/roc_auc: 0.7214, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 20s 281ms, best_update: 6000, best_iteration: 6000, best_val/hateful_memes/roc_auc: 0.721449\n","\u001b[32m2022-04-29T08:11:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0052, train/hateful_memes/cross_entropy/avg: 0.1780, train/total_loss: 0.0052, train/total_loss/avg: 0.1780, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 508ms, time_since_start: 01h 23m 34s 155ms, eta: 03h 34m 16s 799ms\n","\u001b[32m2022-04-29T08:13:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0052, train/hateful_memes/cross_entropy/avg: 0.1760, train/total_loss: 0.0052, train/total_loss/avg: 0.1760, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 475ms, time_since_start: 01h 24m 52s 630ms, eta: 03h 30m 09s 868ms\n","\u001b[32m2022-04-29T08:14:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0040, train/hateful_memes/cross_entropy/avg: 0.1732, train/total_loss: 0.0040, train/total_loss/avg: 0.1732, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 739ms, time_since_start: 01h 26m 11s 370ms, eta: 03h 29m 32s 323ms\n","\u001b[32m2022-04-29T08:15:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1705, train/total_loss: 0.0033, train/total_loss/avg: 0.1705, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 209ms, time_since_start: 01h 27m 29s 580ms, eta: 03h 26m 48s 069ms\n","\u001b[32m2022-04-29T08:17:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1680, train/total_loss: 0.0033, train/total_loss/avg: 0.1680, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 525ms, time_since_start: 01h 28m 48s 105ms, eta: 03h 26m 18s 379ms\n","\u001b[32m2022-04-29T08:18:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.1655, train/total_loss: 0.0029, train/total_loss/avg: 0.1655, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 431ms, time_since_start: 01h 30m 06s 536ms, eta: 03h 24m 43s 709ms\n","\u001b[32m2022-04-29T08:19:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1630, train/total_loss: 0.0023, train/total_loss/avg: 0.1630, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 373ms, time_since_start: 01h 31m 24s 909ms, eta: 03h 23m 14s 925ms\n","\u001b[32m2022-04-29T08:20:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1642, train/total_loss: 0.0023, train/total_loss/avg: 0.1642, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 586ms, time_since_start: 01h 32m 43s 496ms, eta: 03h 22m 28s 165ms\n","\u001b[32m2022-04-29T08:22:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1620, train/total_loss: 0.0023, train/total_loss/avg: 0.1620, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 507ms, time_since_start: 01h 34m 02s 003ms, eta: 03h 20m 56s 151ms\n","\u001b[32m2022-04-29T08:23:35 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T08:23:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T08:23:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T08:23:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T08:23:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1597, train/total_loss: 0.0023, train/total_loss/avg: 0.1597, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 1.16, time: 01m 26s 887ms, time_since_start: 01h 35m 28s 891ms, eta: 03h 40m 54s 731ms\n","\u001b[32m2022-04-29T08:23:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T08:23:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T08:23:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T08:23:48 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T08:23:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T08:23:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T08:23:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T08:23:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 1.9331, val/total_loss: 1.9331, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.4810, val/hateful_memes/roc_auc: 0.7095, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 14s 529ms, best_update: 6000, best_iteration: 6000, best_val/hateful_memes/roc_auc: 0.721449\n","\u001b[32m2022-04-29T08:25:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.1576, train/total_loss: 0.0029, train/total_loss/avg: 0.1576, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 359ms, time_since_start: 01h 37m 02s 782ms, eta: 03h 20m 25s 627ms\n","\u001b[32m2022-04-29T08:26:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1554, train/total_loss: 0.0023, train/total_loss/avg: 0.1554, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 469ms, time_since_start: 01h 38m 21s 251ms, eta: 03h 16m 50s 858ms\n","\u001b[32m2022-04-29T08:27:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1534, train/total_loss: 0.0030, train/total_loss/avg: 0.1534, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 781ms, time_since_start: 01h 39m 40s 032ms, eta: 03h 16m 17s 739ms\n","\u001b[32m2022-04-29T08:29:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1513, train/total_loss: 0.0030, train/total_loss/avg: 0.1513, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 692ms, time_since_start: 01h 40m 58s 725ms, eta: 03h 14m 44s 390ms\n","\u001b[32m2022-04-29T08:30:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.1494, train/total_loss: 0.0041, train/total_loss/avg: 0.1494, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 098ms, time_since_start: 01h 42m 16s 823ms, eta: 03h 11m 56s 838ms\n","\u001b[32m2022-04-29T08:31:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.1474, train/total_loss: 0.0041, train/total_loss/avg: 0.1474, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 686ms, time_since_start: 01h 43m 35s 510ms, eta: 03h 12m 03s 416ms\n","\u001b[32m2022-04-29T08:33:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1455, train/total_loss: 0.0030, train/total_loss/avg: 0.1455, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 794ms, time_since_start: 01h 44m 54s 304ms, eta: 03h 10m 59s 095ms\n","\u001b[32m2022-04-29T08:34:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1437, train/total_loss: 0.0023, train/total_loss/avg: 0.1437, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 461ms, time_since_start: 01h 46m 12s 765ms, eta: 03h 08m 50s 986ms\n","\u001b[32m2022-04-29T08:35:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1419, train/total_loss: 0.0023, train/total_loss/avg: 0.1419, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 671ms, time_since_start: 01h 47m 31s 437ms, eta: 03h 08m 01s 222ms\n","\u001b[32m2022-04-29T08:37:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T08:37:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T08:37:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T08:37:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T08:37:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1403, train/total_loss: 0.0023, train/total_loss/avg: 0.1403, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 500ms, time_since_start: 01h 49m 01s 938ms, eta: 03h 34m 45s 533ms\n","\u001b[32m2022-04-29T08:37:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T08:37:17 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T08:37:21 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T08:37:21 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T08:37:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T08:37:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T08:37:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T08:37:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.4746, val/total_loss: 2.4746, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.4790, val/hateful_memes/roc_auc: 0.7094, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 16s 036ms, best_update: 6000, best_iteration: 6000, best_val/hateful_memes/roc_auc: 0.721449\n","\u001b[32m2022-04-29T08:38:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1397, train/total_loss: 0.0023, train/total_loss/avg: 0.1397, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 482ms, time_since_start: 01h 50m 37s 459ms, eta: 03h 07m 15s 910ms\n","\u001b[32m2022-04-29T08:40:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1380, train/total_loss: 0.0017, train/total_loss/avg: 0.1380, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 868ms, time_since_start: 01h 51m 56s 327ms, eta: 03h 04m 28s 851ms\n","\u001b[32m2022-04-29T08:41:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1363, train/total_loss: 0.0017, train/total_loss/avg: 0.1363, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 450ms, time_since_start: 01h 53m 14s 777ms, eta: 03h 02m 10s 379ms\n","\u001b[32m2022-04-29T08:42:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1347, train/total_loss: 0.0017, train/total_loss/avg: 0.1347, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 845ms, time_since_start: 01h 54m 33s 623ms, eta: 03h 01m 45s 272ms\n","\u001b[32m2022-04-29T08:44:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1338, train/total_loss: 0.0017, train/total_loss/avg: 0.1338, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 819ms, time_since_start: 01h 55m 52s 442ms, eta: 03h 21s 498ms\n","\u001b[32m2022-04-29T08:45:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1322, train/total_loss: 0.0016, train/total_loss/avg: 0.1322, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 178ms, time_since_start: 01h 57m 10s 621ms, eta: 02h 57m 34s 046ms\n","\u001b[32m2022-04-29T08:46:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1307, train/total_loss: 0.0016, train/total_loss/avg: 0.1307, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 703ms, time_since_start: 01h 58m 29s 324ms, eta: 02h 57m 25s 488ms\n","\u001b[32m2022-04-29T08:48:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1295, train/total_loss: 0.0016, train/total_loss/avg: 0.1295, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 479ms, time_since_start: 01h 59m 47s 803ms, eta: 02h 55m 35s 342ms\n","\u001b[32m2022-04-29T08:49:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1281, train/total_loss: 0.0015, train/total_loss/avg: 0.1281, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 598ms, time_since_start: 02h 01m 06s 402ms, eta: 02h 54m 31s 421ms\n","\u001b[32m2022-04-29T08:50:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T08:50:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T08:50:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T08:50:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T08:50:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1267, train/total_loss: 0.0012, train/total_loss/avg: 0.1267, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 940ms, time_since_start: 02h 02m 34s 342ms, eta: 03h 13m 46s 622ms\n","\u001b[32m2022-04-29T08:50:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T08:50:49 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T08:50:54 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T08:50:54 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T08:50:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T08:51:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T08:51:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T08:51:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.3326, val/total_loss: 2.3326, val/hateful_memes/accuracy: 0.7148, val/hateful_memes/binary_f1: 0.5188, val/hateful_memes/roc_auc: 0.7190, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 22000, val_time: 15s 522ms, best_update: 6000, best_iteration: 6000, best_val/hateful_memes/roc_auc: 0.721449\n","\u001b[32m2022-04-29T08:52:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.1253, train/total_loss: 0.0011, train/total_loss/avg: 0.1253, max mem: 9224.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00001, ups: 1.28, time: 01m 18s 945ms, time_since_start: 02h 04m 08s 812ms, eta: 02h 52m 37s 139ms\n"]}],"source":["!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_lr1.yml \\\n","training.batch_size=32 \\\n","model=visual_bert dataset=hateful_memes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDBP84mIy69y"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"UQhnHOtQHEfd"},"source":["# **Visual Bert coco lr1 da**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":688422,"status":"ok","timestamp":1651253777289,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"fdYBWvbqHIQc","outputId":"b6742442-8584-44a7-fd08-189bacca216b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-29T16:18:04 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_lr1_da.yml\n","\u001b[32m2022-04-29T16:18:04 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n","\u001b[32m2022-04-29T16:18:04 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-29T16:18:04 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-29T16:18:04 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-29T16:18:04 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_lr1_da.yml', 'training.batch_size=32', 'model=visual_bert', 'dataset=hateful_memes'])\n","\u001b[32m2022-04-29T16:18:04 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-29T16:18:04 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-04-29T16:18:04 | mmf_cli.run: \u001b[0mUsing seed 48425275\n","\u001b[32m2022-04-29T16:18:04 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [07:20<00:00, 23.4MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 148kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpeuxcuaac\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 23.5kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpd332fnmy\n","Downloading: 100% 570/570 [00:00<00:00, 497kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjxtt1ro9\n","Downloading: 100% 232k/232k [00:00<00:00, 692kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpce4qxwcj\n","Downloading: 100% 466k/466k [00:00<00:00, 1.11MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-29T16:28:23 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-29T16:28:23 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-29T16:28:23 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-29T16:28:23 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmp5ykg6gel\n","Downloading: 100% 440M/440M [00:05<00:00, 76.1MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-29T16:28:40 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-29T16:28:40 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-29T16:28:41 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n","Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:20<00:00, 20.3MB/s]\n","[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n","Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-29T16:29:07 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-29T16:29:07 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-29T16:29:07 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-29T16:29:07 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2022-04-29T16:29:08 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-29T16:29:08 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-29T16:29:08 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-29T16:29:08 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-04-29T16:29:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.7324, train/hateful_memes/cross_entropy/avg: 0.7324, train/total_loss: 0.7324, train/total_loss/avg: 0.7324, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 327ms, time_since_start: 01m 14s 396ms, eta: 02h 55m 40s 915ms\n","\u001b[32m2022-04-29T16:30:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6746, train/hateful_memes/cross_entropy/avg: 0.7035, train/total_loss: 0.6746, train/total_loss/avg: 0.7035, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 626ms, time_since_start: 02m 01s 023ms, eta: 02h 52m 17s 478ms\n","\u001b[32m2022-04-29T16:31:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6746, train/hateful_memes/cross_entropy/avg: 0.6913, train/total_loss: 0.6746, train/total_loss/avg: 0.6913, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 350ms, time_since_start: 02m 47s 373ms, eta: 02h 50m 29s 005ms\n","\u001b[32m2022-04-29T16:32:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6669, train/hateful_memes/cross_entropy/avg: 0.6722, train/total_loss: 0.6669, train/total_loss/avg: 0.6722, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 630ms, time_since_start: 03m 34s 004ms, eta: 02h 50m 43s 530ms\n","\u001b[32m2022-04-29T16:33:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.6669, train/hateful_memes/cross_entropy/avg: 0.6668, train/total_loss: 0.6669, train/total_loss/avg: 0.6668, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 674ms, time_since_start: 04m 20s 679ms, eta: 02h 50m 05s 635ms\n","\u001b[32m2022-04-29T16:33:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.6451, train/hateful_memes/cross_entropy/avg: 0.6516, train/total_loss: 0.6451, train/total_loss/avg: 0.6516, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 325ms, time_since_start: 05m 07s 004ms, eta: 02h 48m 02s 121ms\n","\u001b[32m2022-04-29T16:34:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6451, train/hateful_memes/cross_entropy/avg: 0.6301, train/total_loss: 0.6451, train/total_loss/avg: 0.6301, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 689ms, time_since_start: 05m 53s 694ms, eta: 02h 48m 33s 977ms\n","\u001b[32m2022-04-29T16:35:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.6148, train/hateful_memes/cross_entropy/avg: 0.6098, train/total_loss: 0.6148, train/total_loss/avg: 0.6098, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 678ms, time_since_start: 06m 40s 372ms, eta: 02h 47m 44s 107ms\n","\u001b[32m2022-04-29T16:36:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.6148, train/hateful_memes/cross_entropy/avg: 0.6039, train/total_loss: 0.6148, train/total_loss/avg: 0.6039, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 792ms, time_since_start: 07m 27s 165ms, eta: 02h 47m 21s 008ms\n","\u001b[32m2022-04-29T16:36:55 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T16:36:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T16:37:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T16:37:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T16:37:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.5790, train/hateful_memes/cross_entropy/avg: 0.6014, train/total_loss: 0.5790, train/total_loss/avg: 0.6014, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00001, ups: 1.67, time: 01m 044ms, time_since_start: 08m 27s 210ms, eta: 03h 33m 43s 801ms\n","\u001b[32m2022-04-29T16:37:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T16:37:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T16:37:11 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T16:37:11 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T16:37:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T16:37:20 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-29T16:37:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T16:37:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T16:37:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.7931, val/total_loss: 0.7931, val/hateful_memes/accuracy: 0.6519, val/hateful_memes/binary_f1: 0.2480, val/hateful_memes/roc_auc: 0.6359, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 23s 416ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.635897\n","\u001b[32m2022-04-29T16:38:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.5790, train/hateful_memes/cross_entropy/avg: 0.5769, train/total_loss: 0.5790, train/total_loss/avg: 0.5769, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 492ms, time_since_start: 09m 38s 119ms, eta: 02h 48m 14s 683ms\n","\u001b[32m2022-04-29T16:39:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.5757, train/hateful_memes/cross_entropy/avg: 0.5650, train/total_loss: 0.5757, train/total_loss/avg: 0.5650, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 265ms, time_since_start: 10m 25s 385ms, eta: 02h 46m 38s 416ms\n","\u001b[32m2022-04-29T16:39:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.5757, train/hateful_memes/cross_entropy/avg: 0.5627, train/total_loss: 0.5757, train/total_loss/avg: 0.5627, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 237ms, time_since_start: 11m 12s 623ms, eta: 02h 45m 44s 404ms\n","\u001b[32m2022-04-29T16:40:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.5565, train/hateful_memes/cross_entropy/avg: 0.5495, train/total_loss: 0.5565, train/total_loss/avg: 0.5495, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 151ms, time_since_start: 11m 59s 774ms, eta: 02h 44m 38s 319ms\n","\u001b[32m2022-04-29T16:41:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.5565, train/hateful_memes/cross_entropy/avg: 0.5333, train/total_loss: 0.5565, train/total_loss/avg: 0.5333, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 408ms, time_since_start: 12m 47s 183ms, eta: 02h 44m 43s 958ms\n","\u001b[32m2022-04-29T16:42:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.5351, train/hateful_memes/cross_entropy/avg: 0.5163, train/total_loss: 0.5351, train/total_loss/avg: 0.5163, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 914ms, time_since_start: 13m 34s 098ms, eta: 02h 42m 13s 329ms\n","\u001b[32m2022-04-29T16:43:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.5351, train/hateful_memes/cross_entropy/avg: 0.5018, train/total_loss: 0.5351, train/total_loss/avg: 0.5018, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 210ms, time_since_start: 14m 21s 308ms, eta: 02h 42m 26s 706ms\n","\u001b[32m2022-04-29T16:43:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.5011, train/hateful_memes/cross_entropy/avg: 0.4952, train/total_loss: 0.5011, train/total_loss/avg: 0.4952, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 374ms, time_since_start: 15m 08s 683ms, eta: 02h 42m 12s 347ms\n","\u001b[32m2022-04-29T16:44:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.5011, train/hateful_memes/cross_entropy/avg: 0.4711, train/total_loss: 0.5011, train/total_loss/avg: 0.4711, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 299ms, time_since_start: 15m 55s 983ms, eta: 02h 41m 08s 863ms\n","\u001b[32m2022-04-29T16:45:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T16:45:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T16:45:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T16:45:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T16:45:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.4679, train/hateful_memes/cross_entropy/avg: 0.4600, train/total_loss: 0.4679, train/total_loss/avg: 0.4600, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 1.79, time: 56s 520ms, time_since_start: 16m 52s 503ms, eta: 03h 11m 36s 275ms\n","\u001b[32m2022-04-29T16:45:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T16:45:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T16:45:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T16:45:39 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T16:45:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T16:45:45 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-29T16:45:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T16:45:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T16:45:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.0824, val/total_loss: 1.0824, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4577, val/hateful_memes/roc_auc: 0.6916, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 22s 544ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.691618\n","\u001b[32m2022-04-29T16:46:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.4338, train/hateful_memes/cross_entropy/avg: 0.4468, train/total_loss: 0.4338, train/total_loss/avg: 0.4468, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 138ms, time_since_start: 18m 03s 188ms, eta: 02h 42m 22s 456ms\n","\u001b[32m2022-04-29T16:47:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.3835, train/hateful_memes/cross_entropy/avg: 0.4303, train/total_loss: 0.3835, train/total_loss/avg: 0.4303, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 382ms, time_since_start: 18m 50s 570ms, eta: 02h 39m 01s 255ms\n","\u001b[32m2022-04-29T16:48:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.3789, train/hateful_memes/cross_entropy/avg: 0.4280, train/total_loss: 0.3789, train/total_loss/avg: 0.4280, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 501ms, time_since_start: 19m 38s 072ms, eta: 02h 38m 36s 901ms\n","\u001b[32m2022-04-29T16:49:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.3775, train/hateful_memes/cross_entropy/avg: 0.4139, train/total_loss: 0.3775, train/total_loss/avg: 0.4139, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 158ms, time_since_start: 20m 25s 230ms, eta: 02h 36m 40s 109ms\n","\u001b[32m2022-04-29T16:49:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.3321, train/hateful_memes/cross_entropy/avg: 0.3979, train/total_loss: 0.3321, train/total_loss/avg: 0.3979, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 349ms, time_since_start: 21m 12s 580ms, eta: 02h 36m 30s 161ms\n","\u001b[32m2022-04-29T16:50:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.3068, train/hateful_memes/cross_entropy/avg: 0.3840, train/total_loss: 0.3068, train/total_loss/avg: 0.3840, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 404ms, time_since_start: 21m 59s 984ms, eta: 02h 35m 52s 841ms\n","\u001b[32m2022-04-29T16:51:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.2701, train/hateful_memes/cross_entropy/avg: 0.3715, train/total_loss: 0.2701, train/total_loss/avg: 0.3715, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 968ms, time_since_start: 22m 46s 953ms, eta: 02h 33m 39s 063ms\n","\u001b[32m2022-04-29T16:52:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.2611, train/hateful_memes/cross_entropy/avg: 0.3613, train/total_loss: 0.2611, train/total_loss/avg: 0.3613, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 317ms, time_since_start: 23m 34s 271ms, eta: 02h 33m 59s 391ms\n","\u001b[32m2022-04-29T16:53:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.2488, train/hateful_memes/cross_entropy/avg: 0.3544, train/total_loss: 0.2488, train/total_loss/avg: 0.3544, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 328ms, time_since_start: 24m 21s 599ms, eta: 02h 33m 13s 421ms\n","\u001b[32m2022-04-29T16:53:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T16:53:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T16:53:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T16:53:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T16:53:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.1831, train/hateful_memes/cross_entropy/avg: 0.3432, train/total_loss: 0.1831, train/total_loss/avg: 0.3432, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 1.75, time: 57s 047ms, time_since_start: 25m 18s 647ms, eta: 03h 03m 43s 332ms\n","\u001b[32m2022-04-29T16:53:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T16:53:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T16:54:02 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T16:54:02 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T16:54:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T16:54:07 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-29T16:54:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T16:54:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T16:54:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.6292, val/total_loss: 1.6292, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.4645, val/hateful_memes/roc_auc: 0.7184, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 21s 091ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.718360\n","\u001b[32m2022-04-29T16:55:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.1613, train/hateful_memes/cross_entropy/avg: 0.3339, train/total_loss: 0.1613, train/total_loss/avg: 0.3339, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 251ms, time_since_start: 26m 27s 992ms, eta: 02h 34m 34s 625ms\n","\u001b[32m2022-04-29T16:55:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.0892, train/hateful_memes/cross_entropy/avg: 0.3239, train/total_loss: 0.0892, train/total_loss/avg: 0.3239, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 325ms, time_since_start: 27m 15s 318ms, eta: 02h 30m 48s 501ms\n","\u001b[32m2022-04-29T16:56:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0892, train/hateful_memes/cross_entropy/avg: 0.3181, train/total_loss: 0.0892, train/total_loss/avg: 0.3181, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 556ms, time_since_start: 28m 02s 874ms, eta: 02h 30m 44s 173ms\n","\u001b[32m2022-04-29T16:57:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0892, train/hateful_memes/cross_entropy/avg: 0.3129, train/total_loss: 0.0892, train/total_loss/avg: 0.3129, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 478ms, time_since_start: 28m 50s 352ms, eta: 02h 29m 41s 085ms\n","\u001b[32m2022-04-29T16:58:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0840, train/hateful_memes/cross_entropy/avg: 0.3040, train/total_loss: 0.0840, train/total_loss/avg: 0.3040, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 385ms, time_since_start: 29m 37s 738ms, eta: 02h 28m 35s 351ms\n","\u001b[32m2022-04-29T16:59:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0836, train/hateful_memes/cross_entropy/avg: 0.2956, train/total_loss: 0.0836, train/total_loss/avg: 0.2956, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 657ms, time_since_start: 30m 25s 395ms, eta: 02h 28m 38s 054ms\n","\u001b[32m2022-04-29T16:59:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0559, train/hateful_memes/cross_entropy/avg: 0.2877, train/total_loss: 0.0559, train/total_loss/avg: 0.2877, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 607ms, time_since_start: 31m 13s 003ms, eta: 02h 27m 40s 346ms\n","\u001b[32m2022-04-29T17:00:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0559, train/hateful_memes/cross_entropy/avg: 0.2833, train/total_loss: 0.0559, train/total_loss/avg: 0.2833, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 224ms, time_since_start: 32m 228ms, eta: 02h 25m 40s 992ms\n","\u001b[32m2022-04-29T17:01:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0559, train/hateful_memes/cross_entropy/avg: 0.2762, train/total_loss: 0.0559, train/total_loss/avg: 0.2762, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 560ms, time_since_start: 32m 47s 789ms, eta: 02h 25m 54s 876ms\n","\u001b[32m2022-04-29T17:02:16 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T17:02:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T17:02:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T17:02:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T17:02:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0476, train/hateful_memes/cross_entropy/avg: 0.2694, train/total_loss: 0.0476, train/total_loss/avg: 0.2694, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 1.79, time: 56s 490ms, time_since_start: 33m 44s 280ms, eta: 02h 52m 21s 221ms\n","\u001b[32m2022-04-29T17:02:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T17:02:25 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T17:02:28 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T17:02:28 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T17:02:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T17:02:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T17:02:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T17:02:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.9777, val/total_loss: 1.9777, val/hateful_memes/accuracy: 0.7000, val/hateful_memes/binary_f1: 0.4808, val/hateful_memes/roc_auc: 0.7097, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 12s 728ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.718360\n","\u001b[32m2022-04-29T17:03:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0372, train/hateful_memes/cross_entropy/avg: 0.2630, train/total_loss: 0.0372, train/total_loss/avg: 0.2630, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 254ms, time_since_start: 34m 45s 265ms, eta: 02h 26m 24s 433ms\n","\u001b[32m2022-04-29T17:04:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0372, train/hateful_memes/cross_entropy/avg: 0.2576, train/total_loss: 0.0372, train/total_loss/avg: 0.2576, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 862ms, time_since_start: 35m 33s 128ms, eta: 02h 24m 24s 448ms\n","\u001b[32m2022-04-29T17:05:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0184, train/hateful_memes/cross_entropy/avg: 0.2517, train/total_loss: 0.0184, train/total_loss/avg: 0.2517, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 244ms, time_since_start: 36m 20s 373ms, eta: 02h 21m 44s 514ms\n","\u001b[32m2022-04-29T17:05:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0141, train/hateful_memes/cross_entropy/avg: 0.2461, train/total_loss: 0.0141, train/total_loss/avg: 0.2461, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 499ms, time_since_start: 37m 07s 872ms, eta: 02h 21m 42s 074ms\n","\u001b[32m2022-04-29T17:06:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0141, train/hateful_memes/cross_entropy/avg: 0.2408, train/total_loss: 0.0141, train/total_loss/avg: 0.2408, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 619ms, time_since_start: 37m 55s 492ms, eta: 02h 21m 15s 048ms\n","\u001b[32m2022-04-29T17:07:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0105, train/hateful_memes/cross_entropy/avg: 0.2358, train/total_loss: 0.0105, train/total_loss/avg: 0.2358, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 351ms, time_since_start: 38m 42s 844ms, eta: 02h 19m 39s 318ms\n","\u001b[32m2022-04-29T17:08:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0105, train/hateful_memes/cross_entropy/avg: 0.2313, train/total_loss: 0.0105, train/total_loss/avg: 0.2313, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 463ms, time_since_start: 39m 30s 308ms, eta: 02h 19m 10s 829ms\n","\u001b[32m2022-04-29T17:08:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.2267, train/total_loss: 0.0089, train/total_loss/avg: 0.2267, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 114ms, time_since_start: 40m 17s 422ms, eta: 02h 17m 21s 387ms\n","\u001b[32m2022-04-29T17:09:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.2222, train/total_loss: 0.0089, train/total_loss/avg: 0.2222, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 439ms, time_since_start: 41m 04s 861ms, eta: 02h 17m 30s 025ms\n","\u001b[32m2022-04-29T17:10:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T17:10:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T17:10:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T17:10:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T17:10:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.2179, train/total_loss: 0.0068, train/total_loss/avg: 0.2179, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 1.79, time: 56s 627ms, time_since_start: 42m 01s 489ms, eta: 02h 43m 10s 363ms\n","\u001b[32m2022-04-29T17:10:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T17:10:42 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T17:10:45 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T17:10:45 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T17:10:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T17:10:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T17:10:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T17:10:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.8764, val/total_loss: 1.8764, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.5015, val/hateful_memes/roc_auc: 0.7088, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 13s 958ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.718360\n","\u001b[32m2022-04-29T17:11:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.2136, train/total_loss: 0.0067, train/total_loss/avg: 0.2136, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 093ms, time_since_start: 43m 03s 541ms, eta: 02h 17m 45s 924ms\n","\u001b[32m2022-04-29T17:12:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.2099, train/total_loss: 0.0067, train/total_loss/avg: 0.2099, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 621ms, time_since_start: 43m 51s 163ms, eta: 02h 15m 36s 377ms\n","\u001b[32m2022-04-29T17:13:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0066, train/hateful_memes/cross_entropy/avg: 0.2060, train/total_loss: 0.0066, train/total_loss/avg: 0.2060, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 594ms, time_since_start: 44m 38s 757ms, eta: 02h 14m 43s 429ms\n","\u001b[32m2022-04-29T17:14:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.2023, train/total_loss: 0.0065, train/total_loss/avg: 0.2023, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 266ms, time_since_start: 45m 26s 024ms, eta: 02h 12m 59s 653ms\n","\u001b[32m2022-04-29T17:14:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0066, train/hateful_memes/cross_entropy/avg: 0.1995, train/total_loss: 0.0066, train/total_loss/avg: 0.1995, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 572ms, time_since_start: 46m 13s 597ms, eta: 02h 13m 02s 946ms\n","\u001b[32m2022-04-29T17:15:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1962, train/total_loss: 0.0067, train/total_loss/avg: 0.1962, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 481ms, time_since_start: 47m 01s 078ms, eta: 02h 11m 59s 263ms\n","\u001b[32m2022-04-29T17:16:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1934, train/total_loss: 0.0068, train/total_loss/avg: 0.1934, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 577ms, time_since_start: 47m 48s 656ms, eta: 02h 11m 27s 016ms\n","\u001b[32m2022-04-29T17:17:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1901, train/total_loss: 0.0067, train/total_loss/avg: 0.1901, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 405ms, time_since_start: 48m 36s 062ms, eta: 02h 10m 10s 314ms\n","\u001b[32m2022-04-29T17:18:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1869, train/total_loss: 0.0067, train/total_loss/avg: 0.1869, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 927ms, time_since_start: 49m 22s 990ms, eta: 02h 08m 03s 846ms\n","\u001b[32m2022-04-29T17:18:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T17:18:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T17:18:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T17:19:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T17:19:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1837, train/total_loss: 0.0067, train/total_loss/avg: 0.1837, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 1.79, time: 56s 482ms, time_since_start: 50m 19s 472ms, eta: 02h 33m 10s 857ms\n","\u001b[32m2022-04-29T17:19:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T17:19:00 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T17:19:04 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T17:19:04 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T17:19:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T17:19:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T17:19:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T17:19:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.2599, val/total_loss: 2.2599, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.4625, val/hateful_memes/roc_auc: 0.7028, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 16s 977ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.718360\n","\u001b[32m2022-04-29T17:20:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1822, train/total_loss: 0.0068, train/total_loss/avg: 0.1822, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 339ms, time_since_start: 51m 24s 791ms, eta: 02h 10m 16s 721ms\n","\u001b[32m2022-04-29T17:20:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1807, train/total_loss: 0.0068, train/total_loss/avg: 0.1807, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 688ms, time_since_start: 52m 12s 480ms, eta: 02h 07m 42s 899ms\n","\u001b[32m2022-04-29T17:21:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1778, train/total_loss: 0.0068, train/total_loss/avg: 0.1778, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 564ms, time_since_start: 53m 044ms, eta: 02h 06m 34s 551ms\n","\u001b[32m2022-04-29T17:22:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1751, train/total_loss: 0.0068, train/total_loss/avg: 0.1751, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 970ms, time_since_start: 53m 47s 014ms, eta: 02h 04m 11s 893ms\n","\u001b[32m2022-04-29T17:23:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1733, train/total_loss: 0.0068, train/total_loss/avg: 0.1733, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 407ms, time_since_start: 54m 34s 422ms, eta: 02h 04m 33s 136ms\n","\u001b[32m2022-04-29T17:24:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1707, train/total_loss: 0.0065, train/total_loss/avg: 0.1707, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 433ms, time_since_start: 55m 21s 856ms, eta: 02h 03m 48s 975ms\n","\u001b[32m2022-04-29T17:24:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0057, train/hateful_memes/cross_entropy/avg: 0.1682, train/total_loss: 0.0057, train/total_loss/avg: 0.1682, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 285ms, time_since_start: 56m 09s 141ms, eta: 02h 02m 37s 632ms\n","\u001b[32m2022-04-29T17:25:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1657, train/total_loss: 0.0043, train/total_loss/avg: 0.1657, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 346ms, time_since_start: 56m 56s 488ms, eta: 02h 01m 59s 019ms\n","\u001b[32m2022-04-29T17:26:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1634, train/total_loss: 0.0043, train/total_loss/avg: 0.1634, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 307ms, time_since_start: 57m 43s 795ms, eta: 02h 01m 04s 859ms\n","\u001b[32m2022-04-29T17:27:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T17:27:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T17:27:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T17:27:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T17:27:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.1610, train/total_loss: 0.0029, train/total_loss/avg: 0.1610, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 1.72, time: 58s 128ms, time_since_start: 58m 41s 923ms, eta: 02h 27m 47s 459ms\n","\u001b[32m2022-04-29T17:27:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T17:27:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T17:27:25 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T17:27:25 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T17:27:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T17:27:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T17:27:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T17:27:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.3457, val/total_loss: 2.3457, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.4641, val/hateful_memes/roc_auc: 0.7020, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 13s 409ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.718360\n","\u001b[32m2022-04-29T17:28:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.1588, train/total_loss: 0.0029, train/total_loss/avg: 0.1588, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 135ms, time_since_start: 59m 43s 469ms, eta: 02h 01m 34s 062ms\n","\u001b[32m2022-04-29T17:29:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.1567, train/total_loss: 0.0029, train/total_loss/avg: 0.1567, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 623ms, time_since_start: 01h 31s 092ms, eta: 01h 59m 28s 087ms\n","\u001b[32m2022-04-29T17:29:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1545, train/total_loss: 0.0025, train/total_loss/avg: 0.1545, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 327ms, time_since_start: 01h 01m 18s 420ms, eta: 01h 57m 55s 455ms\n","\u001b[32m2022-04-29T17:30:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1527, train/total_loss: 0.0025, train/total_loss/avg: 0.1527, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 410ms, time_since_start: 01h 02m 05s 830ms, eta: 01h 57m 19s 542ms\n","\u001b[32m2022-04-29T17:31:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1507, train/total_loss: 0.0017, train/total_loss/avg: 0.1507, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 000ms, time_since_start: 01h 02m 52s 831ms, eta: 01h 55m 30s 973ms\n","\u001b[32m2022-04-29T17:32:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1487, train/total_loss: 0.0012, train/total_loss/avg: 0.1487, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 350ms, time_since_start: 01h 03m 40s 182ms, eta: 01h 55m 34s 399ms\n","\u001b[32m2022-04-29T17:33:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1471, train/total_loss: 0.0012, train/total_loss/avg: 0.1471, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 353ms, time_since_start: 01h 04m 27s 535ms, eta: 01h 54m 46s 602ms\n","\u001b[32m2022-04-29T17:33:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1453, train/total_loss: 0.0017, train/total_loss/avg: 0.1453, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 287ms, time_since_start: 01h 05m 14s 822ms, eta: 01h 53m 48s 996ms\n","\u001b[32m2022-04-29T17:34:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1435, train/total_loss: 0.0017, train/total_loss/avg: 0.1435, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 343ms, time_since_start: 01h 06m 02s 166ms, eta: 01h 53m 08s 916ms\n","\u001b[32m2022-04-29T17:35:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-29T17:35:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-29T17:35:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-29T17:35:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-29T17:35:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1417, train/total_loss: 0.0017, train/total_loss/avg: 0.1417, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 1.79, time: 56s 964ms, time_since_start: 01h 06m 59s 130ms, eta: 02h 15m 10s 572ms\n","\u001b[32m2022-04-29T17:35:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-29T17:35:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-29T17:35:43 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-29T17:35:43 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T17:35:43 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-04-29T17:35:43 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-04-29T17:35:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-29T17:35:51 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n","\u001b[32m2022-04-29T17:35:51 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n","\u001b[32m2022-04-29T17:35:51 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 12\n","\u001b[32m2022-04-29T17:35:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 2.3251, val/total_loss: 2.3251, val/hateful_memes/accuracy: 0.7000, val/hateful_memes/binary_f1: 0.4808, val/hateful_memes/roc_auc: 0.7092, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 12s 464ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.718360\n","\u001b[32m2022-04-29T17:35:53 | mmf.trainers.core.training_loop: \u001b[0mEarly stopping activated\n","\u001b[32m2022-04-29T17:35:53 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2022-04-29T17:35:53 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-04-29T17:35:53 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-04-29T17:35:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-29T17:35:54 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n","\u001b[32m2022-04-29T17:35:54 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n","\u001b[32m2022-04-29T17:35:54 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 12\n","\u001b[32m2022-04-29T17:35:56 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-04-29T17:35:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:16<00:00,  3.92it/s]\n","\u001b[32m2022-04-29T17:36:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 63\n","\u001b[32m2022-04-29T17:36:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-29T17:36:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, test/hateful_memes/cross_entropy: 1.5585, test/total_loss: 1.5585, test/hateful_memes/accuracy: 0.6945, test/hateful_memes/binary_f1: 0.4826, test/hateful_memes/roc_auc: 0.7435\n","\u001b[32m2022-04-29T17:36:12 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 07m 31s 169ms\n"]}],"source":["!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_lr1_da.yml \\\n","training.batch_size=32 \\\n","model=visual_bert dataset=hateful_memes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"no6kb9FLHSFn"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"qKiu6m1aMBqk"},"source":["# **Visual Bert Coco bs 32 lr 1 50 da**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2744557,"status":"ok","timestamp":1651283265646,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"I6G2Btg7MIsk","outputId":"bb01ec2a-b41a-43a7-c935-812f2b455541"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-04-30T00:27:05 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_lr1_da.yml\n","\u001b[32m2022-04-30T00:27:05 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n","\u001b[32m2022-04-30T00:27:05 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-04-30T00:27:05 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-04-30T00:27:05 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-04-30T00:27:05 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_lr1_da.yml', 'training.batch_size=32', 'model=visual_bert', 'dataset=hateful_memes'])\n","\u001b[32m2022-04-30T00:27:05 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-04-30T00:27:05 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-04-30T00:27:05 | mmf_cli.run: \u001b[0mUsing seed 48425275\n","\u001b[32m2022-04-30T00:27:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [09:54<00:00, 17.3MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 147kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpap2f6qzi\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 23.5kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6t63cnf9\n","Downloading: 100% 570/570 [00:00<00:00, 454kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi33hohah\n","Downloading: 100% 232k/232k [00:00<00:00, 683kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpimrtj89i\n","Downloading: 100% 466k/466k [00:00<00:00, 1.11MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-04-30T00:40:06 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-30T00:40:06 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-30T00:40:06 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-04-30T00:40:06 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmpsmh2z8w8\n","Downloading: 100% 440M/440M [00:05<00:00, 74.7MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.bias', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-04-30T00:40:24 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-04-30T00:40:24 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-04-30T00:40:24 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n","Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:22<00:00, 18.3MB/s]\n","[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n","Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-30T00:40:53 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-30T00:40:53 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-30T00:40:53 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-30T00:40:53 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2022-04-30T00:40:53 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-04-30T00:40:53 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-04-30T00:40:53 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-04-30T00:40:53 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-04-30T00:41:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.7324, train/hateful_memes/cross_entropy/avg: 0.7324, train/total_loss: 0.7324, train/total_loss/avg: 0.7324, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 2.08, time: 48s 772ms, time_since_start: 01m 17s 689ms, eta: 03h 01m 02s 713ms\n","\u001b[32m2022-04-30T00:42:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6746, train/hateful_memes/cross_entropy/avg: 0.7035, train/total_loss: 0.6746, train/total_loss/avg: 0.7035, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 723ms, time_since_start: 02m 04s 413ms, eta: 02h 52m 38s 985ms\n","\u001b[32m2022-04-30T00:43:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6746, train/hateful_memes/cross_entropy/avg: 0.6913, train/total_loss: 0.6746, train/total_loss/avg: 0.6913, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 618ms, time_since_start: 02m 51s 031ms, eta: 02h 51m 28s 088ms\n","\u001b[32m2022-04-30T00:44:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6669, train/hateful_memes/cross_entropy/avg: 0.6722, train/total_loss: 0.6669, train/total_loss/avg: 0.6722, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 715ms, time_since_start: 03m 37s 746ms, eta: 02h 51m 02s 015ms\n","\u001b[32m2022-04-30T00:44:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.6669, train/hateful_memes/cross_entropy/avg: 0.6668, train/total_loss: 0.6669, train/total_loss/avg: 0.6668, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 683ms, time_since_start: 04m 24s 429ms, eta: 02h 50m 07s 604ms\n","\u001b[32m2022-04-30T00:45:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.6451, train/hateful_memes/cross_entropy/avg: 0.6516, train/total_loss: 0.6451, train/total_loss/avg: 0.6516, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 319ms, time_since_start: 05m 10s 749ms, eta: 02h 48m 900ms\n","\u001b[32m2022-04-30T00:46:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6451, train/hateful_memes/cross_entropy/avg: 0.6301, train/total_loss: 0.6451, train/total_loss/avg: 0.6301, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 758ms, time_since_start: 05m 57s 507ms, eta: 02h 48m 48s 809ms\n","\u001b[32m2022-04-30T00:47:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.6148, train/hateful_memes/cross_entropy/avg: 0.6098, train/total_loss: 0.6148, train/total_loss/avg: 0.6098, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 660ms, time_since_start: 06m 44s 168ms, eta: 02h 47m 40s 261ms\n","\u001b[32m2022-04-30T00:47:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.6148, train/hateful_memes/cross_entropy/avg: 0.6039, train/total_loss: 0.6148, train/total_loss/avg: 0.6039, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 757ms, time_since_start: 07m 30s 926ms, eta: 02h 47m 13s 577ms\n","\u001b[32m2022-04-30T00:48:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-30T00:48:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T00:48:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T00:48:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T00:48:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.5790, train/hateful_memes/cross_entropy/avg: 0.6014, train/total_loss: 0.5790, train/total_loss/avg: 0.6014, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00001, ups: 1.79, time: 56s 860ms, time_since_start: 08m 27s 786ms, eta: 03h 22m 23s 763ms\n","\u001b[32m2022-04-30T00:48:52 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-30T00:48:52 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-30T00:48:58 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-30T00:48:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-30T00:48:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T00:49:06 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-30T00:49:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T00:49:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T00:49:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.7931, val/total_loss: 0.7931, val/hateful_memes/accuracy: 0.6519, val/hateful_memes/binary_f1: 0.2480, val/hateful_memes/roc_auc: 0.6359, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 24s 228ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.635897\n","\u001b[32m2022-04-30T00:50:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.5790, train/hateful_memes/cross_entropy/avg: 0.5769, train/total_loss: 0.5790, train/total_loss/avg: 0.5769, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 577ms, time_since_start: 09m 39s 594ms, eta: 02h 48m 32s 651ms\n","\u001b[32m2022-04-30T00:50:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.5757, train/hateful_memes/cross_entropy/avg: 0.5650, train/total_loss: 0.5757, train/total_loss/avg: 0.5650, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 029ms, time_since_start: 10m 26s 623ms, eta: 02h 45m 48s 478ms\n","\u001b[32m2022-04-30T00:51:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.5757, train/hateful_memes/cross_entropy/avg: 0.5627, train/total_loss: 0.5757, train/total_loss/avg: 0.5627, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 029ms, time_since_start: 11m 13s 653ms, eta: 02h 45m 643ms\n","\u001b[32m2022-04-30T00:52:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.5565, train/hateful_memes/cross_entropy/avg: 0.5495, train/total_loss: 0.5565, train/total_loss/avg: 0.5495, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 727ms, time_since_start: 12m 381ms, eta: 02h 43m 09s 533ms\n","\u001b[32m2022-04-30T00:53:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.5565, train/hateful_memes/cross_entropy/avg: 0.5333, train/total_loss: 0.5565, train/total_loss/avg: 0.5333, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 034ms, time_since_start: 12m 47s 415ms, eta: 02h 43m 26s 008ms\n","\u001b[32m2022-04-30T00:53:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.5351, train/hateful_memes/cross_entropy/avg: 0.5163, train/total_loss: 0.5351, train/total_loss/avg: 0.5163, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 732ms, time_since_start: 13m 34s 148ms, eta: 02h 41m 35s 478ms\n","\u001b[32m2022-04-30T00:54:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.5351, train/hateful_memes/cross_entropy/avg: 0.5018, train/total_loss: 0.5351, train/total_loss/avg: 0.5018, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 022ms, time_since_start: 14m 21s 170ms, eta: 02h 41m 47s 804ms\n","\u001b[32m2022-04-30T00:55:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.5011, train/hateful_memes/cross_entropy/avg: 0.4952, train/total_loss: 0.5011, train/total_loss/avg: 0.4952, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 018ms, time_since_start: 15m 08s 189ms, eta: 02h 40m 59s 208ms\n","\u001b[32m2022-04-30T00:56:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.5011, train/hateful_memes/cross_entropy/avg: 0.4711, train/total_loss: 0.5011, train/total_loss/avg: 0.4711, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 988ms, time_since_start: 15m 55s 177ms, eta: 02h 40m 05s 272ms\n","\u001b[32m2022-04-30T00:57:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-30T00:57:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T00:57:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T00:57:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T00:57:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.4679, train/hateful_memes/cross_entropy/avg: 0.4600, train/total_loss: 0.4679, train/total_loss/avg: 0.4600, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 1.79, time: 56s 313ms, time_since_start: 16m 51s 491ms, eta: 03h 10m 54s 166ms\n","\u001b[32m2022-04-30T00:57:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-30T00:57:16 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-30T00:57:21 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-30T00:57:21 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-30T00:57:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T00:57:27 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-30T00:57:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T00:57:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T00:57:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.0824, val/total_loss: 1.0824, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4577, val/hateful_memes/roc_auc: 0.6916, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 21s 644ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.691618\n","\u001b[32m2022-04-30T00:58:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.4338, train/hateful_memes/cross_entropy/avg: 0.4468, train/total_loss: 0.4338, train/total_loss/avg: 0.4468, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 742ms, time_since_start: 18m 879ms, eta: 02h 41m 02s 268ms\n","\u001b[32m2022-04-30T00:59:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.3835, train/hateful_memes/cross_entropy/avg: 0.4303, train/total_loss: 0.3835, train/total_loss/avg: 0.4303, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 734ms, time_since_start: 18m 47s 613ms, eta: 02h 36m 50s 684ms\n","\u001b[32m2022-04-30T00:59:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.3789, train/hateful_memes/cross_entropy/avg: 0.4280, train/total_loss: 0.3789, train/total_loss/avg: 0.4280, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 017ms, time_since_start: 19m 34s 631ms, eta: 02h 36m 59s 961ms\n","\u001b[32m2022-04-30T01:00:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.3775, train/hateful_memes/cross_entropy/avg: 0.4139, train/total_loss: 0.3775, train/total_loss/avg: 0.4139, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 025ms, time_since_start: 20m 21s 656ms, eta: 02h 36m 13s 612ms\n","\u001b[32m2022-04-30T01:01:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.3321, train/hateful_memes/cross_entropy/avg: 0.3979, train/total_loss: 0.3321, train/total_loss/avg: 0.3979, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 230ms, time_since_start: 21m 08s 886ms, eta: 02h 36m 06s 458ms\n","\u001b[32m2022-04-30T01:02:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.3068, train/hateful_memes/cross_entropy/avg: 0.3840, train/total_loss: 0.3068, train/total_loss/avg: 0.3840, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 252ms, time_since_start: 21m 56s 139ms, eta: 02h 35m 22s 734ms\n","\u001b[32m2022-04-30T01:03:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.2701, train/hateful_memes/cross_entropy/avg: 0.3715, train/total_loss: 0.2701, train/total_loss/avg: 0.3715, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 857ms, time_since_start: 22m 42s 996ms, eta: 02h 33m 17s 310ms\n","\u001b[32m2022-04-30T01:03:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.2611, train/hateful_memes/cross_entropy/avg: 0.3613, train/total_loss: 0.2611, train/total_loss/avg: 0.3613, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 063ms, time_since_start: 23m 30s 060ms, eta: 02h 33m 09s 738ms\n","\u001b[32m2022-04-30T01:04:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.2488, train/hateful_memes/cross_entropy/avg: 0.3544, train/total_loss: 0.2488, train/total_loss/avg: 0.3544, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 095ms, time_since_start: 24m 17s 155ms, eta: 02h 32m 28s 080ms\n","\u001b[32m2022-04-30T01:05:28 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-30T01:05:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T01:05:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T01:05:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T01:05:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.1831, train/hateful_memes/cross_entropy/avg: 0.3432, train/total_loss: 0.1831, train/total_loss/avg: 0.3432, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 1.72, time: 58s 441ms, time_since_start: 25m 15s 596ms, eta: 03h 08m 12s 600ms\n","\u001b[32m2022-04-30T01:05:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-30T01:05:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-30T01:05:43 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-30T01:05:43 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-30T01:05:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T01:05:47 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-04-30T01:05:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T01:05:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T01:05:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.6292, val/total_loss: 1.6292, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.4645, val/hateful_memes/roc_auc: 0.7184, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 19s 157ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.718360\n","\u001b[32m2022-04-30T01:06:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.1613, train/hateful_memes/cross_entropy/avg: 0.3339, train/total_loss: 0.1613, train/total_loss/avg: 0.3339, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 881ms, time_since_start: 26m 22s 637ms, eta: 02h 33m 23s 482ms\n","\u001b[32m2022-04-30T01:07:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.0892, train/hateful_memes/cross_entropy/avg: 0.3239, train/total_loss: 0.0892, train/total_loss/avg: 0.3239, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 018ms, time_since_start: 27m 09s 655ms, eta: 02h 29m 49s 680ms\n","\u001b[32m2022-04-30T01:08:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0892, train/hateful_memes/cross_entropy/avg: 0.3181, train/total_loss: 0.0892, train/total_loss/avg: 0.3181, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 143ms, time_since_start: 27m 56s 799ms, eta: 02h 29m 25s 792ms\n","\u001b[32m2022-04-30T01:09:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0892, train/hateful_memes/cross_entropy/avg: 0.3129, train/total_loss: 0.0892, train/total_loss/avg: 0.3129, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 025ms, time_since_start: 28m 43s 824ms, eta: 02h 28m 15s 382ms\n","\u001b[32m2022-04-30T01:09:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0840, train/hateful_memes/cross_entropy/avg: 0.3040, train/total_loss: 0.0840, train/total_loss/avg: 0.3040, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 946ms, time_since_start: 29m 30s 771ms, eta: 02h 27m 12s 844ms\n","\u001b[32m2022-04-30T01:10:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0836, train/hateful_memes/cross_entropy/avg: 0.2956, train/total_loss: 0.0836, train/total_loss/avg: 0.2956, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 112ms, time_since_start: 30m 17s 884ms, eta: 02h 26m 56s 160ms\n","\u001b[32m2022-04-30T01:11:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0559, train/hateful_memes/cross_entropy/avg: 0.2877, train/total_loss: 0.0559, train/total_loss/avg: 0.2877, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 110ms, time_since_start: 31m 04s 995ms, eta: 02h 26m 07s 843ms\n","\u001b[32m2022-04-30T01:12:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0559, train/hateful_memes/cross_entropy/avg: 0.2833, train/total_loss: 0.0559, train/total_loss/avg: 0.2833, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 548ms, time_since_start: 31m 51s 543ms, eta: 02h 23m 35s 826ms\n","\u001b[32m2022-04-30T01:13:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0559, train/hateful_memes/cross_entropy/avg: 0.2762, train/total_loss: 0.0559, train/total_loss/avg: 0.2762, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 030ms, time_since_start: 32m 38s 574ms, eta: 02h 24m 17s 214ms\n","\u001b[32m2022-04-30T01:13:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-30T01:13:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T01:13:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T01:13:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T01:13:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0476, train/hateful_memes/cross_entropy/avg: 0.2694, train/total_loss: 0.0476, train/total_loss/avg: 0.2694, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 737ms, time_since_start: 33m 34s 312ms, eta: 02h 50m 03s 365ms\n","\u001b[32m2022-04-30T01:13:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-30T01:13:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-30T01:14:02 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-30T01:14:02 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-30T01:14:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T01:14:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T01:14:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T01:14:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.9777, val/total_loss: 1.9777, val/hateful_memes/accuracy: 0.7000, val/hateful_memes/binary_f1: 0.4808, val/hateful_memes/roc_auc: 0.7097, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 12s 458ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.718360\n","\u001b[32m2022-04-30T01:14:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0372, train/hateful_memes/cross_entropy/avg: 0.2630, train/total_loss: 0.0372, train/total_loss/avg: 0.2630, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 751ms, time_since_start: 34m 34s 523ms, eta: 02h 24m 52s 816ms\n","\u001b[32m2022-04-30T01:15:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0372, train/hateful_memes/cross_entropy/avg: 0.2576, train/total_loss: 0.0372, train/total_loss/avg: 0.2576, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 171ms, time_since_start: 35m 21s 694ms, eta: 02h 22m 19s 201ms\n","\u001b[32m2022-04-30T01:16:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0184, train/hateful_memes/cross_entropy/avg: 0.2517, train/total_loss: 0.0184, train/total_loss/avg: 0.2517, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 818ms, time_since_start: 36m 08s 513ms, eta: 02h 20m 27s 765ms\n","\u001b[32m2022-04-30T01:17:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0141, train/hateful_memes/cross_entropy/avg: 0.2461, train/total_loss: 0.0141, train/total_loss/avg: 0.2461, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 245ms, time_since_start: 36m 55s 758ms, eta: 02h 20m 56s 482ms\n","\u001b[32m2022-04-30T01:18:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0141, train/hateful_memes/cross_entropy/avg: 0.2408, train/total_loss: 0.0141, train/total_loss/avg: 0.2408, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 191ms, time_since_start: 37m 42s 949ms, eta: 02h 19m 58s 871ms\n","\u001b[32m2022-04-30T01:18:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0105, train/hateful_memes/cross_entropy/avg: 0.2358, train/total_loss: 0.0105, train/total_loss/avg: 0.2358, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 166ms, time_since_start: 38m 30s 116ms, eta: 02h 19m 06s 512ms\n","\u001b[32m2022-04-30T01:19:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0105, train/hateful_memes/cross_entropy/avg: 0.2313, train/total_loss: 0.0105, train/total_loss/avg: 0.2313, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 202ms, time_since_start: 39m 17s 318ms, eta: 02h 18m 24s 862ms\n","\u001b[32m2022-04-30T01:20:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.2267, train/total_loss: 0.0089, train/total_loss/avg: 0.2267, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 785ms, time_since_start: 40m 04s 104ms, eta: 02h 16m 23s 920ms\n","\u001b[32m2022-04-30T01:21:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.2222, train/total_loss: 0.0089, train/total_loss/avg: 0.2222, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 162ms, time_since_start: 40m 51s 266ms, eta: 02h 16m 41s 813ms\n","\u001b[32m2022-04-30T01:22:03 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-30T01:22:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T01:22:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T01:22:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T01:22:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.2179, train/total_loss: 0.0068, train/total_loss/avg: 0.2179, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 1.75, time: 57s 861ms, time_since_start: 41m 49s 128ms, eta: 02h 46m 43s 743ms\n","\u001b[32m2022-04-30T01:22:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-30T01:22:13 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-30T01:22:16 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-30T01:22:16 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-30T01:22:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T01:22:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T01:22:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T01:22:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.8764, val/total_loss: 1.8764, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.5015, val/hateful_memes/roc_auc: 0.7088, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 15s 637ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.718360\n","\u001b[32m2022-04-30T01:23:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.2136, train/total_loss: 0.0067, train/total_loss/avg: 0.2136, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 528ms, time_since_start: 42m 53s 296ms, eta: 02h 19m 802ms\n","\u001b[32m2022-04-30T01:24:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.2099, train/total_loss: 0.0067, train/total_loss/avg: 0.2099, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 839ms, time_since_start: 43m 41s 136ms, eta: 02h 16m 13s 725ms\n","\u001b[32m2022-04-30T01:24:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0066, train/hateful_memes/cross_entropy/avg: 0.2060, train/total_loss: 0.0066, train/total_loss/avg: 0.2060, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 475ms, time_since_start: 44m 28s 611ms, eta: 02h 14m 23s 168ms\n","\u001b[32m2022-04-30T01:25:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.2023, train/total_loss: 0.0065, train/total_loss/avg: 0.2023, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 186ms, time_since_start: 45m 15s 798ms, eta: 02h 12m 46s 126ms\n","\u001b[32m2022-04-30T01:26:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0066, train/hateful_memes/cross_entropy/avg: 0.1995, train/total_loss: 0.0066, train/total_loss/avg: 0.1995, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 738ms, time_since_start: 46m 03s 536ms, eta: 02h 13m 30s 769ms\n","\u001b[32m2022-04-30T01:27:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1962, train/total_loss: 0.0067, train/total_loss/avg: 0.1962, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 707ms, time_since_start: 46m 51s 244ms, eta: 02h 12m 36s 996ms\n","\u001b[32m2022-04-30T01:28:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1934, train/total_loss: 0.0068, train/total_loss/avg: 0.1934, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 633ms, time_since_start: 47m 38s 877ms, eta: 02h 11m 36s 231ms\n","\u001b[32m2022-04-30T01:28:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1901, train/total_loss: 0.0067, train/total_loss/avg: 0.1901, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 596ms, time_since_start: 48m 26s 473ms, eta: 02h 10m 41s 653ms\n","\u001b[32m2022-04-30T01:29:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1869, train/total_loss: 0.0067, train/total_loss/avg: 0.1869, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 034ms, time_since_start: 49m 13s 507ms, eta: 02h 08m 21s 233ms\n","\u001b[32m2022-04-30T01:30:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-30T01:30:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T01:30:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T01:30:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T01:30:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1837, train/total_loss: 0.0067, train/total_loss/avg: 0.1837, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 1.79, time: 56s 543ms, time_since_start: 50m 10s 051ms, eta: 02h 33m 20s 755ms\n","\u001b[32m2022-04-30T01:30:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-30T01:30:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-30T01:30:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-30T01:30:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-30T01:30:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T01:30:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T01:30:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T01:30:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.2599, val/total_loss: 2.2599, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.4625, val/hateful_memes/roc_auc: 0.7028, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 13s 919ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.718360\n","\u001b[32m2022-04-30T01:31:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1822, train/total_loss: 0.0068, train/total_loss/avg: 0.1822, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 197ms, time_since_start: 51m 12s 170ms, eta: 02h 09m 53s 724ms\n","\u001b[32m2022-04-30T01:32:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1807, train/total_loss: 0.0068, train/total_loss/avg: 0.1807, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 611ms, time_since_start: 51m 59s 781ms, eta: 02h 07m 30s 452ms\n","\u001b[32m2022-04-30T01:33:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1778, train/total_loss: 0.0068, train/total_loss/avg: 0.1778, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 673ms, time_since_start: 52m 47s 455ms, eta: 02h 06m 51s 983ms\n","\u001b[32m2022-04-30T01:33:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1751, train/total_loss: 0.0068, train/total_loss/avg: 0.1751, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 157ms, time_since_start: 53m 34s 612ms, eta: 02h 04m 41s 595ms\n","\u001b[32m2022-04-30T01:34:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1733, train/total_loss: 0.0068, train/total_loss/avg: 0.1733, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 453ms, time_since_start: 54m 22s 066ms, eta: 02h 04m 40s 350ms\n","\u001b[32m2022-04-30T01:35:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1707, train/total_loss: 0.0065, train/total_loss/avg: 0.1707, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 402ms, time_since_start: 55m 09s 468ms, eta: 02h 03m 44s 094ms\n","\u001b[32m2022-04-30T01:36:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0057, train/hateful_memes/cross_entropy/avg: 0.1682, train/total_loss: 0.0057, train/total_loss/avg: 0.1682, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 322ms, time_since_start: 55m 56s 790ms, eta: 02h 02m 43s 375ms\n","\u001b[32m2022-04-30T01:37:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1657, train/total_loss: 0.0043, train/total_loss/avg: 0.1657, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 468ms, time_since_start: 56m 44s 258ms, eta: 02h 02m 17s 807ms\n","\u001b[32m2022-04-30T01:37:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1634, train/total_loss: 0.0043, train/total_loss/avg: 0.1634, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 469ms, time_since_start: 57m 31s 728ms, eta: 02h 01m 29s 680ms\n","\u001b[32m2022-04-30T01:38:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-30T01:38:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T01:38:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T01:38:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T01:38:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.1610, train/total_loss: 0.0029, train/total_loss/avg: 0.1610, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 766ms, time_since_start: 58m 27s 494ms, eta: 02h 21m 47s 177ms\n","\u001b[32m2022-04-30T01:38:52 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-30T01:38:52 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-30T01:38:55 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-30T01:38:55 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-30T01:38:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T01:38:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T01:39:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T01:39:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.3457, val/total_loss: 2.3457, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.4641, val/hateful_memes/roc_auc: 0.7020, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 12s 597ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.718360\n","\u001b[32m2022-04-30T01:39:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.1588, train/total_loss: 0.0029, train/total_loss/avg: 0.1588, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 062ms, time_since_start: 59m 28s 155ms, eta: 02h 01m 22s 985ms\n","\u001b[32m2022-04-30T01:40:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.1567, train/total_loss: 0.0029, train/total_loss/avg: 0.1567, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 544ms, time_since_start: 01h 15s 699ms, eta: 01h 59m 16s 207ms\n","\u001b[32m2022-04-30T01:41:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1545, train/total_loss: 0.0025, train/total_loss/avg: 0.1545, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 461ms, time_since_start: 01h 01m 03s 160ms, eta: 01h 58m 15s 384ms\n","\u001b[32m2022-04-30T01:42:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1527, train/total_loss: 0.0025, train/total_loss/avg: 0.1527, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 460ms, time_since_start: 01h 01m 50s 621ms, eta: 01h 57m 27s 104ms\n","\u001b[32m2022-04-30T01:43:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1507, train/total_loss: 0.0017, train/total_loss/avg: 0.1507, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 305ms, time_since_start: 01h 02m 37s 927ms, eta: 01h 56m 15s 916ms\n","\u001b[32m2022-04-30T01:43:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1487, train/total_loss: 0.0012, train/total_loss/avg: 0.1487, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 647ms, time_since_start: 01h 03m 25s 574ms, eta: 01h 56m 17s 848ms\n","\u001b[32m2022-04-30T01:44:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1471, train/total_loss: 0.0012, train/total_loss/avg: 0.1471, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 808ms, time_since_start: 01h 04m 13s 383ms, eta: 01h 55m 52s 869ms\n","\u001b[32m2022-04-30T01:45:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1453, train/total_loss: 0.0017, train/total_loss/avg: 0.1453, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 593ms, time_since_start: 01h 05m 976ms, eta: 01h 54m 33s 119ms\n","\u001b[32m2022-04-30T01:46:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1435, train/total_loss: 0.0017, train/total_loss/avg: 0.1435, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 761ms, time_since_start: 01h 05m 48s 738ms, eta: 01h 54m 08s 918ms\n","\u001b[32m2022-04-30T01:47:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-04-30T01:47:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-04-30T01:47:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-04-30T01:47:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-04-30T01:47:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1417, train/total_loss: 0.0017, train/total_loss/avg: 0.1417, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 293ms, time_since_start: 01h 06m 44s 032ms, eta: 02h 11m 12s 745ms\n","\u001b[32m2022-04-30T01:47:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-04-30T01:47:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-04-30T01:47:11 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-04-30T01:47:11 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-30T01:47:11 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-04-30T01:47:11 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-04-30T01:47:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-30T01:47:20 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n","\u001b[32m2022-04-30T01:47:20 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n","\u001b[32m2022-04-30T01:47:20 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 12\n","\u001b[32m2022-04-30T01:47:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 2.3251, val/total_loss: 2.3251, val/hateful_memes/accuracy: 0.7000, val/hateful_memes/binary_f1: 0.4808, val/hateful_memes/roc_auc: 0.7092, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 13s 125ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.718360\n","\u001b[32m2022-04-30T01:47:22 | mmf.trainers.core.training_loop: \u001b[0mEarly stopping activated\n","\u001b[32m2022-04-30T01:47:22 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2022-04-30T01:47:22 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-04-30T01:47:22 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-04-30T01:47:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-04-30T01:47:24 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n","\u001b[32m2022-04-30T01:47:24 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n","\u001b[32m2022-04-30T01:47:24 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 12\n","\u001b[32m2022-04-30T01:47:25 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-04-30T01:47:25 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:14<00:00,  4.25it/s]\n","\u001b[32m2022-04-30T01:47:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 63\n","\u001b[32m2022-04-30T01:47:40 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-04-30T01:47:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, test/hateful_memes/cross_entropy: 1.5585, test/total_loss: 1.5585, test/hateful_memes/accuracy: 0.6945, test/hateful_memes/binary_f1: 0.4826, test/hateful_memes/roc_auc: 0.7435\n","\u001b[32m2022-04-30T01:47:40 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 07m 15s 368ms\n"]}],"source":["!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_lr1_da.yml \\\n","training.batch_size=32 \\\n","model=visual_bert dataset=hateful_memes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYFSe27FMTyd"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"E1A0TZZXSYT9"},"source":["# **Visual Bert COCO bs 32 da xlm**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":418159,"status":"ok","timestamp":1651390772288,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"MFfKWrMxSq6e","outputId":"f8218845-a60b-4847-a4f8-0a80edfbefd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-01T06:12:34 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf/projects/hateful_memes/configs/visual_bert/from_coco_da_loss.yml\n","\u001b[32m2022-05-01T06:12:34 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n","\u001b[32m2022-05-01T06:12:34 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-05-01T06:12:34 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-01T06:12:34 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-01T06:12:35 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf/projects/hateful_memes/configs/visual_bert/from_coco_da_loss.yml', 'training.batch_size=32', 'model=visual_bert', 'dataset=hateful_memes'])\n","\u001b[32m2022-05-01T06:12:35 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-01T06:12:35 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-05-01T06:12:35 | mmf_cli.run: \u001b[0mUsing seed 48425275\n","\u001b[32m2022-05-01T06:12:35 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [07:25<00:00, 23.1MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 170kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpe9jjeg6e\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 24.7kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpd9ispskx\n","Downloading: 100% 570/570 [00:00<00:00, 460kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpd67m4uc9\n","Downloading: 100% 232k/232k [00:00<00:00, 687kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0ngd3n4t\n","Downloading: 100% 466k/466k [00:00<00:00, 1.12MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-01T06:22:49 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-01T06:22:49 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-01T06:22:49 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-01T06:22:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"soft_label_cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmp8_mlpssx\n","Downloading: 100% 440M/440M [00:06<00:00, 69.1MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-05-01T06:23:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-01T06:23:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-01T06:23:07 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n","Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:38<00:00, 10.7MB/s]\n","[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n","Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-01T06:23:52 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-01T06:23:52 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-01T06:23:52 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-01T06:23:52 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2022-05-01T06:23:52 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-05-01T06:23:52 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): SoftLabelCrossEntropyLoss()\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-05-01T06:23:52 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-05-01T06:23:52 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-05-01T06:24:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/soft_label_cross_entropy: 0.7083, train/hateful_memes/soft_label_cross_entropy/avg: 0.7083, train/total_loss: 0.7083, train/total_loss/avg: 0.7083, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 2.04, time: 49s 683ms, time_since_start: 01m 34s 676ms, eta: 03h 04m 25s 557ms\n","\u001b[32m2022-05-01T06:25:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/soft_label_cross_entropy: 0.6679, train/hateful_memes/soft_label_cross_entropy/avg: 0.6881, train/total_loss: 0.6679, train/total_loss/avg: 0.6881, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 187ms, time_since_start: 02m 21s 864ms, eta: 02h 54m 21s 798ms\n","\u001b[32m2022-05-01T06:26:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/soft_label_cross_entropy: 0.6679, train/hateful_memes/soft_label_cross_entropy/avg: 0.6540, train/total_loss: 0.6679, train/total_loss/avg: 0.6540, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 890ms, time_since_start: 03m 08s 754ms, eta: 02h 52m 28s 150ms\n","\u001b[32m2022-05-01T06:27:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/soft_label_cross_entropy: 0.5859, train/hateful_memes/soft_label_cross_entropy/avg: 0.6310, train/total_loss: 0.5859, train/total_loss/avg: 0.6310, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 107ms, time_since_start: 03m 55s 862ms, eta: 02h 52m 28s 265ms\n","\u001b[32m2022-05-01T06:27:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/soft_label_cross_entropy: 0.5859, train/hateful_memes/soft_label_cross_entropy/avg: 0.6070, train/total_loss: 0.5859, train/total_loss/avg: 0.6070, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 327ms, time_since_start: 04m 43s 189ms, eta: 02h 52m 28s 378ms\n","\u001b[32m2022-05-01T06:28:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/soft_label_cross_entropy: 0.5619, train/hateful_memes/soft_label_cross_entropy/avg: 0.5775, train/total_loss: 0.5619, train/total_loss/avg: 0.5775, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 989ms, time_since_start: 05m 30s 178ms, eta: 02h 50m 26s 639ms\n","\u001b[32m2022-05-01T06:29:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/soft_label_cross_entropy: 0.5619, train/hateful_memes/soft_label_cross_entropy/avg: 0.5611, train/total_loss: 0.5619, train/total_loss/avg: 0.5611, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 069ms, time_since_start: 06m 17s 248ms, eta: 02h 49m 56s 263ms\n","\u001b[32m2022-05-01T06:30:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/soft_label_cross_entropy: 0.5109, train/hateful_memes/soft_label_cross_entropy/avg: 0.5311, train/total_loss: 0.5109, train/total_loss/avg: 0.5311, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 880ms, time_since_start: 07m 04s 129ms, eta: 02h 48m 27s 710ms\n","\u001b[32m2022-05-01T06:30:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/soft_label_cross_entropy: 0.5109, train/hateful_memes/soft_label_cross_entropy/avg: 0.5076, train/total_loss: 0.5109, train/total_loss/avg: 0.5076, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 237ms, time_since_start: 07m 51s 367ms, eta: 02h 48m 56s 590ms\n","\u001b[32m2022-05-01T06:31:46 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T06:31:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T06:31:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T06:31:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T06:31:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/soft_label_cross_entropy: 0.4625, train/hateful_memes/soft_label_cross_entropy/avg: 0.4956, train/total_loss: 0.4625, train/total_loss/avg: 0.4956, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.69, time: 59s 239ms, time_since_start: 08m 50s 606ms, eta: 03h 30m 51s 749ms\n","\u001b[32m2022-05-01T06:31:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T06:31:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T06:32:02 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T06:32:02 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T06:32:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T06:32:07 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-01T06:32:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T06:32:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T06:32:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/soft_label_cross_entropy: 0.8796, val/total_loss: 0.8796, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4750, val/hateful_memes/roc_auc: 0.7010, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 22s 653ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.700985\n","\u001b[32m2022-05-01T06:33:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/soft_label_cross_entropy: 0.4625, train/hateful_memes/soft_label_cross_entropy/avg: 0.4579, train/total_loss: 0.4625, train/total_loss/avg: 0.4579, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 013ms, time_since_start: 10m 01s 274ms, eta: 02h 50m 05s 359ms\n","\u001b[32m2022-05-01T06:33:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/soft_label_cross_entropy: 0.4302, train/hateful_memes/soft_label_cross_entropy/avg: 0.4544, train/total_loss: 0.4302, train/total_loss/avg: 0.4544, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 280ms, time_since_start: 10m 48s 555ms, eta: 02h 46m 41s 630ms\n","\u001b[32m2022-05-01T06:34:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/soft_label_cross_entropy: 0.4302, train/hateful_memes/soft_label_cross_entropy/avg: 0.4378, train/total_loss: 0.4302, train/total_loss/avg: 0.4378, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 217ms, time_since_start: 11m 35s 773ms, eta: 02h 45m 40s 234ms\n","\u001b[32m2022-05-01T06:35:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/soft_label_cross_entropy: 0.4160, train/hateful_memes/soft_label_cross_entropy/avg: 0.4115, train/total_loss: 0.4160, train/total_loss/avg: 0.4115, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 158ms, time_since_start: 12m 22s 931ms, eta: 02h 44m 39s 770ms\n","\u001b[32m2022-05-01T06:36:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/soft_label_cross_entropy: 0.4160, train/hateful_memes/soft_label_cross_entropy/avg: 0.4052, train/total_loss: 0.4160, train/total_loss/avg: 0.4052, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 387ms, time_since_start: 13m 10s 319ms, eta: 02h 44m 39s 684ms\n","\u001b[32m2022-05-01T06:37:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/soft_label_cross_entropy: 0.3871, train/hateful_memes/soft_label_cross_entropy/avg: 0.3872, train/total_loss: 0.3871, train/total_loss/avg: 0.3872, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 041ms, time_since_start: 13m 57s 361ms, eta: 02h 42m 39s 666ms\n","\u001b[32m2022-05-01T06:37:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/soft_label_cross_entropy: 0.3871, train/hateful_memes/soft_label_cross_entropy/avg: 0.3728, train/total_loss: 0.3871, train/total_loss/avg: 0.3728, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 344ms, time_since_start: 14m 44s 705ms, eta: 02h 42m 54s 324ms\n","\u001b[32m2022-05-01T06:38:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/soft_label_cross_entropy: 0.3214, train/hateful_memes/soft_label_cross_entropy/avg: 0.3606, train/total_loss: 0.3214, train/total_loss/avg: 0.3606, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 363ms, time_since_start: 15m 32s 069ms, eta: 02h 42m 10s 061ms\n","\u001b[32m2022-05-01T06:39:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/soft_label_cross_entropy: 0.3214, train/hateful_memes/soft_label_cross_entropy/avg: 0.3437, train/total_loss: 0.3214, train/total_loss/avg: 0.3437, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 120ms, time_since_start: 16m 19s 189ms, eta: 02h 40m 32s 230ms\n","\u001b[32m2022-05-01T06:40:14 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T06:40:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T06:40:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T06:40:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T06:40:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/soft_label_cross_entropy: 0.3199, train/hateful_memes/soft_label_cross_entropy/avg: 0.3306, train/total_loss: 0.3199, train/total_loss/avg: 0.3306, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.79, time: 56s 593ms, time_since_start: 17m 15s 783ms, eta: 03h 11m 51s 101ms\n","\u001b[32m2022-05-01T06:40:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T06:40:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T06:40:28 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T06:40:28 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T06:40:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T06:40:34 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-01T06:40:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T06:40:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T06:40:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/soft_label_cross_entropy: 1.3422, val/total_loss: 1.3422, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.4889, val/hateful_memes/roc_auc: 0.7013, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 21s 867ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.701309\n","\u001b[32m2022-05-01T06:41:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/soft_label_cross_entropy: 0.3168, train/hateful_memes/soft_label_cross_entropy/avg: 0.3182, train/total_loss: 0.3168, train/total_loss/avg: 0.3182, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 040ms, time_since_start: 18m 25s 692ms, eta: 02h 42m 02s 584ms\n","\u001b[32m2022-05-01T06:42:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/soft_label_cross_entropy: 0.2384, train/hateful_memes/soft_label_cross_entropy/avg: 0.3066, train/total_loss: 0.2384, train/total_loss/avg: 0.3066, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 2.17, time: 46s 920ms, time_since_start: 19m 12s 613ms, eta: 02h 37m 28s 159ms\n","\u001b[32m2022-05-01T06:43:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/soft_label_cross_entropy: 0.1534, train/hateful_memes/soft_label_cross_entropy/avg: 0.2949, train/total_loss: 0.1534, train/total_loss/avg: 0.2949, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 354ms, time_since_start: 19m 59s 967ms, eta: 02h 38m 07s 420ms\n","\u001b[32m2022-05-01T06:43:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/soft_label_cross_entropy: 0.1431, train/hateful_memes/soft_label_cross_entropy/avg: 0.2844, train/total_loss: 0.1431, train/total_loss/avg: 0.2844, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 278ms, time_since_start: 20m 47s 246ms, eta: 02h 37m 04s 084ms\n","\u001b[32m2022-05-01T06:44:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/soft_label_cross_entropy: 0.1168, train/hateful_memes/soft_label_cross_entropy/avg: 0.2749, train/total_loss: 0.1168, train/total_loss/avg: 0.2749, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 450ms, time_since_start: 21m 34s 696ms, eta: 02h 36m 50s 133ms\n","\u001b[32m2022-05-01T06:45:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0812, train/hateful_memes/soft_label_cross_entropy/avg: 0.2647, train/total_loss: 0.0812, train/total_loss/avg: 0.2647, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 372ms, time_since_start: 22m 22s 069ms, eta: 02h 35m 46s 568ms\n","\u001b[32m2022-05-01T06:46:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0807, train/hateful_memes/soft_label_cross_entropy/avg: 0.2554, train/total_loss: 0.0807, train/total_loss/avg: 0.2554, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 011ms, time_since_start: 23m 09s 081ms, eta: 02h 33m 47s 516ms\n","\u001b[32m2022-05-01T06:47:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0717, train/hateful_memes/soft_label_cross_entropy/avg: 0.2477, train/total_loss: 0.0717, train/total_loss/avg: 0.2477, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 336ms, time_since_start: 23m 56s 417ms, eta: 02h 34m 03s 042ms\n","\u001b[32m2022-05-01T06:47:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0695, train/hateful_memes/soft_label_cross_entropy/avg: 0.2397, train/total_loss: 0.0695, train/total_loss/avg: 0.2397, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 323ms, time_since_start: 24m 43s 741ms, eta: 02h 33m 12s 548ms\n","\u001b[32m2022-05-01T06:48:38 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T06:48:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T06:48:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T06:48:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T06:48:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0623, train/hateful_memes/soft_label_cross_entropy/avg: 0.2318, train/total_loss: 0.0623, train/total_loss/avg: 0.2318, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.79, time: 56s 760ms, time_since_start: 25m 40s 501ms, eta: 03h 02m 47s 772ms\n","\u001b[32m2022-05-01T06:48:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T06:48:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T06:48:51 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T06:48:52 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T06:48:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T06:48:56 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-01T06:49:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T06:49:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T06:49:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/soft_label_cross_entropy: 1.5267, val/total_loss: 1.5267, val/hateful_memes/accuracy: 0.7259, val/hateful_memes/binary_f1: 0.5488, val/hateful_memes/roc_auc: 0.7264, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 23s 531ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.726426\n","\u001b[32m2022-05-01T06:49:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0473, train/hateful_memes/soft_label_cross_entropy/avg: 0.2252, train/total_loss: 0.0473, train/total_loss/avg: 0.2252, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 762ms, time_since_start: 26m 51s 797ms, eta: 02h 33m 582ms\n","\u001b[32m2022-05-01T06:50:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0433, train/hateful_memes/soft_label_cross_entropy/avg: 0.2187, train/total_loss: 0.0433, train/total_loss/avg: 0.2187, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 035ms, time_since_start: 27m 38s 832ms, eta: 02h 29m 53s 012ms\n","\u001b[32m2022-05-01T06:51:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0396, train/hateful_memes/soft_label_cross_entropy/avg: 0.2122, train/total_loss: 0.0396, train/total_loss/avg: 0.2122, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 147ms, time_since_start: 28m 25s 980ms, eta: 02h 29m 26s 510ms\n","\u001b[32m2022-05-01T06:52:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0395, train/hateful_memes/soft_label_cross_entropy/avg: 0.2064, train/total_loss: 0.0395, train/total_loss/avg: 0.2064, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 259ms, time_since_start: 29m 13s 239ms, eta: 02h 28m 59s 668ms\n","\u001b[32m2022-05-01T06:53:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0395, train/hateful_memes/soft_label_cross_entropy/avg: 0.2076, train/total_loss: 0.0395, train/total_loss/avg: 0.2076, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 223ms, time_since_start: 30m 463ms, eta: 02h 28m 04s 944ms\n","\u001b[32m2022-05-01T06:53:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0370, train/hateful_memes/soft_label_cross_entropy/avg: 0.2021, train/total_loss: 0.0370, train/total_loss/avg: 0.2021, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 324ms, time_since_start: 30m 47s 788ms, eta: 02h 27m 35s 724ms\n","\u001b[32m2022-05-01T06:54:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0264, train/hateful_memes/soft_label_cross_entropy/avg: 0.1966, train/total_loss: 0.0264, train/total_loss/avg: 0.1966, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 268ms, time_since_start: 31m 35s 057ms, eta: 02h 26m 37s 260ms\n","\u001b[32m2022-05-01T06:55:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0200, train/hateful_memes/soft_label_cross_entropy/avg: 0.1918, train/total_loss: 0.0200, train/total_loss/avg: 0.1918, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 2.17, time: 46s 831ms, time_since_start: 32m 21s 888ms, eta: 02h 24m 28s 237ms\n","\u001b[32m2022-05-01T06:56:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0178, train/hateful_memes/soft_label_cross_entropy/avg: 0.1869, train/total_loss: 0.0178, train/total_loss/avg: 0.1869, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 265ms, time_since_start: 33m 09s 154ms, eta: 02h 25m 582ms\n","\u001b[32m2022-05-01T06:57:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T06:57:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T06:57:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T06:57:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T06:57:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0178, train/hateful_memes/soft_label_cross_entropy/avg: 0.1829, train/total_loss: 0.0178, train/total_loss/avg: 0.1829, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.75, time: 57s 830ms, time_since_start: 34m 06s 984ms, eta: 02h 56m 26s 385ms\n","\u001b[32m2022-05-01T06:57:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T06:57:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T06:57:17 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T06:57:17 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T06:57:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T06:57:22 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-01T06:57:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T06:57:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T06:57:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/soft_label_cross_entropy: 1.7098, val/total_loss: 1.7098, val/hateful_memes/accuracy: 0.7093, val/hateful_memes/binary_f1: 0.4291, val/hateful_memes/roc_auc: 0.7399, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 20s 760ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.739945\n","\u001b[32m2022-05-01T06:58:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0178, train/hateful_memes/soft_label_cross_entropy/avg: 0.1798, train/total_loss: 0.0178, train/total_loss/avg: 0.1798, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 031ms, time_since_start: 35m 15s 778ms, eta: 02h 25m 43s 835ms\n","\u001b[32m2022-05-01T06:59:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0150, train/hateful_memes/soft_label_cross_entropy/avg: 0.1756, train/total_loss: 0.0150, train/total_loss/avg: 0.1756, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 537ms, time_since_start: 36m 03s 316ms, eta: 02h 23m 25s 571ms\n","\u001b[32m2022-05-01T06:59:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0128, train/hateful_memes/soft_label_cross_entropy/avg: 0.1715, train/total_loss: 0.0128, train/total_loss/avg: 0.1715, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 006ms, time_since_start: 36m 50s 323ms, eta: 02h 21m 01s 644ms\n","\u001b[32m2022-05-01T07:00:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0128, train/hateful_memes/soft_label_cross_entropy/avg: 0.1680, train/total_loss: 0.0128, train/total_loss/avg: 0.1680, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 388ms, time_since_start: 37m 37s 711ms, eta: 02h 21m 22s 127ms\n","\u001b[32m2022-05-01T07:01:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0126, train/hateful_memes/soft_label_cross_entropy/avg: 0.1645, train/total_loss: 0.0126, train/total_loss/avg: 0.1645, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 446ms, time_since_start: 38m 25s 158ms, eta: 02h 20m 44s 272ms\n","\u001b[32m2022-05-01T07:02:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0126, train/hateful_memes/soft_label_cross_entropy/avg: 0.1610, train/total_loss: 0.0126, train/total_loss/avg: 0.1610, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 257ms, time_since_start: 39m 12s 415ms, eta: 02h 19m 22s 592ms\n","\u001b[32m2022-05-01T07:03:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0128, train/hateful_memes/soft_label_cross_entropy/avg: 0.1581, train/total_loss: 0.0128, train/total_loss/avg: 0.1581, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 522ms, time_since_start: 39m 59s 938ms, eta: 02h 19m 21s 184ms\n","\u001b[32m2022-05-01T07:03:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0128, train/hateful_memes/soft_label_cross_entropy/avg: 0.1551, train/total_loss: 0.0128, train/total_loss/avg: 0.1551, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 038ms, time_since_start: 40m 46s 977ms, eta: 02h 17m 08s 208ms\n","\u001b[32m2022-05-01T07:04:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0095, train/hateful_memes/soft_label_cross_entropy/avg: 0.1520, train/total_loss: 0.0095, train/total_loss/avg: 0.1520, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 441ms, time_since_start: 41m 34s 418ms, eta: 02h 17m 30s 380ms\n","\u001b[32m2022-05-01T07:05:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T07:05:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T07:05:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T07:05:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T07:05:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0095, train/hateful_memes/soft_label_cross_entropy/avg: 0.1490, train/total_loss: 0.0095, train/total_loss/avg: 0.1490, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.82, time: 55s 661ms, time_since_start: 42m 30s 079ms, eta: 02h 40m 23s 271ms\n","\u001b[32m2022-05-01T07:05:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T07:05:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T07:05:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T07:05:40 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T07:05:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T07:05:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T07:05:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T07:05:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/soft_label_cross_entropy: 1.3961, val/total_loss: 1.3961, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.5015, val/hateful_memes/roc_auc: 0.7294, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 17s 680ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.739945\n","\u001b[32m2022-05-01T07:06:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0083, train/hateful_memes/soft_label_cross_entropy/avg: 0.1462, train/total_loss: 0.0083, train/total_loss/avg: 0.1462, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 057ms, time_since_start: 43m 35s 819ms, eta: 02h 17m 39s 831ms\n","\u001b[32m2022-05-01T07:07:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0046, train/hateful_memes/soft_label_cross_entropy/avg: 0.1434, train/total_loss: 0.0046, train/total_loss/avg: 0.1434, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 566ms, time_since_start: 44m 23s 385ms, eta: 02h 15m 26s 948ms\n","\u001b[32m2022-05-01T07:08:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0083, train/hateful_memes/soft_label_cross_entropy/avg: 0.1417, train/total_loss: 0.0083, train/total_loss/avg: 0.1417, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 485ms, time_since_start: 45m 10s 870ms, eta: 02h 14m 24s 832ms\n","\u001b[32m2022-05-01T07:09:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0083, train/hateful_memes/soft_label_cross_entropy/avg: 0.1393, train/total_loss: 0.0083, train/total_loss/avg: 0.1393, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 183ms, time_since_start: 45m 58s 054ms, eta: 02h 12m 45s 596ms\n","\u001b[32m2022-05-01T07:09:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0051, train/hateful_memes/soft_label_cross_entropy/avg: 0.1368, train/total_loss: 0.0051, train/total_loss/avg: 0.1368, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 515ms, time_since_start: 46m 45s 569ms, eta: 02h 12m 53s 322ms\n","\u001b[32m2022-05-01T07:10:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0051, train/hateful_memes/soft_label_cross_entropy/avg: 0.1346, train/total_loss: 0.0051, train/total_loss/avg: 0.1346, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 611ms, time_since_start: 47m 33s 181ms, eta: 02h 12m 21s 024ms\n","\u001b[32m2022-05-01T07:11:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0089, train/hateful_memes/soft_label_cross_entropy/avg: 0.1358, train/total_loss: 0.0089, train/total_loss/avg: 0.1358, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 591ms, time_since_start: 48m 20s 773ms, eta: 02h 11m 29s 352ms\n","\u001b[32m2022-05-01T07:12:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0069, train/hateful_memes/soft_label_cross_entropy/avg: 0.1336, train/total_loss: 0.0069, train/total_loss/avg: 0.1336, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 595ms, time_since_start: 49m 08s 368ms, eta: 02h 10m 41s 578ms\n","\u001b[32m2022-05-01T07:13:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0069, train/hateful_memes/soft_label_cross_entropy/avg: 0.1313, train/total_loss: 0.0069, train/total_loss/avg: 0.1313, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 051ms, time_since_start: 49m 55s 420ms, eta: 02h 08m 24s 086ms\n","\u001b[32m2022-05-01T07:13:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T07:13:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T07:13:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T07:14:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T07:14:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0061, train/hateful_memes/soft_label_cross_entropy/avg: 0.1292, train/total_loss: 0.0061, train/total_loss/avg: 0.1292, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.75, time: 57s 343ms, time_since_start: 50m 52s 763ms, eta: 02h 35m 30s 944ms\n","\u001b[32m2022-05-01T07:14:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T07:14:00 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T07:14:03 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T07:14:03 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T07:14:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T07:14:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T07:14:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T07:14:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/soft_label_cross_entropy: 1.4577, val/total_loss: 1.4577, val/hateful_memes/accuracy: 0.7204, val/hateful_memes/binary_f1: 0.5237, val/hateful_memes/roc_auc: 0.7135, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 13s 666ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.739945\n","\u001b[32m2022-05-01T07:15:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0058, train/hateful_memes/soft_label_cross_entropy/avg: 0.1272, train/total_loss: 0.0058, train/total_loss/avg: 0.1272, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 320ms, time_since_start: 51m 54s 753ms, eta: 02h 10m 13s 576ms\n","\u001b[32m2022-05-01T07:15:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0061, train/hateful_memes/soft_label_cross_entropy/avg: 0.1280, train/total_loss: 0.0061, train/total_loss/avg: 0.1280, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 479ms, time_since_start: 52m 42s 233ms, eta: 02h 07m 09s 367ms\n","\u001b[32m2022-05-01T07:16:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0061, train/hateful_memes/soft_label_cross_entropy/avg: 0.1260, train/total_loss: 0.0061, train/total_loss/avg: 0.1260, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 501ms, time_since_start: 53m 29s 734ms, eta: 02h 06m 24s 501ms\n","\u001b[32m2022-05-01T07:17:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0058, train/hateful_memes/soft_label_cross_entropy/avg: 0.1240, train/total_loss: 0.0058, train/total_loss/avg: 0.1240, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 237ms, time_since_start: 54m 16s 971ms, eta: 02h 04m 54s 253ms\n","\u001b[32m2022-05-01T07:18:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0051, train/hateful_memes/soft_label_cross_entropy/avg: 0.1221, train/total_loss: 0.0051, train/total_loss/avg: 0.1221, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 382ms, time_since_start: 55m 04s 353ms, eta: 02h 04m 29s 089ms\n","\u001b[32m2022-05-01T07:18:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0051, train/hateful_memes/soft_label_cross_entropy/avg: 0.1203, train/total_loss: 0.0051, train/total_loss/avg: 0.1203, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 562ms, time_since_start: 55m 51s 916ms, eta: 02h 04m 09s 128ms\n","\u001b[32m2022-05-01T07:19:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0046, train/hateful_memes/soft_label_cross_entropy/avg: 0.1186, train/total_loss: 0.0046, train/total_loss/avg: 0.1186, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 271ms, time_since_start: 56m 39s 187ms, eta: 02h 02m 35s 429ms\n","\u001b[32m2022-05-01T07:20:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0046, train/hateful_memes/soft_label_cross_entropy/avg: 0.1188, train/total_loss: 0.0046, train/total_loss/avg: 0.1188, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 322ms, time_since_start: 57m 26s 509ms, eta: 02h 01m 55s 243ms\n","\u001b[32m2022-05-01T07:21:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0046, train/hateful_memes/soft_label_cross_entropy/avg: 0.1171, train/total_loss: 0.0046, train/total_loss/avg: 0.1171, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 310ms, time_since_start: 58m 13s 819ms, eta: 02h 01m 05s 302ms\n","\u001b[32m2022-05-01T07:22:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T07:22:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T07:22:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T07:22:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T07:22:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0046, train/hateful_memes/soft_label_cross_entropy/avg: 0.1154, train/total_loss: 0.0046, train/total_loss/avg: 0.1154, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.79, time: 56s 120ms, time_since_start: 59m 09s 940ms, eta: 02h 22m 41s 186ms\n","\u001b[32m2022-05-01T07:22:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T07:22:17 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T07:22:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T07:22:20 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T07:22:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T07:22:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T07:22:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T07:22:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/soft_label_cross_entropy: 1.6636, val/total_loss: 1.6636, val/hateful_memes/accuracy: 0.7056, val/hateful_memes/binary_f1: 0.5077, val/hateful_memes/roc_auc: 0.7276, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 12s 479ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.739945\n","\u001b[32m2022-05-01T07:23:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0042, train/hateful_memes/soft_label_cross_entropy/avg: 0.1138, train/total_loss: 0.0042, train/total_loss/avg: 0.1138, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 034ms, time_since_start: 01h 10s 455ms, eta: 02h 01m 18s 812ms\n","\u001b[32m2022-05-01T07:24:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0042, train/hateful_memes/soft_label_cross_entropy/avg: 0.1122, train/total_loss: 0.0042, train/total_loss/avg: 0.1122, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 459ms, time_since_start: 01h 57s 914ms, eta: 01h 59m 03s 388ms\n","\u001b[32m2022-05-01T07:24:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0041, train/hateful_memes/soft_label_cross_entropy/avg: 0.1107, train/total_loss: 0.0041, train/total_loss/avg: 0.1107, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 443ms, time_since_start: 01h 01m 45s 358ms, eta: 01h 58m 12s 774ms\n","\u001b[32m2022-05-01T07:25:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0041, train/hateful_memes/soft_label_cross_entropy/avg: 0.1093, train/total_loss: 0.0041, train/total_loss/avg: 0.1093, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 408ms, time_since_start: 01h 02m 32s 766ms, eta: 01h 57m 19s 308ms\n","\u001b[32m2022-05-01T07:26:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0041, train/hateful_memes/soft_label_cross_entropy/avg: 0.1079, train/total_loss: 0.0041, train/total_loss/avg: 0.1079, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 097ms, time_since_start: 01h 03m 19s 864ms, eta: 01h 55m 45s 246ms\n","\u001b[32m2022-05-01T07:27:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0021, train/hateful_memes/soft_label_cross_entropy/avg: 0.1065, train/total_loss: 0.0021, train/total_loss/avg: 0.1065, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 432ms, time_since_start: 01h 04m 07s 297ms, eta: 01h 55m 46s 408ms\n","\u001b[32m2022-05-01T07:28:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0021, train/hateful_memes/soft_label_cross_entropy/avg: 0.1052, train/total_loss: 0.0021, train/total_loss/avg: 0.1052, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 679ms, time_since_start: 01h 04m 54s 976ms, eta: 01h 55m 34s 061ms\n","\u001b[32m2022-05-01T07:28:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0021, train/hateful_memes/soft_label_cross_entropy/avg: 0.1053, train/total_loss: 0.0021, train/total_loss/avg: 0.1053, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 514ms, time_since_start: 01h 05m 42s 490ms, eta: 01h 54m 21s 760ms\n","\u001b[32m2022-05-01T07:29:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0021, train/hateful_memes/soft_label_cross_entropy/avg: 0.1040, train/total_loss: 0.0021, train/total_loss/avg: 0.1040, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 398ms, time_since_start: 01h 06m 29s 889ms, eta: 01h 53m 16s 808ms\n","\u001b[32m2022-05-01T07:30:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T07:30:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T07:30:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T07:30:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T07:30:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0021, train/hateful_memes/soft_label_cross_entropy/avg: 0.1030, train/total_loss: 0.0021, train/total_loss/avg: 0.1030, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.75, time: 57s 883ms, time_since_start: 01h 07m 27s 773ms, eta: 02h 17m 21s 514ms\n","\u001b[32m2022-05-01T07:30:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T07:30:35 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T07:30:38 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T07:30:38 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T07:30:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T07:30:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T07:30:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T07:30:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/soft_label_cross_entropy: 2.1527, val/total_loss: 2.1527, val/hateful_memes/accuracy: 0.7204, val/hateful_memes/binary_f1: 0.5145, val/hateful_memes/roc_auc: 0.7265, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 15s 781ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.739945\n","\u001b[32m2022-05-01T07:31:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0021, train/hateful_memes/soft_label_cross_entropy/avg: 0.1027, train/total_loss: 0.0021, train/total_loss/avg: 0.1027, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 030ms, time_since_start: 01h 08m 31s 587ms, eta: 01h 53m 09s 683ms\n","\u001b[32m2022-05-01T07:32:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0019, train/hateful_memes/soft_label_cross_entropy/avg: 0.1015, train/total_loss: 0.0019, train/total_loss/avg: 0.1015, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 528ms, time_since_start: 01h 09m 19s 115ms, eta: 01h 51m 10s 461ms\n","\u001b[32m2022-05-01T07:33:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0021, train/hateful_memes/soft_label_cross_entropy/avg: 0.1009, train/total_loss: 0.0021, train/total_loss/avg: 0.1009, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 259ms, time_since_start: 01h 10m 06s 375ms, eta: 01h 49m 44s 592ms\n","\u001b[32m2022-05-01T07:34:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0041, train/hateful_memes/soft_label_cross_entropy/avg: 0.0998, train/total_loss: 0.0041, train/total_loss/avg: 0.0998, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 377ms, time_since_start: 01h 10m 53s 752ms, eta: 01h 49m 12s 912ms\n","\u001b[32m2022-05-01T07:34:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0042, train/hateful_memes/soft_label_cross_entropy/avg: 0.0990, train/total_loss: 0.0042, train/total_loss/avg: 0.0990, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 310ms, time_since_start: 01h 11m 41s 062ms, eta: 01h 48m 15s 447ms\n","\u001b[32m2022-05-01T07:35:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0041, train/hateful_memes/soft_label_cross_entropy/avg: 0.0978, train/total_loss: 0.0041, train/total_loss/avg: 0.0978, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 2.17, time: 46s 961ms, time_since_start: 01h 12m 28s 024ms, eta: 01h 46m 39s 815ms\n","\u001b[32m2022-05-01T07:36:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0042, train/hateful_memes/soft_label_cross_entropy/avg: 0.0968, train/total_loss: 0.0042, train/total_loss/avg: 0.0968, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 293ms, time_since_start: 01h 13m 15s 317ms, eta: 01h 46m 36s 910ms\n","\u001b[32m2022-05-01T07:37:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0042, train/hateful_memes/soft_label_cross_entropy/avg: 0.0958, train/total_loss: 0.0042, train/total_loss/avg: 0.0958, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 066ms, time_since_start: 01h 14m 02s 384ms, eta: 01h 45m 18s 436ms\n","\u001b[32m2022-05-01T07:37:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0045, train/hateful_memes/soft_label_cross_entropy/avg: 0.0953, train/total_loss: 0.0045, train/total_loss/avg: 0.0953, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 291ms, time_since_start: 01h 14m 49s 676ms, eta: 01h 45m 525ms\n","\u001b[32m2022-05-01T07:38:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T07:38:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T07:38:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T07:38:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T07:38:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0045, train/hateful_memes/soft_label_cross_entropy/avg: 0.0943, train/total_loss: 0.0045, train/total_loss/avg: 0.0943, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 1.72, time: 58s 102ms, time_since_start: 01h 15m 47s 778ms, eta: 02h 08m 01s 705ms\n","\u001b[32m2022-05-01T07:38:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T07:38:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T07:38:58 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T07:38:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T07:38:59 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-05-01T07:38:59 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-05-01T07:39:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-01T07:39:08 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 4000\n","\u001b[32m2022-05-01T07:39:08 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 4000\n","\u001b[32m2022-05-01T07:39:08 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 16\n","\u001b[32m2022-05-01T07:39:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/soft_label_cross_entropy: 1.7334, val/total_loss: 1.7334, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.5076, val/hateful_memes/roc_auc: 0.7231, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 13s 894ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.739945\n","\u001b[32m2022-05-01T07:39:09 | mmf.trainers.core.training_loop: \u001b[0mEarly stopping activated\n","\u001b[32m2022-05-01T07:39:09 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2022-05-01T07:39:09 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-05-01T07:39:09 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-05-01T07:39:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-01T07:39:11 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 4000\n","\u001b[32m2022-05-01T07:39:11 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 4000\n","\u001b[32m2022-05-01T07:39:11 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 16\n","\u001b[32m2022-05-01T07:39:12 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-05-01T07:39:12 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:15<00:00,  4.12it/s]\n","\u001b[32m2022-05-01T07:39:28 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 63\n","\u001b[32m2022-05-01T07:39:28 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T07:39:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, test/hateful_memes/soft_label_cross_entropy: 1.7102, test/total_loss: 1.7102, test/hateful_memes/accuracy: 0.7020, test/hateful_memes/binary_f1: 0.4258, test/hateful_memes/roc_auc: 0.7668\n","\u001b[32m2022-05-01T07:39:28 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 16m 20s 259ms\n"]}],"source":["!python tools/run.py config=mmf/projects/hateful_memes/configs/visual_bert/from_coco_da_loss.yml \\\n","training.batch_size=32 \\\n","model=visual_bert dataset=hateful_memes"]},{"cell_type":"markdown","metadata":{"id":"8edaVY4iijXU"},"source":["# **Visual Bert ccoc bs32 default da smoothloss**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5274897,"status":"ok","timestamp":1651429376319,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"},"user_tz":300},"id":"EE6W9VOWTE8Z","outputId":"0573118c-cb4f-4093-b8f8-f1041103964b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-01T16:55:07 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf/projects/hateful_memes/configs/visual_bert/from_coco_da_smoothloss.yml\n","\u001b[32m2022-05-01T16:55:07 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n","\u001b[32m2022-05-01T16:55:07 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-05-01T16:55:07 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-01T16:55:07 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-01T16:55:07 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf/projects/hateful_memes/configs/visual_bert/from_coco_da_smoothloss.yml', 'training.batch_size=32', 'model=visual_bert', 'dataset=hateful_memes'])\n","\u001b[32m2022-05-01T16:55:07 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-01T16:55:07 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-05-01T16:55:07 | mmf_cli.run: \u001b[0mUsing seed 48425275\n","\u001b[32m2022-05-01T16:55:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [08:49<00:00, 19.5MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 153kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpk1h8a04e\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 24.0kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpc5j0xgmi\n","Downloading: 100% 570/570 [00:00<00:00, 492kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpospionxb\n","Downloading: 100% 232k/232k [00:00<00:00, 690kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpum133nt_\n","Downloading: 100% 466k/466k [00:00<00:00, 1.38MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-01T17:06:50 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-01T17:06:50 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-01T17:06:50 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-01T17:06:50 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"label_smoothing_cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmpkaqtvngi\n","Downloading: 100% 440M/440M [00:05<00:00, 77.0MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-05-01T17:07:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-01T17:07:06 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-01T17:07:06 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n","Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:19<00:00, 20.8MB/s]\n","[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n","Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-01T17:07:32 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-01T17:07:32 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-01T17:07:32 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-01T17:07:32 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2022-05-01T17:07:32 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-05-01T17:07:32 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): LabelSmoothingCrossEntropyLoss()\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-05-01T17:07:32 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-05-01T17:07:32 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-05-01T17:08:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.7104, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.7104, train/total_loss: 0.7104, train/total_loss/avg: 0.7104, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 055ms, time_since_start: 01m 13s 321ms, eta: 02h 54m 40s 443ms\n","\u001b[32m2022-05-01T17:09:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.6795, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6949, train/total_loss: 0.6795, train/total_loss/avg: 0.6949, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 629ms, time_since_start: 01m 59s 950ms, eta: 02h 52m 17s 951ms\n","\u001b[32m2022-05-01T17:09:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.6795, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6745, train/total_loss: 0.6795, train/total_loss/avg: 0.6745, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 465ms, time_since_start: 02m 46s 415ms, eta: 02h 50m 54s 345ms\n","\u001b[32m2022-05-01T17:10:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.6337, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6641, train/total_loss: 0.6337, train/total_loss/avg: 0.6641, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 492ms, time_since_start: 03m 32s 908ms, eta: 02h 50m 13s 128ms\n","\u001b[32m2022-05-01T17:11:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.6337, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6474, train/total_loss: 0.6337, train/total_loss/avg: 0.6474, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 488ms, time_since_start: 04m 19s 397ms, eta: 02h 49m 25s 000ms\n","\u001b[32m2022-05-01T17:12:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.6329, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6310, train/total_loss: 0.6329, train/total_loss/avg: 0.6310, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 285ms, time_since_start: 05m 05s 682ms, eta: 02h 47m 53s 400ms\n","\u001b[32m2022-05-01T17:12:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.6329, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6168, train/total_loss: 0.6329, train/total_loss/avg: 0.6168, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 576ms, time_since_start: 05m 52s 259ms, eta: 02h 48m 09s 503ms\n","\u001b[32m2022-05-01T17:13:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.5804, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.6016, train/total_loss: 0.5804, train/total_loss/avg: 0.6016, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 448ms, time_since_start: 06m 38s 708ms, eta: 02h 46m 54s 581ms\n","\u001b[32m2022-05-01T17:14:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.5804, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5910, train/total_loss: 0.5804, train/total_loss/avg: 0.5910, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 761ms, time_since_start: 07m 25s 469ms, eta: 02h 47m 14s 350ms\n","\u001b[32m2022-05-01T17:15:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T17:15:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T17:15:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T17:15:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T17:15:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.5494, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5813, train/total_loss: 0.5494, train/total_loss/avg: 0.5813, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.79, time: 56s 078ms, time_since_start: 08m 21s 547ms, eta: 03h 19m 36s 611ms\n","\u001b[32m2022-05-01T17:15:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T17:15:27 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T17:15:33 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T17:15:33 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T17:15:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T17:15:41 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-01T17:15:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T17:15:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T17:15:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/label_smoothing_cross_entropy: 0.7118, val/total_loss: 0.7118, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4401, val/hateful_memes/roc_auc: 0.6922, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 24s 045ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.692162\n","\u001b[32m2022-05-01T17:16:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.5494, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5613, train/total_loss: 0.5494, train/total_loss/avg: 0.5613, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 448ms, time_since_start: 09m 33s 042ms, eta: 02h 48m 05s 381ms\n","\u001b[32m2022-05-01T17:17:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.5314, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5586, train/total_loss: 0.5314, train/total_loss/avg: 0.5586, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 2.17, time: 46s 731ms, time_since_start: 10m 19s 774ms, eta: 02h 44m 45s 370ms\n","\u001b[32m2022-05-01T17:18:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.5314, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5540, train/total_loss: 0.5314, train/total_loss/avg: 0.5540, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 2.17, time: 46s 773ms, time_since_start: 11m 06s 547ms, eta: 02h 44m 06s 697ms\n","\u001b[32m2022-05-01T17:18:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.5283, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5394, train/total_loss: 0.5283, train/total_loss/avg: 0.5394, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 2.17, time: 46s 682ms, time_since_start: 11m 53s 230ms, eta: 02h 43m 135ms\n","\u001b[32m2022-05-01T17:19:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.5283, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5291, train/total_loss: 0.5283, train/total_loss/avg: 0.5291, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 2.17, time: 46s 883ms, time_since_start: 12m 40s 113ms, eta: 02h 42m 54s 427ms\n","\u001b[32m2022-05-01T17:20:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.5059, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5194, train/total_loss: 0.5059, train/total_loss/avg: 0.5194, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 2.17, time: 46s 347ms, time_since_start: 13m 26s 460ms, eta: 02h 40m 15s 571ms\n","\u001b[32m2022-05-01T17:21:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.5059, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5149, train/total_loss: 0.5059, train/total_loss/avg: 0.5149, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 2.17, time: 46s 813ms, time_since_start: 14m 13s 274ms, eta: 02h 41m 04s 760ms\n","\u001b[32m2022-05-01T17:22:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.4997, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5093, train/total_loss: 0.4997, train/total_loss/avg: 0.5093, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 2.17, time: 46s 756ms, time_since_start: 15m 031ms, eta: 02h 40m 05s 442ms\n","\u001b[32m2022-05-01T17:22:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.4997, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.5033, train/total_loss: 0.4997, train/total_loss/avg: 0.5033, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 2.17, time: 46s 848ms, time_since_start: 15m 46s 879ms, eta: 02h 39m 36s 553ms\n","\u001b[32m2022-05-01T17:23:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T17:23:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T17:23:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T17:23:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T17:23:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.4956, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4978, train/total_loss: 0.4956, train/total_loss/avg: 0.4978, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.75, time: 57s 449ms, time_since_start: 16m 44s 329ms, eta: 03h 14m 45s 309ms\n","\u001b[32m2022-05-01T17:23:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T17:23:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T17:23:54 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T17:23:54 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T17:23:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T17:23:58 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-01T17:24:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T17:24:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T17:24:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/label_smoothing_cross_entropy: 0.7609, val/total_loss: 0.7609, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4720, val/hateful_memes/roc_auc: 0.6966, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 20s 464ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.696618\n","\u001b[32m2022-05-01T17:24:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.4941, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4950, train/total_loss: 0.4941, train/total_loss/avg: 0.4950, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 576ms, time_since_start: 17m 52s 371ms, eta: 02h 40m 28s 583ms\n","\u001b[32m2022-05-01T17:25:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.4434, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4874, train/total_loss: 0.4434, train/total_loss/avg: 0.4874, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 2.17, time: 46s 939ms, time_since_start: 18m 39s 310ms, eta: 02h 37m 31s 947ms\n","\u001b[32m2022-05-01T17:26:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.4434, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4879, train/total_loss: 0.4434, train/total_loss/avg: 0.4879, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 099ms, time_since_start: 19m 26s 410ms, eta: 02h 37m 16s 404ms\n","\u001b[32m2022-05-01T17:27:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.4397, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4813, train/total_loss: 0.4397, train/total_loss/avg: 0.4813, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 2.17, time: 46s 807ms, time_since_start: 20m 13s 217ms, eta: 02h 35m 30s 141ms\n","\u001b[32m2022-05-01T17:28:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.4128, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4768, train/total_loss: 0.4128, train/total_loss/avg: 0.4768, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 308ms, time_since_start: 21m 526ms, eta: 02h 36m 22s 034ms\n","\u001b[32m2022-05-01T17:28:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3952, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4730, train/total_loss: 0.3952, train/total_loss/avg: 0.4730, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 314ms, time_since_start: 21m 47s 841ms, eta: 02h 35m 35s 142ms\n","\u001b[32m2022-05-01T17:29:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3934, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4677, train/total_loss: 0.3934, train/total_loss/avg: 0.4677, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 2.17, time: 46s 898ms, time_since_start: 22m 34s 740ms, eta: 02h 33m 25s 303ms\n","\u001b[32m2022-05-01T17:30:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3857, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4644, train/total_loss: 0.3857, train/total_loss/avg: 0.4644, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 086ms, time_since_start: 23m 21s 827ms, eta: 02h 33m 14s 374ms\n","\u001b[32m2022-05-01T17:31:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3857, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4633, train/total_loss: 0.3857, train/total_loss/avg: 0.4633, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 146ms, time_since_start: 24m 08s 973ms, eta: 02h 32m 37s 975ms\n","\u001b[32m2022-05-01T17:32:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T17:32:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T17:32:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T17:32:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T17:32:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3793, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4599, train/total_loss: 0.3793, train/total_loss/avg: 0.4599, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.82, time: 55s 727ms, time_since_start: 25m 04s 700ms, eta: 02h 59m 28s 192ms\n","\u001b[32m2022-05-01T17:32:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T17:32:10 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T17:32:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T17:32:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T17:32:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T17:32:17 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-01T17:32:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T17:32:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T17:32:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/label_smoothing_cross_entropy: 0.7493, val/total_loss: 0.7493, val/hateful_memes/accuracy: 0.7000, val/hateful_memes/binary_f1: 0.5549, val/hateful_memes/roc_auc: 0.7107, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 22s 244ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.710750\n","\u001b[32m2022-05-01T17:33:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3793, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4569, train/total_loss: 0.3793, train/total_loss/avg: 0.4569, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 807ms, time_since_start: 26m 14s 754ms, eta: 02h 33m 09s 155ms\n","\u001b[32m2022-05-01T17:34:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3759, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4529, train/total_loss: 0.3759, train/total_loss/avg: 0.4529, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 065ms, time_since_start: 27m 01s 819ms, eta: 02h 29m 58s 704ms\n","\u001b[32m2022-05-01T17:34:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3738, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4493, train/total_loss: 0.3738, train/total_loss/avg: 0.4493, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 186ms, time_since_start: 27m 49s 005ms, eta: 02h 29m 33s 837ms\n","\u001b[32m2022-05-01T17:35:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3738, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4471, train/total_loss: 0.3738, train/total_loss/avg: 0.4471, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 027ms, time_since_start: 28m 36s 032ms, eta: 02h 28m 15s 752ms\n","\u001b[32m2022-05-01T17:36:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3727, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4438, train/total_loss: 0.3727, train/total_loss/avg: 0.4438, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 001ms, time_since_start: 29m 23s 034ms, eta: 02h 27m 23s 173ms\n","\u001b[32m2022-05-01T17:37:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3694, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4406, train/total_loss: 0.3694, train/total_loss/avg: 0.4406, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 003ms, time_since_start: 30m 10s 037ms, eta: 02h 26m 35s 596ms\n","\u001b[32m2022-05-01T17:38:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3643, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4375, train/total_loss: 0.3643, train/total_loss/avg: 0.4375, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 006ms, time_since_start: 30m 57s 044ms, eta: 02h 25m 48s 521ms\n","\u001b[32m2022-05-01T17:38:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3622, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4346, train/total_loss: 0.3622, train/total_loss/avg: 0.4346, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 2.17, time: 46s 562ms, time_since_start: 31m 43s 607ms, eta: 02h 23m 38s 511ms\n","\u001b[32m2022-05-01T17:39:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3622, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4331, train/total_loss: 0.3622, train/total_loss/avg: 0.4331, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 165ms, time_since_start: 32m 30s 773ms, eta: 02h 24m 42s 159ms\n","\u001b[32m2022-05-01T17:40:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T17:40:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T17:40:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T17:40:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T17:40:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3369, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4307, train/total_loss: 0.3369, train/total_loss/avg: 0.4307, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.79, time: 56s 889ms, time_since_start: 33m 27s 662ms, eta: 02h 53m 34s 149ms\n","\u001b[32m2022-05-01T17:40:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T17:40:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T17:40:36 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T17:40:36 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T17:40:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T17:40:41 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-01T17:40:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T17:40:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T17:40:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/label_smoothing_cross_entropy: 0.7603, val/total_loss: 0.7603, val/hateful_memes/accuracy: 0.7111, val/hateful_memes/binary_f1: 0.5385, val/hateful_memes/roc_auc: 0.7255, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 19s 949ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.725504\n","\u001b[32m2022-05-01T17:41:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3362, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4282, train/total_loss: 0.3362, train/total_loss/avg: 0.4282, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 939ms, time_since_start: 34m 35s 553ms, eta: 02h 25m 27s 099ms\n","\u001b[32m2022-05-01T17:42:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3362, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4258, train/total_loss: 0.3362, train/total_loss/avg: 0.4258, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 351ms, time_since_start: 35m 22s 904ms, eta: 02h 22m 51s 801ms\n","\u001b[32m2022-05-01T17:43:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3329, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4234, train/total_loss: 0.3329, train/total_loss/avg: 0.4234, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 2.17, time: 46s 916ms, time_since_start: 36m 09s 820ms, eta: 02h 20m 45s 305ms\n","\u001b[32m2022-05-01T17:44:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3362, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4216, train/total_loss: 0.3362, train/total_loss/avg: 0.4216, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 150ms, time_since_start: 36m 56s 971ms, eta: 02h 20m 39s 588ms\n","\u001b[32m2022-05-01T17:44:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3362, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4200, train/total_loss: 0.3362, train/total_loss/avg: 0.4200, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 204ms, time_since_start: 37m 44s 176ms, eta: 02h 20m 01s 230ms\n","\u001b[32m2022-05-01T17:45:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3329, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4181, train/total_loss: 0.3329, train/total_loss/avg: 0.4181, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 200ms, time_since_start: 38m 31s 376ms, eta: 02h 19m 12s 535ms\n","\u001b[32m2022-05-01T17:46:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3362, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4169, train/total_loss: 0.3362, train/total_loss/avg: 0.4169, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 137ms, time_since_start: 39m 18s 513ms, eta: 02h 18m 13s 351ms\n","\u001b[32m2022-05-01T17:47:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3329, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4150, train/total_loss: 0.3329, train/total_loss/avg: 0.4150, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 2.17, time: 46s 863ms, time_since_start: 40m 05s 377ms, eta: 02h 16m 37s 639ms\n","\u001b[32m2022-05-01T17:47:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3292, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4132, train/total_loss: 0.3292, train/total_loss/avg: 0.4132, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 192ms, time_since_start: 40m 52s 569ms, eta: 02h 16m 47s 028ms\n","\u001b[32m2022-05-01T17:48:46 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T17:48:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T17:48:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T17:48:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T17:48:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3292, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4114, train/total_loss: 0.3292, train/total_loss/avg: 0.4114, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.79, time: 56s 382ms, time_since_start: 41m 48s 952ms, eta: 02h 42m 27s 942ms\n","\u001b[32m2022-05-01T17:48:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T17:48:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T17:48:57 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T17:48:57 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T17:48:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T17:49:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T17:49:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T17:49:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/label_smoothing_cross_entropy: 0.7762, val/total_loss: 0.7762, val/hateful_memes/accuracy: 0.7278, val/hateful_memes/binary_f1: 0.5421, val/hateful_memes/roc_auc: 0.7162, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 15s 893ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.725504\n","\u001b[32m2022-05-01T17:49:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3278, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4098, train/total_loss: 0.3278, train/total_loss/avg: 0.4098, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 728ms, time_since_start: 42m 52s 576ms, eta: 02h 16m 43s 270ms\n","\u001b[32m2022-05-01T17:50:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3278, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4094, train/total_loss: 0.3278, train/total_loss/avg: 0.4094, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 551ms, time_since_start: 43m 40s 128ms, eta: 02h 15m 24s 489ms\n","\u001b[32m2022-05-01T17:51:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3278, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4081, train/total_loss: 0.3278, train/total_loss/avg: 0.4081, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 236ms, time_since_start: 44m 27s 364ms, eta: 02h 13m 42s 555ms\n","\u001b[32m2022-05-01T17:52:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3278, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4077, train/total_loss: 0.3278, train/total_loss/avg: 0.4077, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 081ms, time_since_start: 45m 14s 445ms, eta: 02h 12m 28s 342ms\n","\u001b[32m2022-05-01T17:53:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3275, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4062, train/total_loss: 0.3275, train/total_loss/avg: 0.4062, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 294ms, time_since_start: 46m 01s 740ms, eta: 02h 12m 16s 260ms\n","\u001b[32m2022-05-01T17:53:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3275, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4047, train/total_loss: 0.3275, train/total_loss/avg: 0.4047, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 260ms, time_since_start: 46m 49s 000ms, eta: 02h 11m 22s 525ms\n","\u001b[32m2022-05-01T17:54:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3275, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4043, train/total_loss: 0.3275, train/total_loss/avg: 0.4043, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 376ms, time_since_start: 47m 36s 376ms, eta: 02h 10m 53s 600ms\n","\u001b[32m2022-05-01T17:55:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3275, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4029, train/total_loss: 0.3275, train/total_loss/avg: 0.4029, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 353ms, time_since_start: 48m 23s 730ms, eta: 02h 10m 01s 654ms\n","\u001b[32m2022-05-01T17:56:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3271, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4016, train/total_loss: 0.3271, train/total_loss/avg: 0.4016, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 022ms, time_since_start: 49m 10s 753ms, eta: 02h 08m 19s 381ms\n","\u001b[32m2022-05-01T17:57:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T17:57:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T17:57:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T17:57:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T17:57:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3271, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4006, train/total_loss: 0.3271, train/total_loss/avg: 0.4006, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.72, time: 58s 004ms, time_since_start: 50m 08s 757ms, eta: 02h 37m 18s 497ms\n","\u001b[32m2022-05-01T17:57:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T17:57:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T17:57:17 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T17:57:17 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T17:57:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T17:57:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T17:57:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T17:57:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/label_smoothing_cross_entropy: 0.8214, val/total_loss: 0.8214, val/hateful_memes/accuracy: 0.7037, val/hateful_memes/binary_f1: 0.4406, val/hateful_memes/roc_auc: 0.6846, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 16s 656ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.725504\n","\u001b[32m2022-05-01T17:58:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3275, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.4002, train/total_loss: 0.3275, train/total_loss/avg: 0.4002, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 989ms, time_since_start: 51m 13s 404ms, eta: 02h 09m 20s 059ms\n","\u001b[32m2022-05-01T17:59:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3292, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3993, train/total_loss: 0.3292, train/total_loss/avg: 0.3993, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 511ms, time_since_start: 52m 916ms, eta: 02h 07m 14s 414ms\n","\u001b[32m2022-05-01T17:59:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3292, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3981, train/total_loss: 0.3292, train/total_loss/avg: 0.3981, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 732ms, time_since_start: 52m 48s 648ms, eta: 02h 07m 01s 334ms\n","\u001b[32m2022-05-01T18:00:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3275, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3970, train/total_loss: 0.3275, train/total_loss/avg: 0.3970, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 451ms, time_since_start: 53m 36s 099ms, eta: 02h 05m 28s 200ms\n","\u001b[32m2022-05-01T18:01:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3264, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3959, train/total_loss: 0.3264, train/total_loss/avg: 0.3959, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 718ms, time_since_start: 54m 23s 818ms, eta: 02h 05m 22s 161ms\n","\u001b[32m2022-05-01T18:02:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3263, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3948, train/total_loss: 0.3263, train/total_loss/avg: 0.3948, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 604ms, time_since_start: 55m 11s 423ms, eta: 02h 04m 15s 798ms\n","\u001b[32m2022-05-01T18:03:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3263, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3938, train/total_loss: 0.3263, train/total_loss/avg: 0.3938, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 638ms, time_since_start: 55m 59s 061ms, eta: 02h 03m 32s 566ms\n","\u001b[32m2022-05-01T18:03:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3261, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3928, train/total_loss: 0.3261, train/total_loss/avg: 0.3928, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 583ms, time_since_start: 56m 46s 644ms, eta: 02h 02m 35s 627ms\n","\u001b[32m2022-05-01T18:04:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3918, train/total_loss: 0.3260, train/total_loss/avg: 0.3918, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 552ms, time_since_start: 57m 34s 197ms, eta: 02h 01m 42s 485ms\n","\u001b[32m2022-05-01T18:05:27 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T18:05:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T18:05:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T18:05:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T18:05:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3909, train/total_loss: 0.3260, train/total_loss/avg: 0.3909, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.82, time: 55s 393ms, time_since_start: 58m 29s 590ms, eta: 02h 20m 50s 253ms\n","\u001b[32m2022-05-01T18:05:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T18:05:35 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T18:05:38 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T18:05:38 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T18:05:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T18:05:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T18:05:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T18:05:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/label_smoothing_cross_entropy: 0.7603, val/total_loss: 0.7603, val/hateful_memes/accuracy: 0.7407, val/hateful_memes/binary_f1: 0.5732, val/hateful_memes/roc_auc: 0.7109, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 15s 976ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.725504\n","\u001b[32m2022-05-01T18:06:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3907, train/total_loss: 0.3260, train/total_loss/avg: 0.3907, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 186ms, time_since_start: 59m 33s 754ms, eta: 02h 01m 41s 785ms\n","\u001b[32m2022-05-01T18:07:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3900, train/total_loss: 0.3260, train/total_loss/avg: 0.3900, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 786ms, time_since_start: 01h 21s 541ms, eta: 01h 59m 52s 643ms\n","\u001b[32m2022-05-01T18:08:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3891, train/total_loss: 0.3260, train/total_loss/avg: 0.3891, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 636ms, time_since_start: 01h 01m 09s 177ms, eta: 01h 58m 41s 651ms\n","\u001b[32m2022-05-01T18:09:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3883, train/total_loss: 0.3260, train/total_loss/avg: 0.3883, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 630ms, time_since_start: 01h 01m 56s 807ms, eta: 01h 57m 52s 211ms\n","\u001b[32m2022-05-01T18:09:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3875, train/total_loss: 0.3260, train/total_loss/avg: 0.3875, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 352ms, time_since_start: 01h 02m 44s 160ms, eta: 01h 56m 22s 910ms\n","\u001b[32m2022-05-01T18:10:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3867, train/total_loss: 0.3260, train/total_loss/avg: 0.3867, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 663ms, time_since_start: 01h 03m 31s 823ms, eta: 01h 56m 20s 155ms\n","\u001b[32m2022-05-01T18:11:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3865, train/total_loss: 0.3260, train/total_loss/avg: 0.3865, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 888ms, time_since_start: 01h 04m 19s 712ms, eta: 01h 56m 04s 536ms\n","\u001b[32m2022-05-01T18:12:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3857, train/total_loss: 0.3260, train/total_loss/avg: 0.3857, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 800ms, time_since_start: 01h 05m 07s 513ms, eta: 01h 55m 03s 063ms\n","\u001b[32m2022-05-01T18:13:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3850, train/total_loss: 0.3260, train/total_loss/avg: 0.3850, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 716ms, time_since_start: 01h 05m 55s 229ms, eta: 01h 54m 02s 346ms\n","\u001b[32m2022-05-01T18:13:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T18:13:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T18:13:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T18:13:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T18:13:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3842, train/total_loss: 0.3260, train/total_loss/avg: 0.3842, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.75, time: 57s 969ms, time_since_start: 01h 06m 53s 198ms, eta: 02h 17m 33s 633ms\n","\u001b[32m2022-05-01T18:13:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T18:13:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T18:14:02 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T18:14:02 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T18:14:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T18:14:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T18:14:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T18:14:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/label_smoothing_cross_entropy: 0.7505, val/total_loss: 0.7505, val/hateful_memes/accuracy: 0.7333, val/hateful_memes/binary_f1: 0.5294, val/hateful_memes/roc_auc: 0.7243, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 12s 747ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.725504\n","\u001b[32m2022-05-01T18:15:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3836, train/total_loss: 0.3260, train/total_loss/avg: 0.3836, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 236ms, time_since_start: 01h 07m 54s 184ms, eta: 01h 53m 38s 863ms\n","\u001b[32m2022-05-01T18:15:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3829, train/total_loss: 0.3260, train/total_loss/avg: 0.3829, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 622ms, time_since_start: 01h 08m 41s 806ms, eta: 01h 51m 23s 604ms\n","\u001b[32m2022-05-01T18:16:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3259, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3822, train/total_loss: 0.3259, train/total_loss/avg: 0.3822, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 318ms, time_since_start: 01h 09m 29s 125ms, eta: 01h 49m 52s 842ms\n","\u001b[32m2022-05-01T18:17:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3257, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3815, train/total_loss: 0.3257, train/total_loss/avg: 0.3815, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 593ms, time_since_start: 01h 10m 16s 718ms, eta: 01h 49m 42s 692ms\n","\u001b[32m2022-05-01T18:18:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3259, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3809, train/total_loss: 0.3259, train/total_loss/avg: 0.3809, max mem: 9224.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 697ms, time_since_start: 01h 11m 04s 416ms, eta: 01h 49m 08s 681ms\n","\u001b[32m2022-05-01T18:18:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3259, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3802, train/total_loss: 0.3259, train/total_loss/avg: 0.3802, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 504ms, time_since_start: 01h 11m 51s 920ms, eta: 01h 47m 53s 843ms\n","\u001b[32m2022-05-01T18:19:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3259, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3796, train/total_loss: 0.3259, train/total_loss/avg: 0.3796, max mem: 9224.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 519ms, time_since_start: 01h 12m 39s 440ms, eta: 01h 47m 07s 502ms\n","\u001b[32m2022-05-01T18:20:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3259, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3790, train/total_loss: 0.3259, train/total_loss/avg: 0.3790, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 425ms, time_since_start: 01h 13m 26s 865ms, eta: 01h 46m 06s 566ms\n","\u001b[32m2022-05-01T18:21:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3260, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3784, train/total_loss: 0.3260, train/total_loss/avg: 0.3784, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 463ms, time_since_start: 01h 14m 14s 328ms, eta: 01h 45m 23s 370ms\n","\u001b[32m2022-05-01T18:22:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-01T18:22:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-01T18:22:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-01T18:22:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-01T18:22:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/label_smoothing_cross_entropy: 0.3262, train/hateful_memes/label_smoothing_cross_entropy/avg: 0.3783, train/total_loss: 0.3262, train/total_loss/avg: 0.3783, max mem: 9224.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 1.72, time: 58s 403ms, time_since_start: 01h 15m 12s 732ms, eta: 02h 08m 41s 513ms\n","\u001b[32m2022-05-01T18:22:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-01T18:22:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-01T18:22:21 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-01T18:22:21 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T18:22:23 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-05-01T18:22:23 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-05-01T18:22:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-01T18:22:28 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 4000\n","\u001b[32m2022-05-01T18:22:28 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 4000\n","\u001b[32m2022-05-01T18:22:28 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 16\n","\u001b[32m2022-05-01T18:22:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/label_smoothing_cross_entropy: 0.7827, val/total_loss: 0.7827, val/hateful_memes/accuracy: 0.7093, val/hateful_memes/binary_f1: 0.5016, val/hateful_memes/roc_auc: 0.6792, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 10s 882ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.725504\n","\u001b[32m2022-05-01T18:22:30 | mmf.trainers.core.training_loop: \u001b[0mEarly stopping activated\n","\u001b[32m2022-05-01T18:22:30 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2022-05-01T18:22:30 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-05-01T18:22:30 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-05-01T18:22:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-01T18:22:31 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 4000\n","\u001b[32m2022-05-01T18:22:31 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 4000\n","\u001b[32m2022-05-01T18:22:31 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 16\n","\u001b[32m2022-05-01T18:22:35 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-05-01T18:22:35 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:14<00:00,  4.24it/s]\n","\u001b[32m2022-05-01T18:22:50 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 63\n","\u001b[32m2022-05-01T18:22:50 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-01T18:22:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, test/hateful_memes/label_smoothing_cross_entropy: 0.7721, test/total_loss: 0.7721, test/hateful_memes/accuracy: 0.7110, test/hateful_memes/binary_f1: 0.5420, test/hateful_memes/roc_auc: 0.7511\n","\u001b[32m2022-05-01T18:22:50 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 15m 44s 747ms\n"]}],"source":["!python tools/run.py config=mmf/projects/hateful_memes/configs/visual_bert/from_coco_da_smoothloss.yml \\\n","training.batch_size=32 \\\n","model=visual_bert dataset=hateful_memes"]},{"cell_type":"markdown","source":["## ** visual bert coco bs 32 no aug + soft label cross entropy loss**"],"metadata":{"id":"zAjoq4L4Yqr1"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJWuXMtoi10P","outputId":"48c621eb-6b33-4f53-8c63-7250ee915abd"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-02T10:33:44 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf/projects/hateful_memes/configs/visual_bert/from_coco_loss.yml\n","\u001b[32m2022-05-02T10:33:44 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n","\u001b[32m2022-05-02T10:33:44 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-05-02T10:33:44 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-02T10:33:44 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-02T10:33:44 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf/projects/hateful_memes/configs/visual_bert/from_coco_loss.yml', 'training.batch_size=32', 'model=visual_bert', 'dataset=hateful_memes'])\n","\u001b[32m2022-05-02T10:33:44 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-02T10:33:44 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2022-05-02T10:33:44 | mmf_cli.run: \u001b[0mUsing seed 48425275\n","\u001b[32m2022-05-02T10:33:44 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [03:43<00:00, 46.1MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:00<00:00, 290kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpboiixb0x\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 28.2kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_c1mgnmo\n","Downloading: 100% 570/570 [00:00<00:00, 444kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpo3wkn5c2\n","Downloading: 100% 232k/232k [00:00<00:00, 3.01MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn8pe_6bg\n","Downloading: 100% 466k/466k [00:00<00:00, 6.25MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-02T10:40:39 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T10:40:39 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T10:40:39 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T10:40:39 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"soft_label_cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmpt_bnleth\n","Downloading: 100% 440M/440M [00:08<00:00, 51.0MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-05-02T10:40:57 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-02T10:40:57 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-02T10:40:57 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n","Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:10<00:00, 39.6MB/s]\n","[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n","Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T10:41:14 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T10:41:14 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T10:41:14 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T10:41:14 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2022-05-02T10:41:15 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-05-02T10:41:15 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): SoftLabelCrossEntropyLoss()\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-05-02T10:41:15 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-05-02T10:41:15 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-05-02T10:42:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/soft_label_cross_entropy: 0.7006, train/hateful_memes/soft_label_cross_entropy/avg: 0.7006, train/total_loss: 0.7006, train/total_loss/avg: 0.7006, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.23, time: 01m 21s 608ms, time_since_start: 01m 39s 054ms, eta: 05h 02m 56s 147ms\n","\u001b[32m2022-05-02T10:43:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/soft_label_cross_entropy: 0.7006, train/hateful_memes/soft_label_cross_entropy/avg: 0.7160, train/total_loss: 0.7006, train/total_loss/avg: 0.7160, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 173ms, time_since_start: 02m 59s 227ms, eta: 04h 56m 14s 887ms\n","\u001b[32m2022-05-02T10:45:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/soft_label_cross_entropy: 0.7006, train/hateful_memes/soft_label_cross_entropy/avg: 0.6832, train/total_loss: 0.7006, train/total_loss/avg: 0.6832, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 625ms, time_since_start: 04m 18s 853ms, eta: 04h 52m 52s 549ms\n","\u001b[32m2022-05-02T10:46:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/soft_label_cross_entropy: 0.6175, train/hateful_memes/soft_label_cross_entropy/avg: 0.6470, train/total_loss: 0.6175, train/total_loss/avg: 0.6470, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 225ms, time_since_start: 05m 39s 078ms, eta: 04h 53m 43s 331ms\n","\u001b[32m2022-05-02T10:47:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/soft_label_cross_entropy: 0.6175, train/hateful_memes/soft_label_cross_entropy/avg: 0.6196, train/total_loss: 0.6175, train/total_loss/avg: 0.6196, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 1.25, time: 01m 20s 318ms, time_since_start: 06m 59s 396ms, eta: 04h 52m 41s 946ms\n","\u001b[32m2022-05-02T10:49:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/soft_label_cross_entropy: 0.5385, train/hateful_memes/soft_label_cross_entropy/avg: 0.5914, train/total_loss: 0.5385, train/total_loss/avg: 0.5914, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 840ms, time_since_start: 08m 19s 237ms, eta: 04h 49m 36s 381ms\n","\u001b[32m2022-05-02T10:50:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/soft_label_cross_entropy: 0.5385, train/hateful_memes/soft_label_cross_entropy/avg: 0.5495, train/total_loss: 0.5385, train/total_loss/avg: 0.5495, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 262ms, time_since_start: 09m 39s 500ms, eta: 04h 49m 46s 640ms\n","\u001b[32m2022-05-02T10:51:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/soft_label_cross_entropy: 0.5099, train/hateful_memes/soft_label_cross_entropy/avg: 0.5286, train/total_loss: 0.5099, train/total_loss/avg: 0.5286, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 1.27, time: 01m 19s 914ms, time_since_start: 10m 59s 415ms, eta: 04h 47m 09s 940ms\n","\u001b[32m2022-05-02T10:53:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/soft_label_cross_entropy: 0.5099, train/hateful_memes/soft_label_cross_entropy/avg: 0.5140, train/total_loss: 0.5099, train/total_loss/avg: 0.5140, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 1.25, time: 01m 20s 175ms, time_since_start: 12m 19s 591ms, eta: 04h 46m 44s 726ms\n","\u001b[32m2022-05-02T10:54:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T10:54:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T10:54:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T10:54:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T10:54:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/soft_label_cross_entropy: 0.4509, train/hateful_memes/soft_label_cross_entropy/avg: 0.5027, train/total_loss: 0.4509, train/total_loss/avg: 0.5027, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 431ms, time_since_start: 13m 49s 022ms, eta: 05h 18m 19s 851ms\n","\u001b[32m2022-05-02T10:54:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T10:54:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T10:54:55 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T10:54:55 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T10:54:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T10:55:00 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-02T10:55:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T10:55:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T10:55:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/soft_label_cross_entropy: 0.7938, val/total_loss: 0.7938, val/hateful_memes/accuracy: 0.6685, val/hateful_memes/binary_f1: 0.4389, val/hateful_memes/roc_auc: 0.6993, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 23s 850ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.699265\n","\u001b[32m2022-05-02T10:56:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/soft_label_cross_entropy: 0.4509, train/hateful_memes/soft_label_cross_entropy/avg: 0.4624, train/total_loss: 0.4509, train/total_loss/avg: 0.4624, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 991ms, time_since_start: 15m 33s 866ms, eta: 04h 46m 55s 040ms\n","\u001b[32m2022-05-02T10:57:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/soft_label_cross_entropy: 0.4005, train/hateful_memes/soft_label_cross_entropy/avg: 0.4504, train/total_loss: 0.4005, train/total_loss/avg: 0.4504, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 335ms, time_since_start: 16m 54s 201ms, eta: 04h 43m 13s 772ms\n","\u001b[32m2022-05-02T10:59:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/soft_label_cross_entropy: 0.4005, train/hateful_memes/soft_label_cross_entropy/avg: 0.4369, train/total_loss: 0.4005, train/total_loss/avg: 0.4369, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 374ms, time_since_start: 18m 14s 576ms, eta: 04h 42m 390ms\n","\u001b[32m2022-05-02T11:00:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/soft_label_cross_entropy: 0.3974, train/hateful_memes/soft_label_cross_entropy/avg: 0.4119, train/total_loss: 0.3974, train/total_loss/avg: 0.4119, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 1.27, time: 01m 19s 913ms, time_since_start: 19m 34s 490ms, eta: 04h 39m 02s 104ms\n","\u001b[32m2022-05-02T11:01:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/soft_label_cross_entropy: 0.3974, train/hateful_memes/soft_label_cross_entropy/avg: 0.4030, train/total_loss: 0.3974, train/total_loss/avg: 0.4030, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 508ms, time_since_start: 20m 54s 998ms, eta: 04h 39m 44s 744ms\n","\u001b[32m2022-05-02T11:03:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/soft_label_cross_entropy: 0.3821, train/hateful_memes/soft_label_cross_entropy/avg: 0.3806, train/total_loss: 0.3821, train/total_loss/avg: 0.3806, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 066ms, time_since_start: 22m 15s 065ms, eta: 04h 36m 51s 306ms\n","\u001b[32m2022-05-02T11:04:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/soft_label_cross_entropy: 0.3821, train/hateful_memes/soft_label_cross_entropy/avg: 0.3639, train/total_loss: 0.3821, train/total_loss/avg: 0.3639, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 422ms, time_since_start: 23m 35s 487ms, eta: 04h 36m 43s 304ms\n","\u001b[32m2022-05-02T11:05:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/soft_label_cross_entropy: 0.3186, train/hateful_memes/soft_label_cross_entropy/avg: 0.3547, train/total_loss: 0.3186, train/total_loss/avg: 0.3547, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 394ms, time_since_start: 24m 55s 882ms, eta: 04h 35m 15s 755ms\n","\u001b[32m2022-05-02T11:07:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/soft_label_cross_entropy: 0.3186, train/hateful_memes/soft_label_cross_entropy/avg: 0.3415, train/total_loss: 0.3186, train/total_loss/avg: 0.3415, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 1.27, time: 01m 19s 912ms, time_since_start: 26m 15s 795ms, eta: 04h 32m 15s 521ms\n","\u001b[32m2022-05-02T11:08:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T11:08:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T11:08:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T11:08:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T11:08:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/soft_label_cross_entropy: 0.2981, train/hateful_memes/soft_label_cross_entropy/avg: 0.3309, train/total_loss: 0.2981, train/total_loss/avg: 0.3309, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.10, time: 01m 31s 062ms, time_since_start: 27m 46s 857ms, eta: 05h 08m 42s 161ms\n","\u001b[32m2022-05-02T11:08:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T11:08:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T11:08:49 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T11:08:49 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T11:08:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T11:08:55 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-02T11:09:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T11:09:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T11:09:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/soft_label_cross_entropy: 1.0694, val/total_loss: 1.0694, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.5360, val/hateful_memes/roc_auc: 0.7131, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 25s 643ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.713132\n","\u001b[32m2022-05-02T11:10:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/soft_label_cross_entropy: 0.2789, train/hateful_memes/soft_label_cross_entropy/avg: 0.3167, train/total_loss: 0.2789, train/total_loss/avg: 0.3167, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 1.23, time: 01m 21s 260ms, time_since_start: 29m 33s 762ms, eta: 04h 34m 05s 663ms\n","\u001b[32m2022-05-02T11:11:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/soft_label_cross_entropy: 0.2745, train/hateful_memes/soft_label_cross_entropy/avg: 0.3034, train/total_loss: 0.2745, train/total_loss/avg: 0.3034, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 1.27, time: 01m 19s 860ms, time_since_start: 30m 53s 623ms, eta: 04h 28m 01s 235ms\n","\u001b[32m2022-05-02T11:13:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/soft_label_cross_entropy: 0.1969, train/hateful_memes/soft_label_cross_entropy/avg: 0.2938, train/total_loss: 0.1969, train/total_loss/avg: 0.2938, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 316ms, time_since_start: 32m 13s 939ms, eta: 04h 28m 11s 236ms\n","\u001b[32m2022-05-02T11:14:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/soft_label_cross_entropy: 0.1299, train/hateful_memes/soft_label_cross_entropy/avg: 0.2832, train/total_loss: 0.1299, train/total_loss/avg: 0.2832, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 113ms, time_since_start: 33m 34s 052ms, eta: 04h 26m 09s 129ms\n","\u001b[32m2022-05-02T11:15:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/soft_label_cross_entropy: 0.1049, train/hateful_memes/soft_label_cross_entropy/avg: 0.2720, train/total_loss: 0.1049, train/total_loss/avg: 0.2720, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 222ms, time_since_start: 34m 54s 274ms, eta: 04h 25m 09s 245ms\n","\u001b[32m2022-05-02T11:17:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0971, train/hateful_memes/soft_label_cross_entropy/avg: 0.2624, train/total_loss: 0.0971, train/total_loss/avg: 0.2624, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 260ms, time_since_start: 36m 14s 535ms, eta: 04h 23m 55s 277ms\n","\u001b[32m2022-05-02T11:18:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0874, train/hateful_memes/soft_label_cross_entropy/avg: 0.2527, train/total_loss: 0.0874, train/total_loss/avg: 0.2527, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 1.27, time: 01m 19s 846ms, time_since_start: 37m 34s 381ms, eta: 04h 21m 12s 307ms\n","\u001b[32m2022-05-02T11:19:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0826, train/hateful_memes/soft_label_cross_entropy/avg: 0.2439, train/total_loss: 0.0826, train/total_loss/avg: 0.2439, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 389ms, time_since_start: 38m 54s 771ms, eta: 04h 21m 37s 208ms\n","\u001b[32m2022-05-02T11:21:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0694, train/hateful_memes/soft_label_cross_entropy/avg: 0.2379, train/total_loss: 0.0694, train/total_loss/avg: 0.2379, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 459ms, time_since_start: 40m 15s 230ms, eta: 04h 20m 28s 939ms\n","\u001b[32m2022-05-02T11:22:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T11:22:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T11:22:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T11:22:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T11:22:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0593, train/hateful_memes/soft_label_cross_entropy/avg: 0.2313, train/total_loss: 0.0593, train/total_loss/avg: 0.2313, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 750ms, time_since_start: 41m 43s 980ms, eta: 04h 45m 49s 196ms\n","\u001b[32m2022-05-02T11:22:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T11:22:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T11:22:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T11:22:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T11:22:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T11:22:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T11:22:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T11:22:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/soft_label_cross_entropy: 1.5590, val/total_loss: 1.5590, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.4953, val/hateful_memes/roc_auc: 0.7103, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 14s 085ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.713132\n","\u001b[32m2022-05-02T11:24:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0694, train/hateful_memes/soft_label_cross_entropy/avg: 0.2289, train/total_loss: 0.0694, train/total_loss/avg: 0.2289, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 1.23, time: 01m 21s 393ms, time_since_start: 43m 19s 461ms, eta: 04h 20m 44s 900ms\n","\u001b[32m2022-05-02T11:25:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0442, train/hateful_memes/soft_label_cross_entropy/avg: 0.2220, train/total_loss: 0.0442, train/total_loss/avg: 0.2220, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 1.27, time: 01m 19s 669ms, time_since_start: 44m 39s 131ms, eta: 04h 13m 52s 563ms\n","\u001b[32m2022-05-02T11:26:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0442, train/hateful_memes/soft_label_cross_entropy/avg: 0.2170, train/total_loss: 0.0442, train/total_loss/avg: 0.2170, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 429ms, time_since_start: 45m 59s 560ms, eta: 04h 14m 55s 996ms\n","\u001b[32m2022-05-02T11:28:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0397, train/hateful_memes/soft_label_cross_entropy/avg: 0.2116, train/total_loss: 0.0397, train/total_loss/avg: 0.2116, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 312ms, time_since_start: 47m 19s 873ms, eta: 04h 13m 12s 152ms\n","\u001b[32m2022-05-02T11:29:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0394, train/hateful_memes/soft_label_cross_entropy/avg: 0.2062, train/total_loss: 0.0394, train/total_loss/avg: 0.2062, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 371ms, time_since_start: 48m 40s 245ms, eta: 04h 12m 01s 499ms\n","\u001b[32m2022-05-02T11:30:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0347, train/hateful_memes/soft_label_cross_entropy/avg: 0.2011, train/total_loss: 0.0347, train/total_loss/avg: 0.2011, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 321ms, time_since_start: 50m 567ms, eta: 04h 10m 30s 471ms\n","\u001b[32m2022-05-02T11:32:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0347, train/hateful_memes/soft_label_cross_entropy/avg: 0.1975, train/total_loss: 0.0347, train/total_loss/avg: 0.1975, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 366ms, time_since_start: 51m 20s 933ms, eta: 04h 09m 17s 151ms\n","\u001b[32m2022-05-02T11:33:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0329, train/hateful_memes/soft_label_cross_entropy/avg: 0.1924, train/total_loss: 0.0329, train/total_loss/avg: 0.1924, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 1.27, time: 01m 19s 688ms, time_since_start: 52m 40s 622ms, eta: 04h 05m 49s 955ms\n","\u001b[32m2022-05-02T11:34:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0229, train/hateful_memes/soft_label_cross_entropy/avg: 0.1875, train/total_loss: 0.0229, train/total_loss/avg: 0.1875, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 1.25, time: 01m 20s 266ms, time_since_start: 54m 889ms, eta: 04h 06m 15s 282ms\n","\u001b[32m2022-05-02T11:36:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T11:36:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T11:36:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T11:36:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T11:36:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0228, train/hateful_memes/soft_label_cross_entropy/avg: 0.1830, train/total_loss: 0.0228, train/total_loss/avg: 0.1830, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 354ms, time_since_start: 55m 29s 244ms, eta: 04h 29m 34s 187ms\n","\u001b[32m2022-05-02T11:36:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T11:36:27 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T11:36:31 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T11:36:31 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T11:36:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T11:36:36 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-02T11:36:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T11:36:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T11:36:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/soft_label_cross_entropy: 1.5961, val/total_loss: 1.5961, val/hateful_memes/accuracy: 0.7259, val/hateful_memes/binary_f1: 0.5978, val/hateful_memes/roc_auc: 0.7228, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 22s 915ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722842\n","\u001b[32m2022-05-02T11:38:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0222, train/hateful_memes/soft_label_cross_entropy/avg: 0.1787, train/total_loss: 0.0222, train/total_loss/avg: 0.1787, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 485ms, time_since_start: 57m 13s 646ms, eta: 04h 07m 13s 942ms\n","\u001b[32m2022-05-02T11:39:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0214, train/hateful_memes/soft_label_cross_entropy/avg: 0.1746, train/total_loss: 0.0214, train/total_loss/avg: 0.1746, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 564ms, time_since_start: 58m 34s 211ms, eta: 04h 03m 04s 333ms\n","\u001b[32m2022-05-02T11:40:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0087, train/hateful_memes/soft_label_cross_entropy/avg: 0.1706, train/total_loss: 0.0087, train/total_loss/avg: 0.1706, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 835ms, time_since_start: 59m 54s 047ms, eta: 03h 59m 31s 095ms\n","\u001b[32m2022-05-02T11:42:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0087, train/hateful_memes/soft_label_cross_entropy/avg: 0.1672, train/total_loss: 0.0087, train/total_loss/avg: 0.1672, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 434ms, time_since_start: 01h 01m 14s 481ms, eta: 03h 59m 57s 186ms\n","\u001b[32m2022-05-02T11:43:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0201, train/hateful_memes/soft_label_cross_entropy/avg: 0.1646, train/total_loss: 0.0201, train/total_loss/avg: 0.1646, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 386ms, time_since_start: 01h 02m 34s 869ms, eta: 03h 58m 26s 876ms\n","\u001b[32m2022-05-02T11:44:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0087, train/hateful_memes/soft_label_cross_entropy/avg: 0.1612, train/total_loss: 0.0087, train/total_loss/avg: 0.1612, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 067ms, time_since_start: 01h 03m 54s 936ms, eta: 03h 56m 08s 614ms\n","\u001b[32m2022-05-02T11:46:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0201, train/hateful_memes/soft_label_cross_entropy/avg: 0.1583, train/total_loss: 0.0201, train/total_loss/avg: 0.1583, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 472ms, time_since_start: 01h 05m 15s 409ms, eta: 03h 55m 58s 427ms\n","\u001b[32m2022-05-02T11:47:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0201, train/hateful_memes/soft_label_cross_entropy/avg: 0.1552, train/total_loss: 0.0201, train/total_loss/avg: 0.1552, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 980ms, time_since_start: 01h 06m 35s 389ms, eta: 03h 53m 10s 522ms\n","\u001b[32m2022-05-02T11:48:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0201, train/hateful_memes/soft_label_cross_entropy/avg: 0.1535, train/total_loss: 0.0201, train/total_loss/avg: 0.1535, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 555ms, time_since_start: 01h 07m 55s 945ms, eta: 03h 53m 29s 234ms\n","\u001b[32m2022-05-02T11:50:14 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T11:50:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T11:50:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T11:50:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T11:50:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0178, train/hateful_memes/soft_label_cross_entropy/avg: 0.1508, train/total_loss: 0.0178, train/total_loss/avg: 0.1508, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 988ms, time_since_start: 01h 09m 24s 933ms, eta: 04h 16m 25s 141ms\n","\u001b[32m2022-05-02T11:50:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T11:50:22 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T11:50:27 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T11:50:27 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T11:50:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T11:50:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T11:50:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T11:50:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/soft_label_cross_entropy: 1.7330, val/total_loss: 1.7330, val/hateful_memes/accuracy: 0.7130, val/hateful_memes/binary_f1: 0.5345, val/hateful_memes/roc_auc: 0.7149, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 16s 786ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722842\n","\u001b[32m2022-05-02T11:52:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0087, train/hateful_memes/soft_label_cross_entropy/avg: 0.1478, train/total_loss: 0.0087, train/total_loss/avg: 0.1478, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 279ms, time_since_start: 01h 11m 03s 001ms, eta: 03h 52m 49s 800ms\n","\u001b[32m2022-05-02T11:53:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0087, train/hateful_memes/soft_label_cross_entropy/avg: 0.1451, train/total_loss: 0.0087, train/total_loss/avg: 0.1451, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 419ms, time_since_start: 01h 12m 23s 420ms, eta: 03h 49m 102ms\n","\u001b[32m2022-05-02T11:54:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0076, train/hateful_memes/soft_label_cross_entropy/avg: 0.1424, train/total_loss: 0.0076, train/total_loss/avg: 0.1424, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 435ms, time_since_start: 01h 13m 43s 856ms, eta: 03h 47m 41s 054ms\n","\u001b[32m2022-05-02T11:56:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0075, train/hateful_memes/soft_label_cross_entropy/avg: 0.1398, train/total_loss: 0.0075, train/total_loss/avg: 0.1398, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 773ms, time_since_start: 01h 15m 03s 629ms, eta: 03h 44m 27s 560ms\n","\u001b[32m2022-05-02T11:57:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0052, train/hateful_memes/soft_label_cross_entropy/avg: 0.1373, train/total_loss: 0.0052, train/total_loss/avg: 0.1373, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 397ms, time_since_start: 01h 16m 24s 027ms, eta: 03h 44m 51s 105ms\n","\u001b[32m2022-05-02T11:58:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0050, train/hateful_memes/soft_label_cross_entropy/avg: 0.1349, train/total_loss: 0.0050, train/total_loss/avg: 0.1349, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 047ms, time_since_start: 01h 17m 44s 074ms, eta: 03h 42m 30s 937ms\n","\u001b[32m2022-05-02T12:00:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0050, train/hateful_memes/soft_label_cross_entropy/avg: 0.1327, train/total_loss: 0.0050, train/total_loss/avg: 0.1327, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 178ms, time_since_start: 01h 19m 04s 253ms, eta: 03h 41m 31s 351ms\n","\u001b[32m2022-05-02T12:01:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0052, train/hateful_memes/soft_label_cross_entropy/avg: 0.1305, train/total_loss: 0.0052, train/total_loss/avg: 0.1305, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 220ms, time_since_start: 01h 20m 24s 474ms, eta: 03h 40m 16s 620ms\n","\u001b[32m2022-05-02T12:02:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0054, train/hateful_memes/soft_label_cross_entropy/avg: 0.1288, train/total_loss: 0.0054, train/total_loss/avg: 0.1288, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 964ms, time_since_start: 01h 21m 44s 438ms, eta: 03h 38m 13s 107ms\n","\u001b[32m2022-05-02T12:04:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T12:04:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T12:04:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T12:04:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T12:04:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0052, train/hateful_memes/soft_label_cross_entropy/avg: 0.1267, train/total_loss: 0.0052, train/total_loss/avg: 0.1267, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 668ms, time_since_start: 01h 23m 13s 107ms, eta: 04h 28s 179ms\n","\u001b[32m2022-05-02T12:04:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T12:04:10 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T12:04:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T12:04:15 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T12:04:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T12:04:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T12:04:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T12:04:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/soft_label_cross_entropy: 1.8895, val/total_loss: 1.8895, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.5242, val/hateful_memes/roc_auc: 0.7133, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 18s 233ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722842\n","\u001b[32m2022-05-02T12:05:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0052, train/hateful_memes/soft_label_cross_entropy/avg: 0.1246, train/total_loss: 0.0052, train/total_loss/avg: 0.1246, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 405ms, time_since_start: 01h 24m 52s 747ms, eta: 03h 39m 23s 520ms\n","\u001b[32m2022-05-02T12:07:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0052, train/hateful_memes/soft_label_cross_entropy/avg: 0.1266, train/total_loss: 0.0052, train/total_loss/avg: 0.1266, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 167ms, time_since_start: 01h 26m 12s 915ms, eta: 03h 34m 41s 857ms\n","\u001b[32m2022-05-02T12:08:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0054, train/hateful_memes/soft_label_cross_entropy/avg: 0.1247, train/total_loss: 0.0054, train/total_loss/avg: 0.1247, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 340ms, time_since_start: 01h 27m 33s 256ms, eta: 03h 33m 47s 935ms\n","\u001b[32m2022-05-02T12:09:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0052, train/hateful_memes/soft_label_cross_entropy/avg: 0.1228, train/total_loss: 0.0052, train/total_loss/avg: 0.1228, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 867ms, time_since_start: 01h 28m 53s 123ms, eta: 03h 31m 11s 157ms\n","\u001b[32m2022-05-02T12:11:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0046, train/hateful_memes/soft_label_cross_entropy/avg: 0.1209, train/total_loss: 0.0046, train/total_loss/avg: 0.1209, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 421ms, time_since_start: 01h 30m 13s 545ms, eta: 03h 31m 17s 231ms\n","\u001b[32m2022-05-02T12:12:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0036, train/hateful_memes/soft_label_cross_entropy/avg: 0.1192, train/total_loss: 0.0036, train/total_loss/avg: 0.1192, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 524ms, time_since_start: 01h 31m 34s 069ms, eta: 03h 30m 11s 595ms\n","\u001b[32m2022-05-02T12:13:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0036, train/hateful_memes/soft_label_cross_entropy/avg: 0.1179, train/total_loss: 0.0036, train/total_loss/avg: 0.1179, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 971ms, time_since_start: 01h 32m 54s 041ms, eta: 03h 27m 23s 648ms\n","\u001b[32m2022-05-02T12:15:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0036, train/hateful_memes/soft_label_cross_entropy/avg: 0.1184, train/total_loss: 0.0036, train/total_loss/avg: 0.1184, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 375ms, time_since_start: 01h 34m 14s 416ms, eta: 03h 27m 04s 721ms\n","\u001b[32m2022-05-02T12:16:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0032, train/hateful_memes/soft_label_cross_entropy/avg: 0.1167, train/total_loss: 0.0032, train/total_loss/avg: 0.1167, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 292ms, time_since_start: 01h 35m 34s 708ms, eta: 03h 25m 30s 223ms\n","\u001b[32m2022-05-02T12:17:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T12:17:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T12:17:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T12:18:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T12:18:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0031, train/hateful_memes/soft_label_cross_entropy/avg: 0.1150, train/total_loss: 0.0031, train/total_loss/avg: 0.1150, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 021ms, time_since_start: 01h 37m 05s 729ms, eta: 03h 51m 25s 265ms\n","\u001b[32m2022-05-02T12:18:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T12:18:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T12:18:08 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T12:18:08 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T12:18:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T12:18:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T12:18:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T12:18:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/soft_label_cross_entropy: 1.7853, val/total_loss: 1.7853, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.5363, val/hateful_memes/roc_auc: 0.7007, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 17s 294ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722842\n","\u001b[32m2022-05-02T12:19:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0031, train/hateful_memes/soft_label_cross_entropy/avg: 0.1134, train/total_loss: 0.0031, train/total_loss/avg: 0.1134, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 1.23, time: 01m 21s 388ms, time_since_start: 01h 38m 44s 414ms, eta: 03h 25m 33s 104ms\n","\u001b[32m2022-05-02T12:21:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0031, train/hateful_memes/soft_label_cross_entropy/avg: 0.1119, train/total_loss: 0.0031, train/total_loss/avg: 0.1119, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 156ms, time_since_start: 01h 40m 04s 571ms, eta: 03h 21m 04s 867ms\n","\u001b[32m2022-05-02T12:22:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0032, train/hateful_memes/soft_label_cross_entropy/avg: 0.1105, train/total_loss: 0.0032, train/total_loss/avg: 0.1105, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 435ms, time_since_start: 01h 41m 25s 006ms, eta: 03h 20m 24s 972ms\n","\u001b[32m2022-05-02T12:23:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0036, train/hateful_memes/soft_label_cross_entropy/avg: 0.1097, train/total_loss: 0.0036, train/total_loss/avg: 0.1097, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 443ms, time_since_start: 01h 42m 45s 450ms, eta: 03h 19m 04s 469ms\n","\u001b[32m2022-05-02T12:25:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0036, train/hateful_memes/soft_label_cross_entropy/avg: 0.1083, train/total_loss: 0.0036, train/total_loss/avg: 0.1083, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 1.27, time: 01m 19s 995ms, time_since_start: 01h 44m 05s 445ms, eta: 03h 16m 36s 470ms\n","\u001b[32m2022-05-02T12:26:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0036, train/hateful_memes/soft_label_cross_entropy/avg: 0.1068, train/total_loss: 0.0036, train/total_loss/avg: 0.1068, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 611ms, time_since_start: 01h 45m 26s 057ms, eta: 03h 16m 45s 412ms\n","\u001b[32m2022-05-02T12:27:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0036, train/hateful_memes/soft_label_cross_entropy/avg: 0.1055, train/total_loss: 0.0036, train/total_loss/avg: 0.1055, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 414ms, time_since_start: 01h 46m 46s 472ms, eta: 03h 14m 54s 829ms\n","\u001b[32m2022-05-02T12:29:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0032, train/hateful_memes/soft_label_cross_entropy/avg: 0.1042, train/total_loss: 0.0032, train/total_loss/avg: 0.1042, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 087ms, time_since_start: 01h 48m 06s 559ms, eta: 03h 12m 45s 746ms\n","\u001b[32m2022-05-02T12:30:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0032, train/hateful_memes/soft_label_cross_entropy/avg: 0.1029, train/total_loss: 0.0032, train/total_loss/avg: 0.1029, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 1.25, time: 01m 20s 524ms, time_since_start: 01h 49m 27s 083ms, eta: 03h 12m 26s 927ms\n","\u001b[32m2022-05-02T12:31:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T12:31:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T12:31:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T12:31:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T12:31:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0032, train/hateful_memes/soft_label_cross_entropy/avg: 0.1016, train/total_loss: 0.0032, train/total_loss/avg: 0.1016, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 752ms, time_since_start: 01h 50m 55s 836ms, eta: 03h 30m 36s 532ms\n","\u001b[32m2022-05-02T12:31:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T12:31:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T12:31:58 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T12:31:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T12:31:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T12:32:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T12:32:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T12:32:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/soft_label_cross_entropy: 2.0911, val/total_loss: 2.0911, val/hateful_memes/accuracy: 0.7148, val/hateful_memes/binary_f1: 0.5032, val/hateful_memes/roc_auc: 0.6991, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 14s 432ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.722842\n"]}],"source":["!python tools/run.py config=mmf/projects/hateful_memes/configs/visual_bert/from_coco_loss.yml \\\n","training.batch_size=32 \\\n","model=visual_bert dataset=hateful_memes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qVwu-E9bugBf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651462025512,"user_tz":300,"elapsed":4757776,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"}},"outputId":"e917a283-3656-4883-c641-fea25a04c323"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-02T02:07:56 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf/projects/hateful_memes/configs/visual_bert/defaults_loss.yml\n","\u001b[32m2022-05-02T02:07:56 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n","\u001b[32m2022-05-02T02:07:56 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-05-02T02:07:56 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-02T02:07:56 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-02T02:07:56 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf/projects/hateful_memes/configs/visual_bert/defaults_loss.yml', 'training.batch_size=32', 'model=visual_bert', 'dataset=hateful_memes'])\n","\u001b[32m2022-05-02T02:07:56 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-02T02:07:56 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-05-02T02:07:56 | mmf_cli.run: \u001b[0mUsing seed 27716496\n","\u001b[32m2022-05-02T02:07:56 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [08:15<00:00, 20.8MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 154kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmzoebpab\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 22.0kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxjex2s4i\n","Downloading: 100% 570/570 [00:00<00:00, 387kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8lm70hea\n","Downloading: 100% 232k/232k [00:00<00:00, 910kB/s] \n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvs8k5ji7\n","Downloading: 100% 466k/466k [00:00<00:00, 1.37MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-02T02:19:13 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T02:19:13 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T02:19:13 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T02:19:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"soft_label_cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmpaglt3z4x\n","Downloading: 100% 440M/440M [00:05<00:00, 76.6MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-05-02T02:19:30 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-02T02:19:30 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-02T02:19:30 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-05-02T02:19:30 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): SoftLabelCrossEntropyLoss()\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-05-02T02:19:30 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-05-02T02:19:30 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-05-02T02:20:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/soft_label_cross_entropy: 0.7451, train/hateful_memes/soft_label_cross_entropy/avg: 0.7451, train/total_loss: 0.7451, train/total_loss/avg: 0.7451, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 2.08, time: 48s 190ms, time_since_start: 48s 223ms, eta: 02h 58m 53s 205ms\n","\u001b[32m2022-05-02T02:21:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/soft_label_cross_entropy: 0.7451, train/hateful_memes/soft_label_cross_entropy/avg: 0.7580, train/total_loss: 0.7451, train/total_loss/avg: 0.7580, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 423ms, time_since_start: 01m 35s 647ms, eta: 02h 55m 14s 020ms\n","\u001b[32m2022-05-02T02:21:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/soft_label_cross_entropy: 0.7451, train/hateful_memes/soft_label_cross_entropy/avg: 0.6954, train/total_loss: 0.7451, train/total_loss/avg: 0.6954, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 181ms, time_since_start: 02m 22s 829ms, eta: 02h 53m 32s 519ms\n","\u001b[32m2022-05-02T02:22:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/soft_label_cross_entropy: 0.5704, train/hateful_memes/soft_label_cross_entropy/avg: 0.6564, train/total_loss: 0.5704, train/total_loss/avg: 0.6564, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 411ms, time_since_start: 03m 10s 240ms, eta: 02h 53m 34s 938ms\n","\u001b[32m2022-05-02T02:23:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/soft_label_cross_entropy: 0.6315, train/hateful_memes/soft_label_cross_entropy/avg: 0.6515, train/total_loss: 0.6315, train/total_loss/avg: 0.6515, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 460ms, time_since_start: 03m 57s 700ms, eta: 02h 52m 57s 401ms\n","\u001b[32m2022-05-02T02:24:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/soft_label_cross_entropy: 0.5704, train/hateful_memes/soft_label_cross_entropy/avg: 0.6296, train/total_loss: 0.5704, train/total_loss/avg: 0.6296, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 173ms, time_since_start: 04m 44s 873ms, eta: 02h 51m 06s 662ms\n","\u001b[32m2022-05-02T02:25:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/soft_label_cross_entropy: 0.5704, train/hateful_memes/soft_label_cross_entropy/avg: 0.5961, train/total_loss: 0.5704, train/total_loss/avg: 0.5961, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 388ms, time_since_start: 05m 32s 262ms, eta: 02h 51m 05s 345ms\n","\u001b[32m2022-05-02T02:25:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/soft_label_cross_entropy: 0.5394, train/hateful_memes/soft_label_cross_entropy/avg: 0.5748, train/total_loss: 0.5394, train/total_loss/avg: 0.5748, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 987ms, time_since_start: 06m 19s 250ms, eta: 02h 48m 50s 739ms\n","\u001b[32m2022-05-02T02:26:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/soft_label_cross_entropy: 0.5394, train/hateful_memes/soft_label_cross_entropy/avg: 0.5680, train/total_loss: 0.5394, train/total_loss/avg: 0.5680, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 120ms, time_since_start: 07m 06s 370ms, eta: 02h 48m 31s 544ms\n","\u001b[32m2022-05-02T02:27:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T02:27:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T02:27:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T02:27:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T02:27:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/soft_label_cross_entropy: 0.5204, train/hateful_memes/soft_label_cross_entropy/avg: 0.5514, train/total_loss: 0.5204, train/total_loss/avg: 0.5514, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.79, time: 56s 193ms, time_since_start: 08m 02s 564ms, eta: 03h 20m 01s 149ms\n","\u001b[32m2022-05-02T02:27:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T02:27:32 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T02:27:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T02:27:39 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T02:27:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T02:27:44 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-02T02:27:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T02:27:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T02:27:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/soft_label_cross_entropy: 0.8296, val/total_loss: 0.8296, val/hateful_memes/accuracy: 0.6463, val/hateful_memes/binary_f1: 0.3298, val/hateful_memes/roc_auc: 0.6681, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 23s 662ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.668103\n","\u001b[32m2022-05-02T02:28:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/soft_label_cross_entropy: 0.5204, train/hateful_memes/soft_label_cross_entropy/avg: 0.5209, train/total_loss: 0.5204, train/total_loss/avg: 0.5209, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 661ms, time_since_start: 09m 13s 889ms, eta: 02h 48m 50s 493ms\n","\u001b[32m2022-05-02T02:29:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/soft_label_cross_entropy: 0.5137, train/hateful_memes/soft_label_cross_entropy/avg: 0.5114, train/total_loss: 0.5137, train/total_loss/avg: 0.5114, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 413ms, time_since_start: 10m 01s 303ms, eta: 02h 47m 09s 700ms\n","\u001b[32m2022-05-02T02:30:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/soft_label_cross_entropy: 0.5137, train/hateful_memes/soft_label_cross_entropy/avg: 0.4833, train/total_loss: 0.5137, train/total_loss/avg: 0.4833, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 242ms, time_since_start: 10m 48s 545ms, eta: 02h 45m 45s 520ms\n","\u001b[32m2022-05-02T02:31:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/soft_label_cross_entropy: 0.4821, train/hateful_memes/soft_label_cross_entropy/avg: 0.4832, train/total_loss: 0.4821, train/total_loss/avg: 0.4832, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 191ms, time_since_start: 11m 35s 737ms, eta: 02h 44m 46s 779ms\n","\u001b[32m2022-05-02T02:31:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/soft_label_cross_entropy: 0.4821, train/hateful_memes/soft_label_cross_entropy/avg: 0.4633, train/total_loss: 0.4821, train/total_loss/avg: 0.4633, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 419ms, time_since_start: 12m 23s 157ms, eta: 02h 44m 46s 274ms\n","\u001b[32m2022-05-02T02:32:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/soft_label_cross_entropy: 0.4253, train/hateful_memes/soft_label_cross_entropy/avg: 0.4451, train/total_loss: 0.4253, train/total_loss/avg: 0.4451, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 2.17, time: 46s 937ms, time_since_start: 13m 10s 094ms, eta: 02h 42m 18s 029ms\n","\u001b[32m2022-05-02T02:33:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/soft_label_cross_entropy: 0.4253, train/hateful_memes/soft_label_cross_entropy/avg: 0.4291, train/total_loss: 0.4253, train/total_loss/avg: 0.4291, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 317ms, time_since_start: 13m 57s 412ms, eta: 02h 42m 48s 715ms\n","\u001b[32m2022-05-02T02:34:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/soft_label_cross_entropy: 0.4075, train/hateful_memes/soft_label_cross_entropy/avg: 0.4221, train/total_loss: 0.4075, train/total_loss/avg: 0.4221, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 295ms, time_since_start: 14m 44s 707ms, eta: 02h 41m 56s 152ms\n","\u001b[32m2022-05-02T02:35:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/soft_label_cross_entropy: 0.4075, train/hateful_memes/soft_label_cross_entropy/avg: 0.4068, train/total_loss: 0.4075, train/total_loss/avg: 0.4068, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 320ms, time_since_start: 15m 32s 028ms, eta: 02h 41m 13s 098ms\n","\u001b[32m2022-05-02T02:35:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T02:35:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T02:35:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T02:35:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T02:35:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/soft_label_cross_entropy: 0.4018, train/hateful_memes/soft_label_cross_entropy/avg: 0.3906, train/total_loss: 0.4018, train/total_loss/avg: 0.3906, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.79, time: 56s 456ms, time_since_start: 16m 28s 484ms, eta: 03h 11m 23s 156ms\n","\u001b[32m2022-05-02T02:35:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T02:35:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T02:36:04 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T02:36:04 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T02:36:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T02:36:08 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-02T02:36:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T02:36:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T02:36:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/soft_label_cross_entropy: 1.1042, val/total_loss: 1.1042, val/hateful_memes/accuracy: 0.6537, val/hateful_memes/binary_f1: 0.3618, val/hateful_memes/roc_auc: 0.6846, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 25s 789ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.684632\n","\u001b[32m2022-05-02T02:37:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/soft_label_cross_entropy: 0.3953, train/hateful_memes/soft_label_cross_entropy/avg: 0.3749, train/total_loss: 0.3953, train/total_loss/avg: 0.3749, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 229ms, time_since_start: 17m 42s 506ms, eta: 02h 42m 40s 875ms\n","\u001b[32m2022-05-02T02:37:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/soft_label_cross_entropy: 0.3029, train/hateful_memes/soft_label_cross_entropy/avg: 0.3605, train/total_loss: 0.3029, train/total_loss/avg: 0.3605, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 190ms, time_since_start: 18m 29s 696ms, eta: 02h 38m 22s 500ms\n","\u001b[32m2022-05-02T02:38:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/soft_label_cross_entropy: 0.3029, train/hateful_memes/soft_label_cross_entropy/avg: 0.3621, train/total_loss: 0.3029, train/total_loss/avg: 0.3621, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 539ms, time_since_start: 19m 17s 235ms, eta: 02h 38m 44s 422ms\n","\u001b[32m2022-05-02T02:39:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/soft_label_cross_entropy: 0.2159, train/hateful_memes/soft_label_cross_entropy/avg: 0.3547, train/total_loss: 0.2159, train/total_loss/avg: 0.3547, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 442ms, time_since_start: 20m 04s 677ms, eta: 02h 37m 36s 726ms\n","\u001b[32m2022-05-02T02:40:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/soft_label_cross_entropy: 0.1845, train/hateful_memes/soft_label_cross_entropy/avg: 0.3444, train/total_loss: 0.1845, train/total_loss/avg: 0.3444, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 381ms, time_since_start: 20m 52s 059ms, eta: 02h 36m 36s 482ms\n","\u001b[32m2022-05-02T02:41:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/soft_label_cross_entropy: 0.1838, train/hateful_memes/soft_label_cross_entropy/avg: 0.3382, train/total_loss: 0.1838, train/total_loss/avg: 0.3382, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 549ms, time_since_start: 21m 39s 609ms, eta: 02h 36m 21s 421ms\n","\u001b[32m2022-05-02T02:41:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/soft_label_cross_entropy: 0.1838, train/hateful_memes/soft_label_cross_entropy/avg: 0.3353, train/total_loss: 0.1838, train/total_loss/avg: 0.3353, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 079ms, time_since_start: 22m 26s 688ms, eta: 02h 34m 802ms\n","\u001b[32m2022-05-02T02:42:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/soft_label_cross_entropy: 0.1836, train/hateful_memes/soft_label_cross_entropy/avg: 0.3248, train/total_loss: 0.1836, train/total_loss/avg: 0.3248, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 390ms, time_since_start: 23m 14s 079ms, eta: 02h 34m 13s 748ms\n","\u001b[32m2022-05-02T02:43:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/soft_label_cross_entropy: 0.1738, train/hateful_memes/soft_label_cross_entropy/avg: 0.3155, train/total_loss: 0.1738, train/total_loss/avg: 0.3155, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 492ms, time_since_start: 24m 01s 571ms, eta: 02h 33m 45s 256ms\n","\u001b[32m2022-05-02T02:44:19 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T02:44:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T02:44:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T02:44:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T02:44:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/soft_label_cross_entropy: 0.1713, train/hateful_memes/soft_label_cross_entropy/avg: 0.3055, train/total_loss: 0.1713, train/total_loss/avg: 0.3055, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.75, time: 57s 549ms, time_since_start: 24m 59s 120ms, eta: 03h 05m 20s 202ms\n","\u001b[32m2022-05-02T02:44:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T02:44:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T02:44:31 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T02:44:31 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T02:44:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T02:44:39 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-02T02:44:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T02:44:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T02:44:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/soft_label_cross_entropy: 1.4288, val/total_loss: 1.4288, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.5074, val/hateful_memes/roc_auc: 0.6966, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 21s 061ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.696629\n","\u001b[32m2022-05-02T02:45:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/soft_label_cross_entropy: 0.1459, train/hateful_memes/soft_label_cross_entropy/avg: 0.2959, train/total_loss: 0.1459, train/total_loss/avg: 0.2959, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 295ms, time_since_start: 26m 08s 479ms, eta: 02h 34m 42s 980ms\n","\u001b[32m2022-05-02T02:46:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/soft_label_cross_entropy: 0.1321, train/hateful_memes/soft_label_cross_entropy/avg: 0.2869, train/total_loss: 0.1321, train/total_loss/avg: 0.2869, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 668ms, time_since_start: 26m 56s 148ms, eta: 02h 31m 54s 098ms\n","\u001b[32m2022-05-02T02:47:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0982, train/hateful_memes/soft_label_cross_entropy/avg: 0.2784, train/total_loss: 0.0982, train/total_loss/avg: 0.2784, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 633ms, time_since_start: 27m 43s 782ms, eta: 02h 30m 58s 974ms\n","\u001b[32m2022-05-02T02:48:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0823, train/hateful_memes/soft_label_cross_entropy/avg: 0.2714, train/total_loss: 0.0823, train/total_loss/avg: 0.2714, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 556ms, time_since_start: 28m 31s 338ms, eta: 02h 29m 55s 817ms\n","\u001b[32m2022-05-02T02:48:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0605, train/hateful_memes/soft_label_cross_entropy/avg: 0.2637, train/total_loss: 0.0605, train/total_loss/avg: 0.2637, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 282ms, time_since_start: 29m 18s 621ms, eta: 02h 28m 15s 952ms\n","\u001b[32m2022-05-02T02:49:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0595, train/hateful_memes/soft_label_cross_entropy/avg: 0.2565, train/total_loss: 0.0595, train/total_loss/avg: 0.2565, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 519ms, time_since_start: 30m 06s 140ms, eta: 02h 28m 12s 293ms\n","\u001b[32m2022-05-02T02:50:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0595, train/hateful_memes/soft_label_cross_entropy/avg: 0.2515, train/total_loss: 0.0595, train/total_loss/avg: 0.2515, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 515ms, time_since_start: 30m 53s 656ms, eta: 02h 27m 23s 248ms\n","\u001b[32m2022-05-02T02:51:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0595, train/hateful_memes/soft_label_cross_entropy/avg: 0.2497, train/total_loss: 0.0595, train/total_loss/avg: 0.2497, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 049ms, time_since_start: 31m 40s 706ms, eta: 02h 25m 08s 576ms\n","\u001b[32m2022-05-02T02:51:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0562, train/hateful_memes/soft_label_cross_entropy/avg: 0.2436, train/total_loss: 0.0562, train/total_loss/avg: 0.2436, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 661ms, time_since_start: 32m 28s 368ms, eta: 02h 26m 13s 449ms\n","\u001b[32m2022-05-02T02:52:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T02:52:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T02:52:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T02:52:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T02:52:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0415, train/hateful_memes/soft_label_cross_entropy/avg: 0.2381, train/total_loss: 0.0415, train/total_loss/avg: 0.2381, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.69, time: 59s 059ms, time_since_start: 33m 27s 427ms, eta: 03h 11s 421ms\n","\u001b[32m2022-05-02T02:52:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T02:52:57 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T02:53:00 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T02:53:00 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T02:53:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T02:53:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T02:53:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T02:53:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/soft_label_cross_entropy: 1.7504, val/total_loss: 1.7504, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4709, val/hateful_memes/roc_auc: 0.6735, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 15s 143ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.696629\n","\u001b[32m2022-05-02T02:54:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0388, train/hateful_memes/soft_label_cross_entropy/avg: 0.2323, train/total_loss: 0.0388, train/total_loss/avg: 0.2323, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 410ms, time_since_start: 34m 30s 983ms, eta: 02h 26m 52s 782ms\n","\u001b[32m2022-05-02T02:54:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0217, train/hateful_memes/soft_label_cross_entropy/avg: 0.2269, train/total_loss: 0.0217, train/total_loss/avg: 0.2269, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 610ms, time_since_start: 35m 18s 593ms, eta: 02h 23m 38s 777ms\n","\u001b[32m2022-05-02T02:55:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0144, train/hateful_memes/soft_label_cross_entropy/avg: 0.2217, train/total_loss: 0.0144, train/total_loss/avg: 0.2217, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 170ms, time_since_start: 36m 05s 764ms, eta: 02h 21m 31s 114ms\n","\u001b[32m2022-05-02T02:56:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0144, train/hateful_memes/soft_label_cross_entropy/avg: 0.2176, train/total_loss: 0.0144, train/total_loss/avg: 0.2176, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 528ms, time_since_start: 36m 53s 292ms, eta: 02h 21m 47s 207ms\n","\u001b[32m2022-05-02T02:57:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0122, train/hateful_memes/soft_label_cross_entropy/avg: 0.2128, train/total_loss: 0.0122, train/total_loss/avg: 0.2128, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 737ms, time_since_start: 37m 41s 030ms, eta: 02h 21m 36s 142ms\n","\u001b[32m2022-05-02T02:57:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0090, train/hateful_memes/soft_label_cross_entropy/avg: 0.2082, train/total_loss: 0.0090, train/total_loss/avg: 0.2082, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 595ms, time_since_start: 38m 28s 626ms, eta: 02h 20m 22s 476ms\n","\u001b[32m2022-05-02T02:58:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0090, train/hateful_memes/soft_label_cross_entropy/avg: 0.2042, train/total_loss: 0.0090, train/total_loss/avg: 0.2042, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 697ms, time_since_start: 39m 16s 323ms, eta: 02h 19m 51s 909ms\n","\u001b[32m2022-05-02T02:59:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0090, train/hateful_memes/soft_label_cross_entropy/avg: 0.2012, train/total_loss: 0.0090, train/total_loss/avg: 0.2012, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 176ms, time_since_start: 40m 03s 500ms, eta: 02h 17m 32s 357ms\n","\u001b[32m2022-05-02T03:00:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0087, train/hateful_memes/soft_label_cross_entropy/avg: 0.1971, train/total_loss: 0.0087, train/total_loss/avg: 0.1971, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 714ms, time_since_start: 40m 51s 215ms, eta: 02h 18m 17s 952ms\n","\u001b[32m2022-05-02T03:01:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T03:01:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T03:01:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T03:01:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T03:01:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0057, train/hateful_memes/soft_label_cross_entropy/avg: 0.1931, train/total_loss: 0.0057, train/total_loss/avg: 0.1931, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.75, time: 57s 726ms, time_since_start: 41m 48s 941ms, eta: 02h 46m 20s 298ms\n","\u001b[32m2022-05-02T03:01:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T03:01:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T03:01:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T03:01:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T03:01:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T03:01:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T03:01:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T03:01:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/soft_label_cross_entropy: 2.4213, val/total_loss: 2.4213, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4175, val/hateful_memes/roc_auc: 0.6762, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 17s 169ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.696629\n","\u001b[32m2022-05-02T03:02:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0053, train/hateful_memes/soft_label_cross_entropy/avg: 0.1894, train/total_loss: 0.0053, train/total_loss/avg: 0.1894, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 838ms, time_since_start: 42m 55s 709ms, eta: 02h 19m 54s 069ms\n","\u001b[32m2022-05-02T03:03:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0053, train/hateful_memes/soft_label_cross_entropy/avg: 0.1875, train/total_loss: 0.0053, train/total_loss/avg: 0.1875, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 208ms, time_since_start: 43m 43s 918ms, eta: 02h 17m 16s 726ms\n","\u001b[32m2022-05-02T03:04:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0057, train/hateful_memes/soft_label_cross_entropy/avg: 0.1854, train/total_loss: 0.0057, train/total_loss/avg: 0.1854, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 136ms, time_since_start: 44m 32s 055ms, eta: 02h 16m 15s 529ms\n","\u001b[32m2022-05-02T03:04:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0045, train/hateful_memes/soft_label_cross_entropy/avg: 0.1820, train/total_loss: 0.0045, train/total_loss/avg: 0.1820, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 704ms, time_since_start: 45m 19s 759ms, eta: 02h 14m 13s 560ms\n","\u001b[32m2022-05-02T03:05:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0057, train/hateful_memes/soft_label_cross_entropy/avg: 0.1788, train/total_loss: 0.0057, train/total_loss/avg: 0.1788, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 972ms, time_since_start: 46m 07s 731ms, eta: 02h 14m 09s 981ms\n","\u001b[32m2022-05-02T03:06:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0077, train/hateful_memes/soft_label_cross_entropy/avg: 0.1758, train/total_loss: 0.0077, train/total_loss/avg: 0.1758, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 945ms, time_since_start: 46m 55s 676ms, eta: 02h 13m 16s 659ms\n","\u001b[32m2022-05-02T03:07:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0057, train/hateful_memes/soft_label_cross_entropy/avg: 0.1728, train/total_loss: 0.0057, train/total_loss/avg: 0.1728, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 024ms, time_since_start: 47m 43s 701ms, eta: 02h 12m 41s 029ms\n","\u001b[32m2022-05-02T03:08:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0045, train/hateful_memes/soft_label_cross_entropy/avg: 0.1698, train/total_loss: 0.0045, train/total_loss/avg: 0.1698, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 015ms, time_since_start: 48m 31s 716ms, eta: 02h 11m 50s 792ms\n","\u001b[32m2022-05-02T03:08:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0045, train/hateful_memes/soft_label_cross_entropy/avg: 0.1682, train/total_loss: 0.0045, train/total_loss/avg: 0.1682, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 534ms, time_since_start: 49m 19s 251ms, eta: 02h 09m 43s 219ms\n","\u001b[32m2022-05-02T03:09:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T03:09:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T03:09:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T03:09:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T03:09:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0045, train/hateful_memes/soft_label_cross_entropy/avg: 0.1655, train/total_loss: 0.0045, train/total_loss/avg: 0.1655, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.79, time: 56s 496ms, time_since_start: 50m 15s 748ms, eta: 02h 33m 13s 100ms\n","\u001b[32m2022-05-02T03:09:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T03:09:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T03:09:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T03:09:49 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T03:09:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T03:09:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T03:10:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T03:10:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/soft_label_cross_entropy: 1.6595, val/total_loss: 1.6595, val/hateful_memes/accuracy: 0.6741, val/hateful_memes/binary_f1: 0.4699, val/hateful_memes/roc_auc: 0.6669, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 14s 129ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.696629\n","\u001b[32m2022-05-02T03:10:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0045, train/hateful_memes/soft_label_cross_entropy/avg: 0.1629, train/total_loss: 0.0045, train/total_loss/avg: 0.1629, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 724ms, time_since_start: 51m 18s 602ms, eta: 02h 11m 18s 855ms\n","\u001b[32m2022-05-02T03:11:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0045, train/hateful_memes/soft_label_cross_entropy/avg: 0.1605, train/total_loss: 0.0045, train/total_loss/avg: 0.1605, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 724ms, time_since_start: 52m 06s 327ms, eta: 02h 07m 48s 736ms\n","\u001b[32m2022-05-02T03:12:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0045, train/hateful_memes/soft_label_cross_entropy/avg: 0.1579, train/total_loss: 0.0045, train/total_loss/avg: 0.1579, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 924ms, time_since_start: 52m 54s 252ms, eta: 02h 07m 32s 019ms\n","\u001b[32m2022-05-02T03:13:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0043, train/hateful_memes/soft_label_cross_entropy/avg: 0.1555, train/total_loss: 0.0043, train/total_loss/avg: 0.1555, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 431ms, time_since_start: 53m 41s 683ms, eta: 02h 05m 25s 115ms\n","\u001b[32m2022-05-02T03:13:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0043, train/hateful_memes/soft_label_cross_entropy/avg: 0.1531, train/total_loss: 0.0043, train/total_loss/avg: 0.1531, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 775ms, time_since_start: 54m 29s 459ms, eta: 02h 05m 31s 093ms\n","\u001b[32m2022-05-02T03:14:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0045, train/hateful_memes/soft_label_cross_entropy/avg: 0.1569, train/total_loss: 0.0045, train/total_loss/avg: 0.1569, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 882ms, time_since_start: 55m 17s 341ms, eta: 02h 04m 59s 251ms\n","\u001b[32m2022-05-02T03:15:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0043, train/hateful_memes/soft_label_cross_entropy/avg: 0.1546, train/total_loss: 0.0043, train/total_loss/avg: 0.1546, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 799ms, time_since_start: 56m 05s 141ms, eta: 02h 03m 57s 623ms\n","\u001b[32m2022-05-02T03:16:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0039, train/hateful_memes/soft_label_cross_entropy/avg: 0.1524, train/total_loss: 0.0039, train/total_loss/avg: 0.1524, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 953ms, time_since_start: 56m 53s 094ms, eta: 02h 03m 32s 790ms\n","\u001b[32m2022-05-02T03:17:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0043, train/hateful_memes/soft_label_cross_entropy/avg: 0.1543, train/total_loss: 0.0043, train/total_loss/avg: 0.1543, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 956ms, time_since_start: 57m 41s 050ms, eta: 02h 02m 44s 468ms\n","\u001b[32m2022-05-02T03:17:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T03:17:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T03:18:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T03:18:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T03:18:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0043, train/hateful_memes/soft_label_cross_entropy/avg: 0.1521, train/total_loss: 0.0043, train/total_loss/avg: 0.1521, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.75, time: 57s 630ms, time_since_start: 58m 38s 681ms, eta: 02h 26m 31s 566ms\n","\u001b[32m2022-05-02T03:18:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T03:18:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T03:18:11 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T03:18:11 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T03:18:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T03:18:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T03:18:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T03:18:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/soft_label_cross_entropy: 1.9994, val/total_loss: 1.9994, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.4487, val/hateful_memes/roc_auc: 0.6881, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 14s 877ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.696629\n","\u001b[32m2022-05-02T03:19:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/soft_label_cross_entropy: 0.0043, train/hateful_memes/soft_label_cross_entropy/avg: 0.1501, train/total_loss: 0.0043, train/total_loss/avg: 0.1501, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 703ms, time_since_start: 59m 42s 263ms, eta: 02h 03m 148ms\n","\u001b[32m2022-05-02T03:20:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/soft_label_cross_entropy: 0.0043, train/hateful_memes/soft_label_cross_entropy/avg: 0.1541, train/total_loss: 0.0043, train/total_loss/avg: 0.1541, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 960ms, time_since_start: 01h 30s 223ms, eta: 02h 18s 782ms\n","\u001b[32m2022-05-02T03:20:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/soft_label_cross_entropy: 0.0043, train/hateful_memes/soft_label_cross_entropy/avg: 0.1521, train/total_loss: 0.0043, train/total_loss/avg: 0.1521, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 282ms, time_since_start: 01h 01m 18s 505ms, eta: 02h 18s 121ms\n","\u001b[32m2022-05-02T03:21:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/soft_label_cross_entropy: 0.0061, train/hateful_memes/soft_label_cross_entropy/avg: 0.1504, train/total_loss: 0.0061, train/total_loss/avg: 0.1504, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 080ms, time_since_start: 01h 02m 06s 586ms, eta: 01h 58m 59s 144ms\n","\u001b[32m2022-05-02T03:22:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/soft_label_cross_entropy: 0.0043, train/hateful_memes/soft_label_cross_entropy/avg: 0.1485, train/total_loss: 0.0043, train/total_loss/avg: 0.1485, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 723ms, time_since_start: 01h 02m 54s 310ms, eta: 01h 57m 17s 525ms\n","\u001b[32m2022-05-02T03:23:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/soft_label_cross_entropy: 0.0039, train/hateful_memes/soft_label_cross_entropy/avg: 0.1465, train/total_loss: 0.0039, train/total_loss/avg: 0.1465, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 085ms, time_since_start: 01h 03m 42s 396ms, eta: 01h 57m 22s 095ms\n","\u001b[32m2022-05-02T03:24:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/soft_label_cross_entropy: 0.0039, train/hateful_memes/soft_label_cross_entropy/avg: 0.1446, train/total_loss: 0.0039, train/total_loss/avg: 0.1446, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 954ms, time_since_start: 01h 04m 30s 351ms, eta: 01h 56m 14s 138ms\n","\u001b[32m2022-05-02T03:24:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/soft_label_cross_entropy: 0.0039, train/hateful_memes/soft_label_cross_entropy/avg: 0.1428, train/total_loss: 0.0039, train/total_loss/avg: 0.1428, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 894ms, time_since_start: 01h 05m 18s 245ms, eta: 01h 55m 16s 603ms\n","\u001b[32m2022-05-02T03:25:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/soft_label_cross_entropy: 0.0038, train/hateful_memes/soft_label_cross_entropy/avg: 0.1410, train/total_loss: 0.0038, train/total_loss/avg: 0.1410, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 105ms, time_since_start: 01h 06m 06s 350ms, eta: 01h 54m 58s 187ms\n","\u001b[32m2022-05-02T03:26:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T03:26:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T03:26:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T03:26:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T03:26:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/soft_label_cross_entropy: 0.0038, train/hateful_memes/soft_label_cross_entropy/avg: 0.1406, train/total_loss: 0.0038, train/total_loss/avg: 0.1406, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.72, time: 58s 411ms, time_since_start: 01h 07m 04s 762ms, eta: 02h 18m 36s 650ms\n","\u001b[32m2022-05-02T03:26:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T03:26:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T03:26:38 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T03:26:38 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T03:26:38 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-05-02T03:26:38 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-05-02T03:26:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-02T03:26:43 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n","\u001b[32m2022-05-02T03:26:43 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n","\u001b[32m2022-05-02T03:26:43 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 12\n","\u001b[32m2022-05-02T03:26:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/soft_label_cross_entropy: 1.9875, val/total_loss: 1.9875, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.5071, val/hateful_memes/roc_auc: 0.6872, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 09s 965ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.696629\n","\u001b[32m2022-05-02T03:26:45 | mmf.trainers.core.training_loop: \u001b[0mEarly stopping activated\n","\u001b[32m2022-05-02T03:26:45 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2022-05-02T03:26:45 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-05-02T03:26:45 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-05-02T03:26:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-02T03:26:46 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n","\u001b[32m2022-05-02T03:26:46 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n","\u001b[32m2022-05-02T03:26:46 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 12\n","\u001b[32m2022-05-02T03:26:48 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-05-02T03:26:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:15<00:00,  4.10it/s]\n","\u001b[32m2022-05-02T03:27:03 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 63\n","\u001b[32m2022-05-02T03:27:03 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T03:27:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, test/hateful_memes/soft_label_cross_entropy: 1.3093, test/total_loss: 1.3093, test/hateful_memes/accuracy: 0.7085, test/hateful_memes/binary_f1: 0.5113, test/hateful_memes/roc_auc: 0.7428\n","\u001b[32m2022-05-02T03:27:03 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 07m 33s 578ms\n"]}],"source":["# visual bert bs 32 default change loss function\n","\n","!python tools/run.py config=mmf/projects/hateful_memes/configs/visual_bert/defaults_loss.yml \\\n","training.batch_size=32 \\\n","model=visual_bert dataset=hateful_memes"]},{"cell_type":"code","source":["# visua bert coco bs 32 defaut + da + resize\n","\n","!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_da_resize.yml \\\n","training.batch_size=32 \\\n","model=visual_bert dataset=hateful_memes"],"metadata":{"id":"Mj9uZLUhhg8t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651513226910,"user_tz":300,"elapsed":490421,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"}},"outputId":"cfbb3288-c9c3-4d29-a87f-62fe9641e834"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n","  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n","\u001b[32m2022-05-02T16:20:19 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_da_resize.yml\n","\u001b[32m2022-05-02T16:20:19 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n","\u001b[32m2022-05-02T16:20:19 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2022-05-02T16:20:19 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  category=UserWarning,\n","\u001b[32m2022-05-02T16:20:19 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2022-05-02T16:20:19 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/visual_bert/from_coco_da_resize.yml', 'training.batch_size=32', 'model=visual_bert', 'dataset=hateful_memes'])\n","\u001b[32m2022-05-02T16:20:19 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n","\u001b[32m2022-05-02T16:20:19 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla V100-SXM2-16GB\n","\u001b[32m2022-05-02T16:20:19 | mmf_cli.run: \u001b[0mUsing seed 48425275\n","\u001b[32m2022-05-02T16:20:19 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [09:35<00:00, 17.9MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 152kB/s] \n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppxobbplz\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 19.4kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdrdhv2cu\n","Downloading: 100% 570/570 [00:00<00:00, 456kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpj4p0hmfx\n","Downloading: 100% 232k/232k [00:00<00:00, 685kB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9b5qaoy4\n","Downloading: 100% 466k/466k [00:00<00:00, 1.11MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2022-05-02T16:32:53 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T16:32:53 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T16:32:53 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2022-05-02T16:32:53 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.10.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmpb2cfepa1\n","Downloading: 100% 440M/440M [00:06<00:00, 72.3MB/s]\n","storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.projection.bias', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.projection.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2022-05-02T16:33:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2022-05-02T16:33:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2022-05-02T16:33:09 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n","Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:23<00:00, 17.9MB/s]\n","[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n","Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T16:33:38 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T16:33:38 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T16:33:38 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-02T16:33:38 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2022-05-02T16:33:38 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2022-05-02T16:33:38 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2022-05-02T16:33:38 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2022-05-02T16:33:39 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2022-05-02T16:33:39 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2022-05-02T16:34:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.7083, train/hateful_memes/cross_entropy/avg: 0.7083, train/total_loss: 0.7083, train/total_loss/avg: 0.7083, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 721ms, time_since_start: 01m 17s 341ms, eta: 02h 57m 08s 658ms\n","\u001b[32m2022-05-02T16:35:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6679, train/hateful_memes/cross_entropy/avg: 0.6881, train/total_loss: 0.6679, train/total_loss/avg: 0.6881, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 249ms, time_since_start: 02m 04s 590ms, eta: 02h 54m 35s 516ms\n","\u001b[32m2022-05-02T16:36:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6679, train/hateful_memes/cross_entropy/avg: 0.6540, train/total_loss: 0.6679, train/total_loss/avg: 0.6540, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 900ms, time_since_start: 02m 51s 491ms, eta: 02h 52m 30s 407ms\n","\u001b[32m2022-05-02T16:36:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.5859, train/hateful_memes/cross_entropy/avg: 0.6310, train/total_loss: 0.5859, train/total_loss/avg: 0.6310, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 752ms, time_since_start: 03m 38s 243ms, eta: 02h 51m 10s 115ms\n","\u001b[32m2022-05-02T16:37:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.5859, train/hateful_memes/cross_entropy/avg: 0.6060, train/total_loss: 0.5859, train/total_loss/avg: 0.6060, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 2.17, time: 46s 994ms, time_since_start: 04m 25s 238ms, eta: 02h 51m 15s 666ms\n","\u001b[32m2022-05-02T16:38:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.5619, train/hateful_memes/cross_entropy/avg: 0.5768, train/total_loss: 0.5619, train/total_loss/avg: 0.5768, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 596ms, time_since_start: 05m 11s 835ms, eta: 02h 49m 01s 281ms\n","\u001b[32m2022-05-02T16:39:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.5619, train/hateful_memes/cross_entropy/avg: 0.5606, train/total_loss: 0.5619, train/total_loss/avg: 0.5606, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 008ms, time_since_start: 05m 58s 843ms, eta: 02h 49m 42s 963ms\n","\u001b[32m2022-05-02T16:39:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.5062, train/hateful_memes/cross_entropy/avg: 0.5298, train/total_loss: 0.5062, train/total_loss/avg: 0.5298, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 991ms, time_since_start: 06m 45s 834ms, eta: 02h 48m 51s 455ms\n","\u001b[32m2022-05-02T16:40:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.5062, train/hateful_memes/cross_entropy/avg: 0.5140, train/total_loss: 0.5062, train/total_loss/avg: 0.5140, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 187ms, time_since_start: 07m 33s 021ms, eta: 02h 48m 45s 755ms\n","\u001b[32m2022-05-02T16:41:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T16:41:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T16:41:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T16:41:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T16:41:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.4637, train/hateful_memes/cross_entropy/avg: 0.5011, train/total_loss: 0.4637, train/total_loss/avg: 0.5011, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.75, time: 57s 019ms, time_since_start: 08m 30s 041ms, eta: 03h 22m 57s 647ms\n","\u001b[32m2022-05-02T16:41:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T16:41:39 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T16:41:45 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T16:41:45 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T16:41:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T16:41:51 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-02T16:41:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T16:42:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T16:42:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.8669, val/total_loss: 0.8669, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4545, val/hateful_memes/roc_auc: 0.6953, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 23s 165ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.695294\n","\u001b[32m2022-05-02T16:42:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.4637, train/hateful_memes/cross_entropy/avg: 0.4595, train/total_loss: 0.4637, train/total_loss/avg: 0.4595, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 568ms, time_since_start: 09m 40s 776ms, eta: 02h 48m 30s 751ms\n","\u001b[32m2022-05-02T16:43:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.4305, train/hateful_memes/cross_entropy/avg: 0.4564, train/total_loss: 0.4305, train/total_loss/avg: 0.4564, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 102ms, time_since_start: 10m 27s 878ms, eta: 02h 46m 03s 802ms\n","\u001b[32m2022-05-02T16:44:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.4305, train/hateful_memes/cross_entropy/avg: 0.4446, train/total_loss: 0.4305, train/total_loss/avg: 0.4446, max mem: 9224.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 054ms, time_since_start: 11m 14s 932ms, eta: 02h 45m 05s 792ms\n","\u001b[32m2022-05-02T16:45:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.4222, train/hateful_memes/cross_entropy/avg: 0.4182, train/total_loss: 0.4222, train/total_loss/avg: 0.4182, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 2.17, time: 46s 805ms, time_since_start: 12m 01s 738ms, eta: 02h 43m 25s 872ms\n","\u001b[32m2022-05-02T16:45:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.4222, train/hateful_memes/cross_entropy/avg: 0.4045, train/total_loss: 0.4222, train/total_loss/avg: 0.4045, max mem: 9224.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 323ms, time_since_start: 12m 49s 061ms, eta: 02h 44m 26s 328ms\n","\u001b[32m2022-05-02T16:46:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.3877, train/hateful_memes/cross_entropy/avg: 0.3860, train/total_loss: 0.3877, train/total_loss/avg: 0.3860, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 003ms, time_since_start: 13m 36s 065ms, eta: 02h 42m 31s 821ms\n","\u001b[32m2022-05-02T16:47:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.3877, train/hateful_memes/cross_entropy/avg: 0.3692, train/total_loss: 0.3877, train/total_loss/avg: 0.3692, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 249ms, time_since_start: 14m 23s 315ms, eta: 02h 42m 34s 808ms\n","\u001b[32m2022-05-02T16:48:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.3855, train/hateful_memes/cross_entropy/avg: 0.3524, train/total_loss: 0.3855, train/total_loss/avg: 0.3524, max mem: 9224.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 220ms, time_since_start: 15m 10s 536ms, eta: 02h 41m 40s 791ms\n","\u001b[32m2022-05-02T16:49:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.3855, train/hateful_memes/cross_entropy/avg: 0.3359, train/total_loss: 0.3855, train/total_loss/avg: 0.3359, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 248ms, time_since_start: 15m 57s 785ms, eta: 02h 40m 58s 361ms\n","\u001b[32m2022-05-02T16:49:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T16:49:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T16:49:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T16:50:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T16:50:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.3136, train/hateful_memes/cross_entropy/avg: 0.3243, train/total_loss: 0.3136, train/total_loss/avg: 0.3243, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.82, time: 55s 860ms, time_since_start: 16m 53s 645ms, eta: 03h 09m 22s 064ms\n","\u001b[32m2022-05-02T16:50:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T16:50:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T16:50:07 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T16:50:07 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T16:50:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T16:50:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T16:50:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T16:50:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.0776, val/total_loss: 1.0776, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4514, val/hateful_memes/roc_auc: 0.6915, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 16s 295ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.695294\n","\u001b[32m2022-05-02T16:51:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.3026, train/hateful_memes/cross_entropy/avg: 0.3156, train/total_loss: 0.3026, train/total_loss/avg: 0.3156, max mem: 9224.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 066ms, time_since_start: 17m 58s 009ms, eta: 02h 42m 07s 819ms\n","\u001b[32m2022-05-02T16:51:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.2125, train/hateful_memes/cross_entropy/avg: 0.3024, train/total_loss: 0.2125, train/total_loss/avg: 0.3024, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 2.17, time: 46s 860ms, time_since_start: 18m 44s 870ms, eta: 02h 37m 16s 202ms\n","\u001b[32m2022-05-02T16:52:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.1415, train/hateful_memes/cross_entropy/avg: 0.2909, train/total_loss: 0.1415, train/total_loss/avg: 0.2909, max mem: 9224.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 203ms, time_since_start: 19m 32s 073ms, eta: 02h 37m 37s 203ms\n","\u001b[32m2022-05-02T16:53:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.1084, train/hateful_memes/cross_entropy/avg: 0.2805, train/total_loss: 0.1084, train/total_loss/avg: 0.2805, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 2.17, time: 46s 907ms, time_since_start: 20m 18s 981ms, eta: 02h 35m 50s 130ms\n","\u001b[32m2022-05-02T16:54:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.1044, train/hateful_memes/cross_entropy/avg: 0.2695, train/total_loss: 0.1044, train/total_loss/avg: 0.2695, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 166ms, time_since_start: 21m 06s 147ms, eta: 02h 35m 53s 847ms\n","\u001b[32m2022-05-02T16:55:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.1044, train/hateful_memes/cross_entropy/avg: 0.2636, train/total_loss: 0.1044, train/total_loss/avg: 0.2636, max mem: 9224.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 209ms, time_since_start: 21m 53s 357ms, eta: 02h 35m 14s 386ms\n","\u001b[32m2022-05-02T16:55:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.0999, train/hateful_memes/cross_entropy/avg: 0.2542, train/total_loss: 0.0999, train/total_loss/avg: 0.2542, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 2.17, time: 46s 819ms, time_since_start: 22m 40s 176ms, eta: 02h 33m 09s 710ms\n","\u001b[32m2022-05-02T16:56:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.0756, train/hateful_memes/cross_entropy/avg: 0.2455, train/total_loss: 0.0756, train/total_loss/avg: 0.2455, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 214ms, time_since_start: 23m 27s 391ms, eta: 02h 33m 39s 330ms\n","\u001b[32m2022-05-02T16:57:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.0756, train/hateful_memes/cross_entropy/avg: 0.2397, train/total_loss: 0.0756, train/total_loss/avg: 0.2397, max mem: 9224.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 201ms, time_since_start: 24m 14s 593ms, eta: 02h 32m 48s 781ms\n","\u001b[32m2022-05-02T16:58:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T16:58:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T16:58:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T16:58:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T16:58:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.0677, train/hateful_memes/cross_entropy/avg: 0.2327, train/total_loss: 0.0677, train/total_loss/avg: 0.2327, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.79, time: 56s 074ms, time_since_start: 25m 10s 667ms, eta: 03h 35s 362ms\n","\u001b[32m2022-05-02T16:58:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T16:58:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T16:58:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T16:58:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T16:58:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T16:58:27 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2022-05-02T16:58:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T16:58:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T16:58:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.5856, val/total_loss: 1.5856, val/hateful_memes/accuracy: 0.7093, val/hateful_memes/binary_f1: 0.5228, val/hateful_memes/roc_auc: 0.7203, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 19s 579ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.720290\n","\u001b[32m2022-05-02T16:59:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.0677, train/hateful_memes/cross_entropy/avg: 0.2265, train/total_loss: 0.0677, train/total_loss/avg: 0.2265, max mem: 9224.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 2.08, time: 48s 124ms, time_since_start: 26m 18s 374ms, eta: 02h 34m 10s 243ms\n","\u001b[32m2022-05-02T17:00:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.0426, train/hateful_memes/cross_entropy/avg: 0.2196, train/total_loss: 0.0426, train/total_loss/avg: 0.2196, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 303ms, time_since_start: 27m 05s 677ms, eta: 02h 30m 44s 285ms\n","\u001b[32m2022-05-02T17:01:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0426, train/hateful_memes/cross_entropy/avg: 0.2190, train/total_loss: 0.0426, train/total_loss/avg: 0.2190, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 433ms, time_since_start: 27m 53s 111ms, eta: 02h 30m 20s 931ms\n","\u001b[32m2022-05-02T17:01:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0410, train/hateful_memes/cross_entropy/avg: 0.2131, train/total_loss: 0.0410, train/total_loss/avg: 0.2131, max mem: 9224.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 545ms, time_since_start: 28m 40s 656ms, eta: 02h 29m 53s 711ms\n","\u001b[32m2022-05-02T17:02:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0410, train/hateful_memes/cross_entropy/avg: 0.2097, train/total_loss: 0.0410, train/total_loss/avg: 0.2097, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 297ms, time_since_start: 29m 27s 954ms, eta: 02h 28m 18s 784ms\n","\u001b[32m2022-05-02T17:03:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0410, train/hateful_memes/cross_entropy/avg: 0.2092, train/total_loss: 0.0410, train/total_loss/avg: 0.2092, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 145ms, time_since_start: 30m 15s 099ms, eta: 02h 27m 02s 169ms\n","\u001b[32m2022-05-02T17:04:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0388, train/hateful_memes/cross_entropy/avg: 0.2042, train/total_loss: 0.0388, train/total_loss/avg: 0.2042, max mem: 9224.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 193ms, time_since_start: 31m 02s 293ms, eta: 02h 26m 23s 291ms\n","\u001b[32m2022-05-02T17:04:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0368, train/hateful_memes/cross_entropy/avg: 0.1990, train/total_loss: 0.0368, train/total_loss/avg: 0.1990, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 2.17, time: 46s 874ms, time_since_start: 31m 49s 167ms, eta: 02h 24m 36s 152ms\n","\u001b[32m2022-05-02T17:05:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0293, train/hateful_memes/cross_entropy/avg: 0.1943, train/total_loss: 0.0293, train/total_loss/avg: 0.1943, max mem: 9224.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 2.13, time: 47s 291ms, time_since_start: 32m 36s 459ms, eta: 02h 25m 05s 372ms\n","\u001b[32m2022-05-02T17:06:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T17:06:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T17:06:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T17:06:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T17:06:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0293, train/hateful_memes/cross_entropy/avg: 0.1908, train/total_loss: 0.0293, train/total_loss/avg: 0.1908, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.75, time: 57s 220ms, time_since_start: 33m 33s 680ms, eta: 02h 54m 34s 846ms\n","\u001b[32m2022-05-02T17:06:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T17:06:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T17:06:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T17:06:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T17:06:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T17:06:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T17:06:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T17:06:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.5585, val/total_loss: 1.5585, val/hateful_memes/accuracy: 0.6981, val/hateful_memes/binary_f1: 0.4512, val/hateful_memes/roc_auc: 0.7029, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 12s 277ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.720290\n","\u001b[32m2022-05-02T17:07:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0260, train/hateful_memes/cross_entropy/avg: 0.1862, train/total_loss: 0.0260, train/total_loss/avg: 0.1862, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 122ms, time_since_start: 34m 34s 081ms, eta: 02h 26m 354ms\n","\u001b[32m2022-05-02T17:08:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0260, train/hateful_memes/cross_entropy/avg: 0.1822, train/total_loss: 0.0260, train/total_loss/avg: 0.1822, max mem: 9224.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 365ms, time_since_start: 35m 21s 447ms, eta: 02h 22m 54s 328ms\n","\u001b[32m2022-05-02T17:09:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0180, train/hateful_memes/cross_entropy/avg: 0.1780, train/total_loss: 0.0180, train/total_loss/avg: 0.1780, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 2.17, time: 46s 809ms, time_since_start: 36m 08s 256ms, eta: 02h 20m 26s 159ms\n","\u001b[32m2022-05-02T17:10:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0180, train/hateful_memes/cross_entropy/avg: 0.1744, train/total_loss: 0.0180, train/total_loss/avg: 0.1744, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 341ms, time_since_start: 36m 55s 598ms, eta: 02h 21m 13s 730ms\n","\u001b[32m2022-05-02T17:10:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0180, train/hateful_memes/cross_entropy/avg: 0.1706, train/total_loss: 0.0180, train/total_loss/avg: 0.1706, max mem: 9224.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 273ms, time_since_start: 37m 42s 871ms, eta: 02h 20m 13s 436ms\n","\u001b[32m2022-05-02T17:11:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0172, train/hateful_memes/cross_entropy/avg: 0.1671, train/total_loss: 0.0172, train/total_loss/avg: 0.1671, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 183ms, time_since_start: 38m 30s 054ms, eta: 02h 19m 09s 440ms\n","\u001b[32m2022-05-02T17:12:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0172, train/hateful_memes/cross_entropy/avg: 0.1636, train/total_loss: 0.0172, train/total_loss/avg: 0.1636, max mem: 9224.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 327ms, time_since_start: 39m 17s 381ms, eta: 02h 18m 46s 769ms\n","\u001b[32m2022-05-02T17:13:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0172, train/hateful_memes/cross_entropy/avg: 0.1603, train/total_loss: 0.0172, train/total_loss/avg: 0.1603, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 007ms, time_since_start: 40m 04s 388ms, eta: 02h 17m 02s 690ms\n","\u001b[32m2022-05-02T17:14:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0161, train/hateful_memes/cross_entropy/avg: 0.1572, train/total_loss: 0.0161, train/total_loss/avg: 0.1572, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 571ms, time_since_start: 40m 51s 960ms, eta: 02h 17m 53s 097ms\n","\u001b[32m2022-05-02T17:14:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T17:14:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T17:14:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T17:14:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T17:14:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0095, train/hateful_memes/cross_entropy/avg: 0.1541, train/total_loss: 0.0095, train/total_loss/avg: 0.1541, max mem: 9224.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.79, time: 56s 820ms, time_since_start: 41m 48s 781ms, eta: 02h 43m 43s 745ms\n","\u001b[32m2022-05-02T17:14:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T17:14:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T17:15:02 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T17:15:02 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T17:15:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T17:15:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T17:15:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T17:15:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.5284, val/total_loss: 1.5284, val/hateful_memes/accuracy: 0.6981, val/hateful_memes/binary_f1: 0.5192, val/hateful_memes/roc_auc: 0.7068, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 15s 382ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.720290\n","\u001b[32m2022-05-02T17:16:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0071, train/hateful_memes/cross_entropy/avg: 0.1512, train/total_loss: 0.0071, train/total_loss/avg: 0.1512, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 176ms, time_since_start: 42m 52s 341ms, eta: 02h 18m 187ms\n","\u001b[32m2022-05-02T17:16:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0071, train/hateful_memes/cross_entropy/avg: 0.1483, train/total_loss: 0.0071, train/total_loss/avg: 0.1483, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 790ms, time_since_start: 43m 40s 132ms, eta: 02h 16m 05s 380ms\n","\u001b[32m2022-05-02T17:17:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0071, train/hateful_memes/cross_entropy/avg: 0.1457, train/total_loss: 0.0071, train/total_loss/avg: 0.1457, max mem: 9224.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 513ms, time_since_start: 44m 27s 646ms, eta: 02h 14m 29s 677ms\n","\u001b[32m2022-05-02T17:18:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0071, train/hateful_memes/cross_entropy/avg: 0.1434, train/total_loss: 0.0071, train/total_loss/avg: 0.1434, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 208ms, time_since_start: 45m 14s 855ms, eta: 02h 12m 49s 820ms\n","\u001b[32m2022-05-02T17:19:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1408, train/total_loss: 0.0062, train/total_loss/avg: 0.1408, max mem: 9224.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 600ms, time_since_start: 46m 02s 455ms, eta: 02h 13m 07s 616ms\n","\u001b[32m2022-05-02T17:19:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1383, train/total_loss: 0.0043, train/total_loss/avg: 0.1383, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 474ms, time_since_start: 46m 49s 929ms, eta: 02h 11m 58s 098ms\n","\u001b[32m2022-05-02T17:20:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.1359, train/total_loss: 0.0041, train/total_loss/avg: 0.1359, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 587ms, time_since_start: 47m 37s 516ms, eta: 02h 11m 28s 591ms\n","\u001b[32m2022-05-02T17:21:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.1341, train/total_loss: 0.0041, train/total_loss/avg: 0.1341, max mem: 9224.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 591ms, time_since_start: 48m 25s 108ms, eta: 02h 10m 40s 970ms\n","\u001b[32m2022-05-02T17:22:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1319, train/total_loss: 0.0035, train/total_loss/avg: 0.1319, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 092ms, time_since_start: 49m 12s 201ms, eta: 02h 08m 30s 774ms\n","\u001b[32m2022-05-02T17:23:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T17:23:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T17:23:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T17:23:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T17:23:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1298, train/total_loss: 0.0035, train/total_loss/avg: 0.1298, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.79, time: 56s 916ms, time_since_start: 50m 09s 118ms, eta: 02h 34m 21s 468ms\n","\u001b[32m2022-05-02T17:23:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T17:23:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T17:23:22 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T17:23:22 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T17:23:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T17:23:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T17:23:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T17:23:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 1.6275, val/total_loss: 1.6275, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.4595, val/hateful_memes/roc_auc: 0.6954, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 14s 961ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.720290\n","\u001b[32m2022-05-02T17:24:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1277, train/total_loss: 0.0035, train/total_loss/avg: 0.1277, max mem: 9224.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 303ms, time_since_start: 51m 12s 384ms, eta: 02h 10m 10s 782ms\n","\u001b[32m2022-05-02T17:25:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1301, train/total_loss: 0.0035, train/total_loss/avg: 0.1301, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 488ms, time_since_start: 51m 59s 872ms, eta: 02h 07m 10s 805ms\n","\u001b[32m2022-05-02T17:25:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.1295, train/total_loss: 0.0041, train/total_loss/avg: 0.1295, max mem: 9224.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 559ms, time_since_start: 52m 47s 432ms, eta: 02h 06m 33s 840ms\n","\u001b[32m2022-05-02T17:26:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1275, train/total_loss: 0.0035, train/total_loss/avg: 0.1275, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 002ms, time_since_start: 53m 34s 434ms, eta: 02h 04m 16s 969ms\n","\u001b[32m2022-05-02T17:27:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1255, train/total_loss: 0.0035, train/total_loss/avg: 0.1255, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 396ms, time_since_start: 54m 21s 831ms, eta: 02h 04m 31s 407ms\n","\u001b[32m2022-05-02T17:28:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1237, train/total_loss: 0.0030, train/total_loss/avg: 0.1237, max mem: 9224.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 424ms, time_since_start: 55m 09s 256ms, eta: 02h 03m 47s 599ms\n","\u001b[32m2022-05-02T17:29:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1219, train/total_loss: 0.0035, train/total_loss/avg: 0.1219, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 323ms, time_since_start: 55m 56s 580ms, eta: 02h 02m 43s 570ms\n","\u001b[32m2022-05-02T17:29:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1201, train/total_loss: 0.0030, train/total_loss/avg: 0.1201, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 500ms, time_since_start: 56m 44s 080ms, eta: 02h 02m 22s 808ms\n","\u001b[32m2022-05-02T17:30:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1184, train/total_loss: 0.0025, train/total_loss/avg: 0.1184, max mem: 9224.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 347ms, time_since_start: 57m 31s 428ms, eta: 02h 01m 11s 057ms\n","\u001b[32m2022-05-02T17:31:27 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T17:31:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T17:31:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T17:31:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T17:31:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1168, train/total_loss: 0.0024, train/total_loss/avg: 0.1168, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.75, time: 57s 311ms, time_since_start: 58m 28s 739ms, eta: 02h 25m 42s 835ms\n","\u001b[32m2022-05-02T17:31:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T17:31:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T17:31:41 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T17:31:41 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T17:31:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T17:31:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T17:31:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T17:31:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.1396, val/total_loss: 2.1396, val/hateful_memes/accuracy: 0.7148, val/hateful_memes/binary_f1: 0.5000, val/hateful_memes/roc_auc: 0.7190, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 12s 197ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.720290\n","\u001b[32m2022-05-02T17:32:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1185, train/total_loss: 0.0024, train/total_loss/avg: 0.1185, max mem: 9224.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 005ms, time_since_start: 59m 28s 944ms, eta: 02h 01m 14s 447ms\n","\u001b[32m2022-05-02T17:33:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1168, train/total_loss: 0.0024, train/total_loss/avg: 0.1168, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 672ms, time_since_start: 01h 16s 616ms, eta: 01h 59m 35s 409ms\n","\u001b[32m2022-05-02T17:34:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1159, train/total_loss: 0.0024, train/total_loss/avg: 0.1159, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 475ms, time_since_start: 01h 01m 04s 092ms, eta: 01h 58m 17s 547ms\n","\u001b[32m2022-05-02T17:35:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1143, train/total_loss: 0.0014, train/total_loss/avg: 0.1143, max mem: 9224.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 478ms, time_since_start: 01h 01m 51s 570ms, eta: 01h 57m 29s 696ms\n","\u001b[32m2022-05-02T17:35:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1132, train/total_loss: 0.0024, train/total_loss/avg: 0.1132, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 117ms, time_since_start: 01h 02m 38s 688ms, eta: 01h 55m 48s 194ms\n","\u001b[32m2022-05-02T17:36:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1117, train/total_loss: 0.0024, train/total_loss/avg: 0.1117, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 377ms, time_since_start: 01h 03m 26s 065ms, eta: 01h 55m 38s 378ms\n","\u001b[32m2022-05-02T17:37:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1104, train/total_loss: 0.0024, train/total_loss/avg: 0.1104, max mem: 9224.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 486ms, time_since_start: 01h 04m 13s 552ms, eta: 01h 55m 05s 999ms\n","\u001b[32m2022-05-02T17:38:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1092, train/total_loss: 0.0024, train/total_loss/avg: 0.1092, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 334ms, time_since_start: 01h 05m 887ms, eta: 01h 53m 55s 839ms\n","\u001b[32m2022-05-02T17:38:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1078, train/total_loss: 0.0024, train/total_loss/avg: 0.1078, max mem: 9224.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 401ms, time_since_start: 01h 05m 48s 288ms, eta: 01h 53m 17s 185ms\n","\u001b[32m2022-05-02T17:39:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2022-05-02T17:39:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2022-05-02T17:39:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2022-05-02T17:39:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2022-05-02T17:39:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1065, train/total_loss: 0.0024, train/total_loss/avg: 0.1065, max mem: 9224.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.75, time: 57s 392ms, time_since_start: 01h 06m 45s 680ms, eta: 02h 16m 11s 497ms\n","\u001b[32m2022-05-02T17:39:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2022-05-02T17:39:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2022-05-02T17:39:57 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 17\n","\u001b[32m2022-05-02T17:39:57 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T17:39:58 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-05-02T17:39:58 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-05-02T17:40:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-02T17:40:04 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n","\u001b[32m2022-05-02T17:40:04 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n","\u001b[32m2022-05-02T17:40:04 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 12\n","\u001b[32m2022-05-02T17:40:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 2.1357, val/total_loss: 2.1357, val/hateful_memes/accuracy: 0.7167, val/hateful_memes/binary_f1: 0.5513, val/hateful_memes/roc_auc: 0.7063, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 10s 096ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.720290\n","\u001b[32m2022-05-02T17:40:05 | mmf.trainers.core.training_loop: \u001b[0mEarly stopping activated\n","\u001b[32m2022-05-02T17:40:05 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2022-05-02T17:40:05 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2022-05-02T17:40:05 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2022-05-02T17:40:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2022-05-02T17:40:07 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n","\u001b[32m2022-05-02T17:40:07 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n","\u001b[32m2022-05-02T17:40:07 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 12\n","\u001b[32m2022-05-02T17:40:08 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2022-05-02T17:40:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 63/63 [00:14<00:00,  4.23it/s]\n","\u001b[32m2022-05-02T17:40:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 63\n","\u001b[32m2022-05-02T17:40:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2022-05-02T17:40:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, test/hateful_memes/cross_entropy: 1.5365, test/total_loss: 1.5365, test/hateful_memes/accuracy: 0.7005, test/hateful_memes/binary_f1: 0.5037, test/hateful_memes/roc_auc: 0.7643\n","\u001b[32m2022-05-02T17:40:23 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 07m 14s 068ms\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"RNladXdfkc2O","executionInfo":{"status":"ok","timestamp":1651508370243,"user_tz":300,"elapsed":3,"user":{"displayName":"Tianyao Wang","userId":"02589710995949182080"}}},"execution_count":12,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"hateful_memes_demo.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}