{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hateful_memes_demo_Yuguo_50%dataaug_VILCC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount into drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhxPauUIJPvw",
        "outputId": "c2b377ce-bc43-4e23-eaeb-c536c3b09b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si60Z47V0oJh",
        "outputId": "2a4b2e62-a176-483e-a287-1dcf54af6034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNL3zd6n1ppg",
        "outputId": "7eff095c-c5ab-492b-c124-bd2d73674451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 'vilbert_hugo/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ednxfGL_EslI",
        "outputId": "ca7b9671-2941-4b26-ca7c-16f6823ca6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/vilbert_hugo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Verify the contents of the current folder\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVZFqqo8JVA7",
        "outputId": "171adb1a-d40e-490b-b8fc-0e2bc2c68cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hateful-memes  no_text_50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxxqk6uGWgUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0931cfce-8e0f-497e-93d3-bca49860d2aa"
      },
      "source": [
        "# %%writefile setup.sh\n",
        "\n",
        "!pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# urllib3 is only required in colab\n",
        "!pip install urllib3==1.25.10\n",
        "\n",
        "# # import hateful-memes\n",
        "!git clone https://github.com/czh4/hateful-memes.git\n",
        "%cd hateful-memes/\n",
        "\n",
        "# install mmf\n",
        "!git clone https://github.com/czh4/mmf-hateful-memes.git\n",
        "%cd mmf-hateful-memes/\n",
        "!pip install --editable .\n",
        "%cd ..\n",
        "\n",
        "# unzip dataset and move to hateful-memes\n",
        "!unzip datasets.zip\n",
        "!mv datasets hateful-memes/datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.0+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (703.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 703.8 MB 20 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.0+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 73.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0+cu101) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.0+cu101) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.5.0+cu101 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.5.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.5.0+cu101 torchvision-0.6.0+cu101\n",
            "Collecting urllib3==1.25.10\n",
            "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed urllib3-1.25.10\n",
            "fatal: destination path 'hateful-memes' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/vilbert_hugo/hateful-memes\n",
            "fatal: destination path 'mmf-hateful-memes' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/vilbert_hugo/hateful-memes/mmf-hateful-memes\n",
            "Obtaining file:///content/drive/MyDrive/vilbert_hugo/hateful-memes/mmf-hateful-memes\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n",
            "Collecting torchvision==0.7.0\n",
            "  Downloading torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 4.1 MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 73.2 MB/s \n",
            "\u001b[?25hCollecting omegaconf==2.0.1rc4\n",
            "  Downloading omegaconf-2.0.1rc4-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.1.0)\n",
            "Collecting lmdb==0.98\n",
            "  Downloading lmdb-0.98.tar.gz (869 kB)\n",
            "\u001b[K     |████████████████████████████████| 869 kB 69.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.0)\n",
            "Collecting GitPython==3.1.0\n",
            "  Downloading GitPython-3.1.0-py3-none-any.whl (450 kB)\n",
            "\u001b[K     |████████████████████████████████| 450 kB 63.5 MB/s \n",
            "\u001b[?25hCollecting fasttext==0.9.1\n",
            "  Downloading fasttext-0.9.1.tar.gz (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n",
            "Collecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting demjson==2.2.4\n",
            "  Downloading demjson-2.2.4.tar.gz (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 102.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.21.6)\n",
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 19 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (4.64.0)\n",
            "Collecting transformers==2.3.0\n",
            "  Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\n",
            "\u001b[K     |████████████████████████████████| 447 kB 95.7 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (57.4.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->mmf==1.0.0rc12) (1.15.0)\n",
            "Collecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 87.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.0.1rc4->mmf==1.0.0rc12) (4.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.25.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (1.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->mmf==1.0.0rc12) (0.16.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 80.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0->mmf==1.0.0rc12) (7.1.2)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.22.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 74.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 79.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0->mmf==1.0.0rc12) (2019.12.20)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.26.0,>=1.25.1\n",
            "  Downloading botocore-1.25.2-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 85.7 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.1->boto3->transformers==2.3.0->mmf==1.0.0rc12) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0->mmf==1.0.0rc12) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0->mmf==1.0.0rc12) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.4.1)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'omegaconf' candidate (version 2.0.1rc4 at https://files.pythonhosted.org/packages/03/c6/dec84d1b2a3d645f03201dca03bc879b6116cb6503449a31d7ff9c1394a4/omegaconf-2.0.1rc4-py3-none-any.whl#sha256=e04462f7e3d8f51532221471b241f67e35a36a04e364c70987018faadd273cc0 (from https://pypi.org/simple/omegaconf/) (requires-python:>=3.6))\n",
            "Reason for being yanked: <none given>\u001b[0m\n",
            "Building wheels for collected packages: demjson, fasttext, lmdb, nltk\n",
            "  Building wheel for demjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for demjson: filename=demjson-2.2.4-py3-none-any.whl size=73565 sha256=ad80ad26868a6f966a90f0261d120328babc225b625b48d5caa360da06071114\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/94/3d/466801f4a8db8e6fce765d7a0115dfebcc55ddf6b00cd98f59\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2507612 sha256=6710a857ced4222ef70f3c1061c57890d20849ddf141edd422f889345b98739e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/5b/4b/9c582c778bb93aaad8fc855d5e79f49eae34f59e363a22c422\n",
            "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=219741 sha256=77a8b35abafa98746f4484fa64bf0a5683f4a7cc5666cfdfcb3fc30aab1492da\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/24/96/783d4dddcf63e3f8cc92db8b3af3c70cf6d76398bff77f1d5e\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449921 sha256=e5a3c7c38f3e334827b7312396aaf0d374412bab625439819ecd7e44068b1fa2\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "Successfully built demjson fasttext lmdb nltk\n",
            "Installing collected packages: jmespath, botocore, smmap, s3transfer, torch, sentencepiece, sacremoses, PyYAML, pybind11, gitdb, boto3, transformers, torchvision, torchtext, omegaconf, nltk, lmdb, GitPython, fasttext, demjson, mmf\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.6.0+cu101\n",
            "    Uninstalling torchvision-0.6.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.0+cu101\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Attempting uninstall: lmdb\n",
            "    Found existing installation: lmdb 0.99\n",
            "    Uninstalling lmdb-0.99:\n",
            "      Successfully uninstalled lmdb-0.99\n",
            "  Running setup.py develop for mmf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.0 PyYAML-6.0 boto3-1.22.1 botocore-1.25.2 demjson-2.2.4 fasttext-0.9.1 gitdb-4.0.9 jmespath-1.0.0 lmdb-0.98 mmf nltk-3.4.5 omegaconf-2.0.1rc4 pybind11-2.9.2 s3transfer-0.5.2 sacremoses-0.0.49 sentencepiece-0.1.96 smmap-5.0.0 torch-1.6.0 torchtext-0.5.0 torchvision-0.7.0 transformers-2.3.0\n",
            "/content/drive/MyDrive/vilbert_hugo/hateful-memes\n",
            "unzip:  cannot find or open datasets.zip, datasets.zip.zip or datasets.zip.ZIP.\n",
            "mv: cannot stat 'datasets': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp6Vu7bnXJng"
      },
      "source": [
        "# !sh setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # !pwd\n",
        "# # !unzip datasets.zip\n",
        "# !mv datasets hateful-memes/datasets"
      ],
      "metadata": {
        "id": "KIXZE5hDNCxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/facebookresearch/mmf.git\n",
        "# !pip install git+https://github.com/rizavelioglu/mmf.git\n",
        "# !pip uninstall mmf\n",
        "!pip install mmf@https://github.com/facebookresearch/mmf/tarball/master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REVflQ2Y2IK8",
        "outputId": "8fbe4b28-0db9-4038-a58b-9c3adbcb5817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mmf@ https://github.com/facebookresearch/mmf/tarball/master\n",
            "  Using cached https://github.com/facebookresearch/mmf/tarball/master\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning (to revision 9b011606f) to /tmp/pip-install-2syic4wt/pytorch-lightning_483b17d459bd4853a2229439c5816728\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-install-2syic4wt/pytorch-lightning_483b17d459bd4853a2229439c5816728\n",
            "\u001b[33m  WARNING: Did not find branch or tag '9b011606f', assuming revision or ref.\u001b[0m\n",
            "  Running command git checkout -q 9b011606f\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  From https://github.com/PyTorchLightning/lightning-tutorials\n",
            "   * branch            290fb466de1fcc2ac6025f74b56906592911e856 -> FETCH_HEAD\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib==3.3.4 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.3.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (5.4.8)\n",
            "Requirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.0.2)\n",
            "Requirement already satisfied: omegaconf<=2.1,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.1.0)\n",
            "Requirement already satisfied: torchvision<=0.10.0,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.10.0)\n",
            "Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.5.3)\n",
            "Requirement already satisfied: torchtext==0.5.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.5.0)\n",
            "Requirement already satisfied: datasets==1.2.1 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.2.1)\n",
            "Requirement already satisfied: ftfy==5.8 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (5.8)\n",
            "Requirement already satisfied: transformers<=4.10.1,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.10.1)\n",
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.4.5)\n",
            "Requirement already satisfied: torch<=1.9.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.9.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.43.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.49.0)\n",
            "Requirement already satisfied: pillow==9.0.1 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (9.0.1)\n",
            "Requirement already satisfied: lmdb==0.98 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.98)\n",
            "Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.23.0)\n",
            "Requirement already satisfied: numpy<=1.21.4,>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.21.4)\n",
            "Requirement already satisfied: torchaudio<=0.9.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.9.0)\n",
            "Requirement already satisfied: GitPython==3.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.1.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.0)\n",
            "Requirement already satisfied: iopath==0.1.8 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.1.8)\n",
            "Requirement already satisfied: fasttext==0.9.1 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.9.1)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.1.0)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.8.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (6.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (21.3)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2022.3.0)\n",
            "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.3.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.2.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (6.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.3.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.11.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.3.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.70.12.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.9.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (57.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==5.8->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.2.5)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython==3.1.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.0.9)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath==0.1.8->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.15.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.29.28)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.25.10)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.0.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.0.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.1.96)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.8.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython==3.1.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (5.0.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from omegaconf<=2.1,>=2.0.6->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.8)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.44.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.0.49)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.5.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.7.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.3.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (2022.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.10.1,>=3.4.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf@ https://github.com/facebookresearch/mmf/tarball/master) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAnu0kfe38a9",
        "outputId": "68d58018-d8b7-4a42-b39c-6ba7387e873f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hateful-memes  hateful_memes.zip  mmf-hateful-memes  no_text_50.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOj_9q7cBbNN",
        "outputId": "d6b7de32-f73d-4f62-925c-449b6a8f5a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/vilbert_hugo/hateful-memes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_convert_hm --zip_file=\"hateful_memes.zip\" --password=\"1\" --bypass_checksum 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WD-iMI8NP8q",
        "outputId": "42f41289-7e6c-48f6-fd77-2b84135eddee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "Data folder is /root/.cache/torch/mmf/data\n",
            "Zip path is hateful_memes.zip\n",
            "Copying hateful_memes.zip\n",
            "Unzipping hateful_memes.zip\n",
            "Extracting the zip can take time. Sit back and relax.\n",
            "Moving train.jsonl\n",
            "Moving dev_seen.jsonl\n",
            "Moving test_seen.jsonl\n",
            "Moving dev_unseen.jsonl\n",
            "Moving test_unseen.jsonl\n",
            "Moving img\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd hateful-memes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2UQtrgEN_Ni",
        "outputId": "8e9f9fbd-d15b-4367-81b6-28b0599ba07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/vilbert_hugo/hateful-memes/hateful-memes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W9hgpFvRXxJB",
        "outputId": "cb8ec820-f26b-4577-8dc2-0a40e6a89391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/vilbert_hugo/hateful-memes/hateful-memes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hjiR1Ot6RGW",
        "outputId": "6d902e36-14e5-443c-954f-2c80f7c19417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "configs  ensemble  mmf-hateful-memes  README.md  save  tools\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNPI6gj9kwqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daecc280-1ab2-4d24-ae94-e084b63018c4"
      },
      "source": [
        "!python tools/run.py config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml model=vilbert dataset=hateful_memes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-04-28T13:16:18 | matplotlib.font_manager: \u001b[0mGenerating new fontManager, this may take some time...\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "\u001b[32m2022-04-28T13:16:19 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\n",
            "\u001b[32m2022-04-28T13:16:19 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-04-28T13:16:19 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "\u001b[32m2022-04-28T13:16:20 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-04-28T13:16:21 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes'])\n",
            "\u001b[32m2022-04-28T13:16:21 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-04-28T13:16:21 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-04-28T13:16:21 | mmf_cli.run: \u001b[0mUsing seed 19838567\n",
            "\u001b[32m2022-04-28T13:16:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n",
            "Downloading features.tar.gz: 100% 10.3G/10.3G [09:27<00:00, 18.1MB/s]\n",
            "[ Starting checksum for features.tar.gz]\n",
            "[ Checksum successful for features.tar.gz]\n",
            "Unpacking features.tar.gz\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n",
            "Downloading extras.tar.gz: 100% 211k/211k [00:00<00:00, 219kB/s] \n",
            "[ Starting checksum for extras.tar.gz]\n",
            "[ Checksum successful for extras.tar.gz]\n",
            "Unpacking extras.tar.gz\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfhthyn98\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 26.6kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9p_6k4t6\n",
            "Downloading: 100% 570/570 [00:00<00:00, 517kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdubjraph\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 321kB/s] \n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8k2n7z75\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 644kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[32m2022-04-28T13:29:02 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T13:29:02 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T13:29:02 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T13:29:02 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bi_attention_type\": 1,\n",
            "  \"bi_hidden_size\": 1024,\n",
            "  \"bi_intermediate_size\": 1024,\n",
            "  \"bi_num_attention_heads\": 8,\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cut_first\": \"text\",\n",
            "  \"dynamic_attention\": false,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"fast_mode\": false,\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"fixed_t_layer\": 0,\n",
            "  \"fixed_v_layer\": 0,\n",
            "  \"freeze_base\": false,\n",
            "  \"fusion_method\": \"mul\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hard_cap_seq_len\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"in_batch_pairs\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"vilbert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_negative\": 128,\n",
            "  \"objective\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooling_method\": \"mul\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"t_biattention_id\": [\n",
            "    6,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    11\n",
            "  ],\n",
            "  \"task_specific_tokens\": false,\n",
            "  \"text_only\": false,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"v_attention_probs_dropout_prob\": 0.1,\n",
            "  \"v_biattention_id\": [\n",
            "    0,\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    4,\n",
            "    5\n",
            "  ],\n",
            "  \"v_feature_size\": 2048,\n",
            "  \"v_hidden_act\": \"gelu\",\n",
            "  \"v_hidden_dropout_prob\": 0.1,\n",
            "  \"v_hidden_size\": 1024,\n",
            "  \"v_initializer_range\": 0.02,\n",
            "  \"v_intermediate_size\": 1024,\n",
            "  \"v_num_attention_heads\": 8,\n",
            "  \"v_num_hidden_layers\": 6,\n",
            "  \"v_target_size\": 1601,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"visual_target\": 0,\n",
            "  \"visualization\": false,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"with_coattention\": true\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/mmf/distributed_-1/tmpmc_xkv3n\n",
            "Downloading: 100% 440M/440M [00:05<00:00, 77.3MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "creating metadata file for /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.v_pooler.dense.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.v_pooler.dense.weight', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.t_pooler.dense.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-04-28T13:29:18 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-04-28T13:29:18 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-04-28T13:29:19 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-04-28T13:29:19 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
            "  (model): ViLBERTForClassification(\n",
            "    (bert): ViLBERTBase(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (v_embeddings): BertImageFeatureEmbeddings(\n",
            "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (v_layer): ModuleList(\n",
            "          (0): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (c_layer): ModuleList(\n",
            "          (0): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (t_pooler): BertTextPooler(\n",
            "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (v_pooler): BertImagePooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-04-28T13:29:19 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
            "\u001b[32m2022-04-28T13:29:19 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-04-28T13:30:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.6986, train/hateful_memes/cross_entropy/avg: 0.6986, train/total_loss: 0.6986, train/total_loss/avg: 0.6986, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.03, time: 01m 37s 524ms, time_since_start: 01m 37s 615ms, eta: 06h 02m 921ms\n",
            "\u001b[32m2022-04-28T13:32:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6986, train/hateful_memes/cross_entropy/avg: 0.7094, train/total_loss: 0.6986, train/total_loss/avg: 0.7094, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 788ms, time_since_start: 03m 13s 403ms, eta: 05h 53m 56s 868ms\n",
            "\u001b[32m2022-04-28T13:34:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6986, train/hateful_memes/cross_entropy/avg: 0.7008, train/total_loss: 0.6986, train/total_loss/avg: 0.7008, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 405ms, time_since_start: 04m 48s 809ms, eta: 05h 50m 54s 999ms\n",
            "\u001b[32m2022-04-28T13:35:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6836, train/hateful_memes/cross_entropy/avg: 0.6732, train/total_loss: 0.6836, train/total_loss/avg: 0.6732, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 467ms, time_since_start: 06m 24s 276ms, eta: 05h 49m 31s 452ms\n",
            "\u001b[32m2022-04-28T13:37:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.6836, train/hateful_memes/cross_entropy/avg: 0.6450, train/total_loss: 0.6836, train/total_loss/avg: 0.6450, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 369ms, time_since_start: 07m 59s 645ms, eta: 05h 47m 32s 915ms\n",
            "\u001b[32m2022-04-28T13:38:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.5936, train/hateful_memes/cross_entropy/avg: 0.6365, train/total_loss: 0.5936, train/total_loss/avg: 0.6365, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 967ms, time_since_start: 09m 34s 612ms, eta: 05h 44m 28s 436ms\n",
            "\u001b[32m2022-04-28T13:40:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6134, train/hateful_memes/cross_entropy/avg: 0.6332, train/total_loss: 0.6134, train/total_loss/avg: 0.6332, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 382ms, time_since_start: 11m 09s 995ms, eta: 05h 44m 21s 866ms\n",
            "\u001b[32m2022-04-28T13:42:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.5936, train/hateful_memes/cross_entropy/avg: 0.6280, train/total_loss: 0.5936, train/total_loss/avg: 0.6280, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 276ms, time_since_start: 12m 45s 272ms, eta: 05h 42m 22s 066ms\n",
            "\u001b[32m2022-04-28T13:43:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.6134, train/hateful_memes/cross_entropy/avg: 0.6469, train/total_loss: 0.6134, train/total_loss/avg: 0.6469, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 393ms, time_since_start: 14m 20s 666ms, eta: 05h 41m 10s 293ms\n",
            "\u001b[32m2022-04-28T13:45:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T13:45:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T13:45:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T13:46:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T13:46:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.6134, train/hateful_memes/cross_entropy/avg: 0.6467, train/total_loss: 0.6134, train/total_loss/avg: 0.6467, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00001, ups: 0.71, time: 02m 21s 285ms, time_since_start: 16m 41s 951ms, eta: 08h 22m 54s 332ms\n",
            "\u001b[32m2022-04-28T13:46:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T13:46:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T13:46:08 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T13:46:08 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T13:46:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T13:46:21 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-04-28T13:46:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T13:47:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T13:47:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.6996, val/total_loss: 0.6996, val/hateful_memes/accuracy: 0.5741, val/hateful_memes/binary_f1: 0.4575, val/hateful_memes/roc_auc: 0.5924, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 01m 08s 162ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.592441\n",
            "\u001b[32m2022-04-28T13:48:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.6134, train/hateful_memes/cross_entropy/avg: 0.6255, train/total_loss: 0.6134, train/total_loss/avg: 0.6255, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 1.03, time: 01m 37s 228ms, time_since_start: 19m 27s 343ms, eta: 05h 44m 26s 117ms\n",
            "\u001b[32m2022-04-28T13:50:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.5936, train/hateful_memes/cross_entropy/avg: 0.6133, train/total_loss: 0.5936, train/total_loss/avg: 0.6133, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 456ms, time_since_start: 21m 03s 799ms, eta: 05h 40m 03s 923ms\n",
            "\u001b[32m2022-04-28T13:51:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.5936, train/hateful_memes/cross_entropy/avg: 0.6047, train/total_loss: 0.5936, train/total_loss/avg: 0.6047, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 227ms, time_since_start: 22m 40s 026ms, eta: 05h 37m 37s 626ms\n",
            "\u001b[32m2022-04-28T13:53:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.5920, train/hateful_memes/cross_entropy/avg: 0.5816, train/total_loss: 0.5920, train/total_loss/avg: 0.5816, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 587ms, time_since_start: 24m 15s 614ms, eta: 05h 33m 45s 736ms\n",
            "\u001b[32m2022-04-28T13:55:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.5920, train/hateful_memes/cross_entropy/avg: 0.5737, train/total_loss: 0.5920, train/total_loss/avg: 0.5737, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 703ms, time_since_start: 25m 51s 318ms, eta: 05h 32m 32s 814ms\n",
            "\u001b[32m2022-04-28T13:56:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.5904, train/hateful_memes/cross_entropy/avg: 0.5696, train/total_loss: 0.5904, train/total_loss/avg: 0.5696, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 108ms, time_since_start: 27m 26s 426ms, eta: 05h 28m 52s 058ms\n",
            "\u001b[32m2022-04-28T13:58:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.5904, train/hateful_memes/cross_entropy/avg: 0.5654, train/total_loss: 0.5904, train/total_loss/avg: 0.5654, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 417ms, time_since_start: 29m 01s 844ms, eta: 05h 28m 18s 968ms\n",
            "\u001b[32m2022-04-28T13:59:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.5324, train/hateful_memes/cross_entropy/avg: 0.5426, train/total_loss: 0.5324, train/total_loss/avg: 0.5426, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 481ms, time_since_start: 30m 37s 325ms, eta: 05h 26m 55s 117ms\n",
            "\u001b[32m2022-04-28T14:01:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.5324, train/hateful_memes/cross_entropy/avg: 0.5393, train/total_loss: 0.5324, train/total_loss/avg: 0.5393, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 161ms, time_since_start: 32m 12s 486ms, eta: 05h 24m 12s 542ms\n",
            "\u001b[32m2022-04-28T14:03:07 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T14:03:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T14:03:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T14:03:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T14:03:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.5084, train/hateful_memes/cross_entropy/avg: 0.5312, train/total_loss: 0.5084, train/total_loss/avg: 0.5312, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 0.84, time: 01m 59s 241ms, time_since_start: 34m 11s 728ms, eta: 06h 44m 13s 739ms\n",
            "\u001b[32m2022-04-28T14:03:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T14:03:31 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T14:03:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T14:03:39 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T14:03:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T14:03:52 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-04-28T14:04:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T14:04:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T14:04:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 0.9419, val/total_loss: 0.9419, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.3460, val/hateful_memes/roc_auc: 0.6585, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 47s 893ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.658529\n",
            "\u001b[32m2022-04-28T14:05:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.5013, train/hateful_memes/cross_entropy/avg: 0.5189, train/total_loss: 0.5013, train/total_loss/avg: 0.5189, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 1.03, time: 01m 37s 900ms, time_since_start: 36m 37s 524ms, eta: 05h 30m 13s 410ms\n",
            "\u001b[32m2022-04-28T14:07:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.4980, train/hateful_memes/cross_entropy/avg: 0.5115, train/total_loss: 0.4980, train/total_loss/avg: 0.5115, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 330ms, time_since_start: 38m 13s 854ms, eta: 05h 23m 17s 625ms\n",
            "\u001b[32m2022-04-28T14:09:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.4794, train/hateful_memes/cross_entropy/avg: 0.4955, train/total_loss: 0.4794, train/total_loss/avg: 0.4955, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 815ms, time_since_start: 39m 49s 670ms, eta: 05h 19m 56s 552ms\n",
            "\u001b[32m2022-04-28T14:10:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.4791, train/hateful_memes/cross_entropy/avg: 0.4781, train/total_loss: 0.4791, train/total_loss/avg: 0.4781, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 746ms, time_since_start: 41m 25s 417ms, eta: 05h 18m 05s 390ms\n",
            "\u001b[32m2022-04-28T14:12:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.4632, train/hateful_memes/cross_entropy/avg: 0.4658, train/total_loss: 0.4632, train/total_loss/avg: 0.4658, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 605ms, time_since_start: 43m 01s 022ms, eta: 05h 16m 005ms\n",
            "\u001b[32m2022-04-28T14:13:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.4132, train/hateful_memes/cross_entropy/avg: 0.4539, train/total_loss: 0.4132, train/total_loss/avg: 0.4539, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 674ms, time_since_start: 44m 36s 696ms, eta: 05h 14m 36s 315ms\n",
            "\u001b[32m2022-04-28T14:15:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.3765, train/hateful_memes/cross_entropy/avg: 0.4421, train/total_loss: 0.3765, train/total_loss/avg: 0.4421, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 061ms, time_since_start: 46m 11s 758ms, eta: 05h 10m 58s 745ms\n",
            "\u001b[32m2022-04-28T14:17:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.3553, train/hateful_memes/cross_entropy/avg: 0.4321, train/total_loss: 0.3553, train/total_loss/avg: 0.4321, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 650ms, time_since_start: 47m 47s 408ms, eta: 05h 11m 17s 017ms\n",
            "\u001b[32m2022-04-28T14:18:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.2813, train/hateful_memes/cross_entropy/avg: 0.4231, train/total_loss: 0.2813, train/total_loss/avg: 0.4231, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 656ms, time_since_start: 49m 23s 064ms, eta: 05h 09m 40s 958ms\n",
            "\u001b[32m2022-04-28T14:20:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T14:20:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T14:20:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T14:20:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T14:20:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.2744, train/hateful_memes/cross_entropy/avg: 0.4118, train/total_loss: 0.2744, train/total_loss/avg: 0.4118, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 0.84, time: 01m 59s 370ms, time_since_start: 51m 22s 435ms, eta: 06h 24m 25s 994ms\n",
            "\u001b[32m2022-04-28T14:20:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T14:20:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T14:20:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T14:20:48 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T14:20:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T14:21:01 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-04-28T14:21:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T14:21:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T14:21:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.2167, val/total_loss: 1.2167, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.4955, val/hateful_memes/roc_auc: 0.7034, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 46s 049ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.703441\n",
            "\u001b[32m2022-04-28T14:23:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.1715, train/hateful_memes/cross_entropy/avg: 0.4012, train/total_loss: 0.1715, train/total_loss/avg: 0.4012, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 1.01, time: 01m 39s 003ms, time_since_start: 53m 47s 489ms, eta: 05h 17m 09s 694ms\n",
            "\u001b[32m2022-04-28T14:24:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.1698, train/hateful_memes/cross_entropy/avg: 0.3898, train/total_loss: 0.1698, train/total_loss/avg: 0.3898, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 050ms, time_since_start: 55m 23s 539ms, eta: 05h 06m 04s 451ms\n",
            "\u001b[32m2022-04-28T14:26:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.1634, train/hateful_memes/cross_entropy/avg: 0.3785, train/total_loss: 0.1634, train/total_loss/avg: 0.3785, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 717ms, time_since_start: 57m 256ms, eta: 05h 06m 33s 584ms\n",
            "\u001b[32m2022-04-28T14:27:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.1567, train/hateful_memes/cross_entropy/avg: 0.3698, train/total_loss: 0.1567, train/total_loss/avg: 0.3698, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 102ms, time_since_start: 58m 36s 359ms, eta: 05h 02m 58s 928ms\n",
            "\u001b[32m2022-04-28T14:29:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.1561, train/hateful_memes/cross_entropy/avg: 0.3614, train/total_loss: 0.1561, train/total_loss/avg: 0.3614, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 844ms, time_since_start: 01h 12s 203ms, eta: 05h 32s 595ms\n",
            "\u001b[32m2022-04-28T14:31:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.1441, train/hateful_memes/cross_entropy/avg: 0.3519, train/total_loss: 0.1441, train/total_loss/avg: 0.3519, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 847ms, time_since_start: 01h 01m 48s 050ms, eta: 04h 58m 55s 721ms\n",
            "\u001b[32m2022-04-28T14:32:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.1342, train/hateful_memes/cross_entropy/avg: 0.3427, train/total_loss: 0.1342, train/total_loss/avg: 0.3427, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 032ms, time_since_start: 01h 03m 24s 083ms, eta: 04h 57m 52s 788ms\n",
            "\u001b[32m2022-04-28T14:34:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0850, train/hateful_memes/cross_entropy/avg: 0.3340, train/total_loss: 0.0850, train/total_loss/avg: 0.3340, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 578ms, time_since_start: 01h 04m 59s 662ms, eta: 04h 54m 50s 974ms\n",
            "\u001b[32m2022-04-28T14:35:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0842, train/hateful_memes/cross_entropy/avg: 0.3256, train/total_loss: 0.0842, train/total_loss/avg: 0.3256, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 992ms, time_since_start: 01h 06m 35s 654ms, eta: 04h 54m 29s 957ms\n",
            "\u001b[32m2022-04-28T14:37:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T14:37:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T14:37:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T14:37:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T14:37:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0835, train/hateful_memes/cross_entropy/avg: 0.3191, train/total_loss: 0.0835, train/total_loss/avg: 0.3191, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 864ms, time_since_start: 01h 08m 36s 518ms, eta: 06h 08m 45s 468ms\n",
            "\u001b[32m2022-04-28T14:37:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T14:37:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T14:38:03 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T14:38:03 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T14:38:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T14:38:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T14:38:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T14:38:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.5473, val/total_loss: 1.5473, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.5161, val/hateful_memes/roc_auc: 0.7004, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 31s 593ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.703441\n",
            "\u001b[32m2022-04-28T14:40:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0764, train/hateful_memes/cross_entropy/avg: 0.3118, train/total_loss: 0.0764, train/total_loss/avg: 0.3118, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 1.03, time: 01m 37s 857ms, time_since_start: 01h 10m 45s 971ms, eta: 04h 56m 54s 230ms\n",
            "\u001b[32m2022-04-28T14:41:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0756, train/hateful_memes/cross_entropy/avg: 0.3044, train/total_loss: 0.0756, train/total_loss/avg: 0.3044, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 707ms, time_since_start: 01h 12m 22s 679ms, eta: 04h 51m 46s 576ms\n",
            "\u001b[32m2022-04-28T14:43:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0649, train/hateful_memes/cross_entropy/avg: 0.2974, train/total_loss: 0.0649, train/total_loss/avg: 0.2974, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 832ms, time_since_start: 01h 13m 58s 511ms, eta: 04h 47m 30s 754ms\n",
            "\u001b[32m2022-04-28T14:44:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0363, train/hateful_memes/cross_entropy/avg: 0.2907, train/total_loss: 0.0363, train/total_loss/avg: 0.2907, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 201ms, time_since_start: 01h 15m 34s 713ms, eta: 04h 46m 59s 314ms\n",
            "\u001b[32m2022-04-28T14:46:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0243, train/hateful_memes/cross_entropy/avg: 0.2848, train/total_loss: 0.0243, train/total_loss/avg: 0.2848, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 162ms, time_since_start: 01h 17m 10s 876ms, eta: 04h 45m 14s 555ms\n",
            "\u001b[32m2022-04-28T14:48:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0184, train/hateful_memes/cross_entropy/avg: 0.2787, train/total_loss: 0.0184, train/total_loss/avg: 0.2787, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 744ms, time_since_start: 01h 18m 46s 620ms, eta: 04h 42m 22s 708ms\n",
            "\u001b[32m2022-04-28T14:49:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0184, train/hateful_memes/cross_entropy/avg: 0.2734, train/total_loss: 0.0184, train/total_loss/avg: 0.2734, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 125ms, time_since_start: 01h 20m 22s 745ms, eta: 04h 41m 52s 413ms\n",
            "\u001b[32m2022-04-28T14:51:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0184, train/hateful_memes/cross_entropy/avg: 0.2706, train/total_loss: 0.0184, train/total_loss/avg: 0.2706, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 689ms, time_since_start: 01h 21m 58s 435ms, eta: 04h 38m 58s 445ms\n",
            "\u001b[32m2022-04-28T14:52:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0182, train/hateful_memes/cross_entropy/avg: 0.2652, train/total_loss: 0.0182, train/total_loss/avg: 0.2652, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 141ms, time_since_start: 01h 23m 34s 577ms, eta: 04h 38m 39s 767ms\n",
            "\u001b[32m2022-04-28T14:54:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T14:54:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T14:54:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T14:54:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T14:54:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0165, train/hateful_memes/cross_entropy/avg: 0.2600, train/total_loss: 0.0165, train/total_loss/avg: 0.2600, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 01s 790ms, time_since_start: 01h 25m 36s 368ms, eta: 05h 50m 56s 329ms\n",
            "\u001b[32m2022-04-28T14:54:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T14:54:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T14:55:02 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T14:55:02 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T14:55:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T14:55:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T14:55:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T14:55:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.8362, val/total_loss: 1.8362, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.4531, val/hateful_memes/roc_auc: 0.6964, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 31s 503ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.703441\n",
            "\u001b[32m2022-04-28T14:57:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0165, train/hateful_memes/cross_entropy/avg: 0.2557, train/total_loss: 0.0165, train/total_loss/avg: 0.2557, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 1.02, time: 01m 38s 074ms, time_since_start: 01h 27m 45s 952ms, eta: 04h 40m 56s 288ms\n",
            "\u001b[32m2022-04-28T14:58:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0144, train/hateful_memes/cross_entropy/avg: 0.2510, train/total_loss: 0.0144, train/total_loss/avg: 0.2510, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 1.03, time: 01m 37s 249ms, time_since_start: 01h 29m 23s 202ms, eta: 04h 36m 55s 698ms\n",
            "\u001b[32m2022-04-28T15:00:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0127, train/hateful_memes/cross_entropy/avg: 0.2463, train/total_loss: 0.0127, train/total_loss/avg: 0.2463, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 559ms, time_since_start: 01h 30m 59s 762ms, eta: 04h 33m 19s 583ms\n",
            "\u001b[32m2022-04-28T15:01:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0109, train/hateful_memes/cross_entropy/avg: 0.2418, train/total_loss: 0.0109, train/total_loss/avg: 0.2418, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 347ms, time_since_start: 01h 32m 36s 109ms, eta: 04h 31m 05s 564ms\n",
            "\u001b[32m2022-04-28T15:03:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0109, train/hateful_memes/cross_entropy/avg: 0.2376, train/total_loss: 0.0109, train/total_loss/avg: 0.2376, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 408ms, time_since_start: 01h 34m 12s 517ms, eta: 04h 29m 37s 757ms\n",
            "\u001b[32m2022-04-28T15:05:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0066, train/hateful_memes/cross_entropy/avg: 0.2334, train/total_loss: 0.0066, train/total_loss/avg: 0.2334, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 107ms, time_since_start: 01h 35m 48s 625ms, eta: 04h 27m 09s 644ms\n",
            "\u001b[32m2022-04-28T15:06:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0057, train/hateful_memes/cross_entropy/avg: 0.2293, train/total_loss: 0.0057, train/total_loss/avg: 0.2293, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 197ms, time_since_start: 01h 37m 24s 823ms, eta: 04h 25m 46s 833ms\n",
            "\u001b[32m2022-04-28T15:08:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0057, train/hateful_memes/cross_entropy/avg: 0.2274, train/total_loss: 0.0057, train/total_loss/avg: 0.2274, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 267ms, time_since_start: 01h 39m 01s 090ms, eta: 04h 24m 20s 406ms\n",
            "\u001b[32m2022-04-28T15:09:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0052, train/hateful_memes/cross_entropy/avg: 0.2236, train/total_loss: 0.0052, train/total_loss/avg: 0.2236, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 669ms, time_since_start: 01h 40m 36s 760ms, eta: 04h 21m 04s 636ms\n",
            "\u001b[32m2022-04-28T15:11:32 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T15:11:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T15:11:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T15:11:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T15:11:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0052, train/hateful_memes/cross_entropy/avg: 0.2204, train/total_loss: 0.0052, train/total_loss/avg: 0.2204, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 01s 877ms, time_since_start: 01h 42m 38s 638ms, eta: 05h 30m 31s 937ms\n",
            "\u001b[32m2022-04-28T15:11:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T15:11:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T15:12:04 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T15:12:04 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T15:12:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T15:12:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T15:12:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T15:12:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.2451, val/total_loss: 2.2451, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.3696, val/hateful_memes/roc_auc: 0.6918, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 30s 688ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.703441\n",
            "\u001b[32m2022-04-28T15:14:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0052, train/hateful_memes/cross_entropy/avg: 0.2169, train/total_loss: 0.0052, train/total_loss/avg: 0.2169, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 1.02, time: 01m 38s 012ms, time_since_start: 01h 44m 47s 340ms, eta: 04h 24m 08s 846ms\n",
            "\u001b[32m2022-04-28T15:15:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0057, train/hateful_memes/cross_entropy/avg: 0.2138, train/total_loss: 0.0057, train/total_loss/avg: 0.2138, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 979ms, time_since_start: 01h 46m 24s 319ms, eta: 04h 19m 43s 185ms\n",
            "\u001b[32m2022-04-28T15:17:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0086, train/hateful_memes/cross_entropy/avg: 0.2116, train/total_loss: 0.0086, train/total_loss/avg: 0.2116, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 375ms, time_since_start: 01h 48m 694ms, eta: 04h 16m 28s 140ms\n",
            "\u001b[32m2022-04-28T15:18:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0086, train/hateful_memes/cross_entropy/avg: 0.2084, train/total_loss: 0.0086, train/total_loss/avg: 0.2084, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 827ms, time_since_start: 01h 49m 36s 522ms, eta: 04h 13m 23s 192ms\n",
            "\u001b[32m2022-04-28T15:20:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0074, train/hateful_memes/cross_entropy/avg: 0.2053, train/total_loss: 0.0074, train/total_loss/avg: 0.2053, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 170ms, time_since_start: 01h 51m 12s 692ms, eta: 04h 12m 39s 763ms\n",
            "\u001b[32m2022-04-28T15:22:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0074, train/hateful_memes/cross_entropy/avg: 0.2022, train/total_loss: 0.0074, train/total_loss/avg: 0.2022, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 127ms, time_since_start: 01h 52m 48s 819ms, eta: 04h 10m 55s 279ms\n",
            "\u001b[32m2022-04-28T15:23:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0074, train/hateful_memes/cross_entropy/avg: 0.1996, train/total_loss: 0.0074, train/total_loss/avg: 0.1996, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 824ms, time_since_start: 01h 54m 24s 644ms, eta: 04h 08m 30s 384ms\n",
            "\u001b[32m2022-04-28T15:25:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0057, train/hateful_memes/cross_entropy/avg: 0.1967, train/total_loss: 0.0057, train/total_loss/avg: 0.1967, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 064ms, time_since_start: 01h 56m 708ms, eta: 04h 07m 30s 033ms\n",
            "\u001b[32m2022-04-28T15:26:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0057, train/hateful_memes/cross_entropy/avg: 0.1939, train/total_loss: 0.0057, train/total_loss/avg: 0.1939, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 029ms, time_since_start: 01h 57m 36s 738ms, eta: 04h 05m 46s 948ms\n",
            "\u001b[32m2022-04-28T15:28:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T15:28:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T15:28:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T15:28:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T15:28:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0049, train/hateful_memes/cross_entropy/avg: 0.1911, train/total_loss: 0.0049, train/total_loss/avg: 0.1911, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 01s 947ms, time_since_start: 01h 59m 38s 686ms, eta: 05h 10m 03s 168ms\n",
            "\u001b[32m2022-04-28T15:28:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T15:28:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T15:29:04 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T15:29:04 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T15:29:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T15:29:18 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2022-04-28T15:29:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T15:29:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T15:29:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 1.8556, val/total_loss: 1.8556, val/hateful_memes/accuracy: 0.7130, val/hateful_memes/binary_f1: 0.5401, val/hateful_memes/roc_auc: 0.7049, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 46s 084ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T15:31:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1885, train/total_loss: 0.0028, train/total_loss/avg: 0.1885, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 1.02, time: 01m 38s 230ms, time_since_start: 02h 02m 03s 002ms, eta: 04h 08m 05s 116ms\n",
            "\u001b[32m2022-04-28T15:32:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1859, train/total_loss: 0.0028, train/total_loss/avg: 0.1859, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 807ms, time_since_start: 02h 03m 39s 809ms, eta: 04h 02m 51s 015ms\n",
            "\u001b[32m2022-04-28T15:34:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1833, train/total_loss: 0.0025, train/total_loss/avg: 0.1833, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 457ms, time_since_start: 02h 05m 16s 266ms, eta: 04h 20s 285ms\n",
            "\u001b[32m2022-04-28T15:36:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1809, train/total_loss: 0.0025, train/total_loss/avg: 0.1809, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 458ms, time_since_start: 02h 06m 52s 725ms, eta: 03h 58m 42s 411ms\n",
            "\u001b[32m2022-04-28T15:37:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1798, train/total_loss: 0.0025, train/total_loss/avg: 0.1798, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 020ms, time_since_start: 02h 08m 28s 746ms, eta: 03h 55m 59s 712ms\n",
            "\u001b[32m2022-04-28T15:39:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.1775, train/total_loss: 0.0037, train/total_loss/avg: 0.1775, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 136ms, time_since_start: 02h 10m 04s 882ms, eta: 03h 54m 38s 960ms\n",
            "\u001b[32m2022-04-28T15:41:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.1752, train/total_loss: 0.0037, train/total_loss/avg: 0.1752, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 184ms, time_since_start: 02h 11m 41s 067ms, eta: 03h 53m 08s 215ms\n",
            "\u001b[32m2022-04-28T15:42:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.1747, train/total_loss: 0.0037, train/total_loss/avg: 0.1747, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 147ms, time_since_start: 02h 13m 17s 214ms, eta: 03h 51m 24s 991ms\n",
            "\u001b[32m2022-04-28T15:44:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.1736, train/total_loss: 0.0041, train/total_loss/avg: 0.1736, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 307ms, time_since_start: 02h 14m 53s 522ms, eta: 03h 50m 10s 242ms\n",
            "\u001b[32m2022-04-28T15:45:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T15:45:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T15:46:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T15:46:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T15:46:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.1714, train/total_loss: 0.0037, train/total_loss/avg: 0.1714, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 021ms, time_since_start: 02h 16m 53s 543ms, eta: 04h 44m 48s 625ms\n",
            "\u001b[32m2022-04-28T15:46:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T15:46:12 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T15:46:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T15:46:20 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T15:46:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T15:46:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T15:46:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T15:46:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.1962, val/total_loss: 2.1962, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.4908, val/hateful_memes/roc_auc: 0.6808, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 32s 243ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T15:48:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1693, train/total_loss: 0.0033, train/total_loss/avg: 0.1693, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00001, ups: 1.02, time: 01m 38s 753ms, time_since_start: 02h 19m 04s 542ms, eta: 03h 52m 40s 124ms\n",
            "\u001b[32m2022-04-28T15:50:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1673, train/total_loss: 0.0025, train/total_loss/avg: 0.1673, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 566ms, time_since_start: 02h 20m 41s 109ms, eta: 03h 45m 52s 742ms\n",
            "\u001b[32m2022-04-28T15:51:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1653, train/total_loss: 0.0023, train/total_loss/avg: 0.1653, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 075ms, time_since_start: 02h 22m 17s 185ms, eta: 03h 43m 06s 117ms\n",
            "\u001b[32m2022-04-28T15:53:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1633, train/total_loss: 0.0020, train/total_loss/avg: 0.1633, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 210ms, time_since_start: 02h 23m 53s 395ms, eta: 03h 41m 47s 004ms\n",
            "\u001b[32m2022-04-28T15:54:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1616, train/total_loss: 0.0020, train/total_loss/avg: 0.1616, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 134ms, time_since_start: 02h 25m 29s 529ms, eta: 03h 39m 58s 777ms\n",
            "\u001b[32m2022-04-28T15:56:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1598, train/total_loss: 0.0023, train/total_loss/avg: 0.1598, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 658ms, time_since_start: 02h 27m 05s 188ms, eta: 03h 37m 16s 185ms\n",
            "\u001b[32m2022-04-28T15:58:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1580, train/total_loss: 0.0020, train/total_loss/avg: 0.1580, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 989ms, time_since_start: 02h 28m 41s 178ms, eta: 03h 36m 23s 657ms\n",
            "\u001b[32m2022-04-28T15:59:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1563, train/total_loss: 0.0023, train/total_loss/avg: 0.1563, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 692ms, time_since_start: 02h 30m 16s 870ms, eta: 03h 34m 06s 095ms\n",
            "\u001b[32m2022-04-28T16:01:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1545, train/total_loss: 0.0023, train/total_loss/avg: 0.1545, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 724ms, time_since_start: 02h 31m 52s 594ms, eta: 03h 32m 33s 053ms\n",
            "\u001b[32m2022-04-28T16:02:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T16:02:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T16:02:59 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T16:03:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T16:03:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1528, train/total_loss: 0.0021, train/total_loss/avg: 0.1528, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 640ms, time_since_start: 02h 33m 53s 235ms, eta: 04h 25m 49s 934ms\n",
            "\u001b[32m2022-04-28T16:03:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T16:03:12 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T16:03:19 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T16:03:19 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T16:03:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T16:03:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T16:03:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T16:03:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.1533, val/total_loss: 2.1533, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.4812, val/hateful_memes/roc_auc: 0.6892, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 22000, val_time: 32s 431ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T16:05:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1550, train/total_loss: 0.0023, train/total_loss/avg: 0.1550, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00001, ups: 1.03, time: 01m 37s 171ms, time_since_start: 02h 36m 02s 840ms, eta: 03h 32m 28s 165ms\n",
            "\u001b[32m2022-04-28T16:06:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1533, train/total_loss: 0.0023, train/total_loss/avg: 0.1533, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 322ms, time_since_start: 02h 37m 39s 162ms, eta: 03h 28m 58s 885ms\n",
            "\u001b[32m2022-04-28T16:08:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1517, train/total_loss: 0.0033, train/total_loss/avg: 0.1517, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 131ms, time_since_start: 02h 39m 15s 294ms, eta: 03h 26m 56s 315ms\n",
            "\u001b[32m2022-04-28T16:10:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1501, train/total_loss: 0.0021, train/total_loss/avg: 0.1501, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 855ms, time_since_start: 02h 40m 51s 150ms, eta: 03h 24m 43s 180ms\n",
            "\u001b[32m2022-04-28T16:11:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1486, train/total_loss: 0.0020, train/total_loss/avg: 0.1486, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 003ms, time_since_start: 02h 42m 27s 153ms, eta: 03h 23m 24s 396ms\n",
            "\u001b[32m2022-04-28T16:13:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1475, train/total_loss: 0.0020, train/total_loss/avg: 0.1475, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 490ms, time_since_start: 02h 44m 02s 644ms, eta: 03h 20m 42s 069ms\n",
            "\u001b[32m2022-04-28T16:14:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1460, train/total_loss: 0.0019, train/total_loss/avg: 0.1460, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 813ms, time_since_start: 02h 45m 38s 457ms, eta: 03h 19m 45s 434ms\n",
            "\u001b[32m2022-04-28T16:16:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1446, train/total_loss: 0.0019, train/total_loss/avg: 0.1446, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 902ms, time_since_start: 02h 47m 14s 360ms, eta: 03h 18m 19s 054ms\n",
            "\u001b[32m2022-04-28T16:18:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1431, train/total_loss: 0.0016, train/total_loss/avg: 0.1431, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 702ms, time_since_start: 02h 48m 50s 063ms, eta: 03h 16m 16s 877ms\n",
            "\u001b[32m2022-04-28T16:19:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T16:19:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T16:19:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T16:20:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T16:20:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1417, train/total_loss: 0.0016, train/total_loss/avg: 0.1417, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 01s 599ms, time_since_start: 02h 50m 51s 663ms, eta: 04h 07m 20s 058ms\n",
            "\u001b[32m2022-04-28T16:20:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T16:20:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T16:20:18 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T16:20:18 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T16:20:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T16:20:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T16:20:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T16:20:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.4477, val/total_loss: 2.4477, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4783, val/hateful_memes/roc_auc: 0.6873, num_updates: 10000, epoch: 38, iterations: 10000, max_updates: 22000, val_time: 31s 583ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T16:22:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1403, train/total_loss: 0.0016, train/total_loss/avg: 0.1403, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00001, ups: 1.02, time: 01m 38s 043ms, time_since_start: 02h 53m 01s 292ms, eta: 03h 17m 45s 518ms\n",
            "\u001b[32m2022-04-28T16:23:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1390, train/total_loss: 0.0019, train/total_loss/avg: 0.1390, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 265ms, time_since_start: 02h 54m 37s 557ms, eta: 03h 12m 32s 428ms\n",
            "\u001b[32m2022-04-28T16:25:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1376, train/total_loss: 0.0016, train/total_loss/avg: 0.1376, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 665ms, time_since_start: 02h 56m 14s 223ms, eta: 03h 11m 42s 132ms\n",
            "\u001b[32m2022-04-28T16:27:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1363, train/total_loss: 0.0016, train/total_loss/avg: 0.1363, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 934ms, time_since_start: 02h 57m 50s 158ms, eta: 03h 08m 37s 611ms\n",
            "\u001b[32m2022-04-28T16:28:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1351, train/total_loss: 0.0016, train/total_loss/avg: 0.1351, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 122ms, time_since_start: 02h 59m 26s 280ms, eta: 03h 07m 21s 968ms\n",
            "\u001b[32m2022-04-28T16:30:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1338, train/total_loss: 0.0013, train/total_loss/avg: 0.1338, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 075ms, time_since_start: 03h 01m 02s 355ms, eta: 03h 05m 38s 765ms\n",
            "\u001b[32m2022-04-28T16:31:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1325, train/total_loss: 0.0013, train/total_loss/avg: 0.1325, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 436ms, time_since_start: 03h 02m 37s 792ms, eta: 03h 02m 47s 708ms\n",
            "\u001b[32m2022-04-28T16:33:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1313, train/total_loss: 0.0012, train/total_loss/avg: 0.1313, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 745ms, time_since_start: 03h 04m 13s 537ms, eta: 03h 01m 45s 773ms\n",
            "\u001b[32m2022-04-28T16:35:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1301, train/total_loss: 0.0008, train/total_loss/avg: 0.1301, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 705ms, time_since_start: 03h 05m 49s 243ms, eta: 03h 03s 958ms\n",
            "\u001b[32m2022-04-28T16:36:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T16:36:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T16:36:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T16:37:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T16:37:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1289, train/total_loss: 0.0008, train/total_loss/avg: 0.1289, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 666ms, time_since_start: 03h 07m 49s 910ms, eta: 03h 44m 58s 971ms\n",
            "\u001b[32m2022-04-28T16:37:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T16:37:09 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T16:37:16 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T16:37:16 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T16:37:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T16:37:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T16:37:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T16:37:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.4861, val/total_loss: 2.4861, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4295, val/hateful_memes/roc_auc: 0.6887, num_updates: 11000, epoch: 42, iterations: 11000, max_updates: 22000, val_time: 32s 265ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T16:39:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1278, train/total_loss: 0.0007, train/total_loss/avg: 0.1278, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00001, ups: 1.03, time: 01m 37s 729ms, time_since_start: 03h 09m 59s 907ms, eta: 03h 33s 565ms\n",
            "\u001b[32m2022-04-28T16:40:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1267, train/total_loss: 0.0008, train/total_loss/avg: 0.1267, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 270ms, time_since_start: 03h 11m 36s 178ms, eta: 02h 56m 14s 005ms\n",
            "\u001b[32m2022-04-28T16:42:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1256, train/total_loss: 0.0008, train/total_loss/avg: 0.1256, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 351ms, time_since_start: 03h 13m 12s 529ms, eta: 02h 54m 44s 898ms\n",
            "\u001b[32m2022-04-28T16:44:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1245, train/total_loss: 0.0008, train/total_loss/avg: 0.1245, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 038ms, time_since_start: 03h 14m 48s 568ms, eta: 02h 52m 33s 156ms\n",
            "\u001b[32m2022-04-28T16:45:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1234, train/total_loss: 0.0008, train/total_loss/avg: 0.1234, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 878ms, time_since_start: 03h 16m 24s 446ms, eta: 02h 50m 38s 374ms\n",
            "\u001b[32m2022-04-28T16:47:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1224, train/total_loss: 0.0008, train/total_loss/avg: 0.1224, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 877ms, time_since_start: 03h 18m 324ms, eta: 02h 49m 764ms\n",
            "\u001b[32m2022-04-28T16:48:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1213, train/total_loss: 0.0007, train/total_loss/avg: 0.1213, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 839ms, time_since_start: 03h 19m 36s 163ms, eta: 02h 47m 19s 268ms\n",
            "\u001b[32m2022-04-28T16:50:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1203, train/total_loss: 0.0003, train/total_loss/avg: 0.1203, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 545ms, time_since_start: 03h 21m 11s 708ms, eta: 02h 45m 11s 280ms\n",
            "\u001b[32m2022-04-28T16:52:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1193, train/total_loss: 0.0004, train/total_loss/avg: 0.1193, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 977ms, time_since_start: 03h 22m 47s 686ms, eta: 02h 44m 18s 540ms\n",
            "\u001b[32m2022-04-28T16:53:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T16:53:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T16:53:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T16:54:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T16:54:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1183, train/total_loss: 0.0004, train/total_loss/avg: 0.1183, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00001, ups: 0.83, time: 02m 01s 674ms, time_since_start: 03h 24m 49s 361ms, eta: 03h 26m 14s 292ms\n",
            "\u001b[32m2022-04-28T16:54:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T16:54:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T16:54:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T16:54:15 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T16:54:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T16:54:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T16:54:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T16:54:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.4219, val/total_loss: 2.4219, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.4830, val/hateful_memes/roc_auc: 0.6867, num_updates: 12000, epoch: 46, iterations: 12000, max_updates: 22000, val_time: 31s 443ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T16:56:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1173, train/total_loss: 0.0003, train/total_loss/avg: 0.1173, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0., ups: 1.01, time: 01m 39s 337ms, time_since_start: 03h 27m 143ms, eta: 02h 46m 41s 573ms\n",
            "\u001b[32m2022-04-28T16:57:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1163, train/total_loss: 0.0003, train/total_loss/avg: 0.1163, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 834ms, time_since_start: 03h 28m 36s 978ms, eta: 02h 40m 51s 118ms\n",
            "\u001b[32m2022-04-28T16:59:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1154, train/total_loss: 0.0003, train/total_loss/avg: 0.1154, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 200ms, time_since_start: 03h 30m 13s 178ms, eta: 02h 38m 10s 110ms\n",
            "\u001b[32m2022-04-28T17:01:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1145, train/total_loss: 0.0003, train/total_loss/avg: 0.1145, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 213ms, time_since_start: 03h 31m 49s 392ms, eta: 02h 36m 33s 527ms\n",
            "\u001b[32m2022-04-28T17:02:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1136, train/total_loss: 0.0003, train/total_loss/avg: 0.1136, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 111ms, time_since_start: 03h 33m 25s 504ms, eta: 02h 34m 45s 859ms\n",
            "\u001b[32m2022-04-28T17:04:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1127, train/total_loss: 0.0003, train/total_loss/avg: 0.1127, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 902ms, time_since_start: 03h 35m 01s 407ms, eta: 02h 32m 48s 135ms\n",
            "\u001b[32m2022-04-28T17:05:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1118, train/total_loss: 0.0003, train/total_loss/avg: 0.1118, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 867ms, time_since_start: 03h 36m 37s 275ms, eta: 02h 31m 07s 282ms\n",
            "\u001b[32m2022-04-28T17:07:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1109, train/total_loss: 0.0003, train/total_loss/avg: 0.1109, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 261ms, time_since_start: 03h 38m 12s 537ms, eta: 02h 28m 33s 067ms\n",
            "\u001b[32m2022-04-28T17:09:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1101, train/total_loss: 0.0003, train/total_loss/avg: 0.1101, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 672ms, time_since_start: 03h 39m 48s 210ms, eta: 02h 27m 34s 238ms\n",
            "\u001b[32m2022-04-28T17:10:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T17:10:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T17:10:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T17:11:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T17:11:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1092, train/total_loss: 0.0003, train/total_loss/avg: 0.1092, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0., ups: 0.83, time: 02m 386ms, time_since_start: 03h 41m 48s 597ms, eta: 03h 03m 39s 012ms\n",
            "\u001b[32m2022-04-28T17:11:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T17:11:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T17:11:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T17:11:15 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T17:11:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T17:11:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T17:11:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T17:11:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.6260, val/total_loss: 2.6260, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4056, val/hateful_memes/roc_auc: 0.6928, num_updates: 13000, epoch: 49, iterations: 13000, max_updates: 22000, val_time: 34s 097ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T17:13:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1084, train/total_loss: 0.0003, train/total_loss/avg: 0.1084, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 112ms, time_since_start: 03h 44m 812ms, eta: 02h 28m 495ms\n",
            "\u001b[32m2022-04-28T17:14:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1082, train/total_loss: 0.0003, train/total_loss/avg: 0.1082, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 484ms, time_since_start: 03h 45m 37s 296ms, eta: 02h 23m 54s 974ms\n",
            "\u001b[32m2022-04-28T17:16:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1077, train/total_loss: 0.0003, train/total_loss/avg: 0.1077, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 407ms, time_since_start: 03h 47m 12s 704ms, eta: 02h 20m 41s 543ms\n",
            "\u001b[32m2022-04-28T17:18:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1069, train/total_loss: 0.0003, train/total_loss/avg: 0.1069, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 008ms, time_since_start: 03h 48m 48s 713ms, eta: 02h 19m 57s 124ms\n",
            "\u001b[32m2022-04-28T17:19:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1061, train/total_loss: 0.0003, train/total_loss/avg: 0.1061, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 875ms, time_since_start: 03h 50m 24s 588ms, eta: 02h 18m 07s 975ms\n",
            "\u001b[32m2022-04-28T17:21:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1053, train/total_loss: 0.0003, train/total_loss/avg: 0.1053, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 595ms, time_since_start: 03h 52m 184ms, eta: 02h 16m 06s 568ms\n",
            "\u001b[32m2022-04-28T17:22:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1046, train/total_loss: 0.0003, train/total_loss/avg: 0.1046, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 994ms, time_since_start: 03h 53m 36s 179ms, eta: 02h 15m 03s 030ms\n",
            "\u001b[32m2022-04-28T17:24:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1038, train/total_loss: 0.0004, train/total_loss/avg: 0.1038, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 976ms, time_since_start: 03h 55m 12s 156ms, eta: 02h 13m 23s 900ms\n",
            "\u001b[32m2022-04-28T17:26:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1031, train/total_loss: 0.0004, train/total_loss/avg: 0.1031, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 451ms, time_since_start: 03h 56m 47s 608ms, eta: 02h 11m 03s 000ms\n",
            "\u001b[32m2022-04-28T17:27:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T17:27:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T17:27:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T17:28:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T17:28:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1024, train/total_loss: 0.0005, train/total_loss/avg: 0.1024, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0., ups: 0.82, time: 02m 02s 832ms, time_since_start: 03h 58m 50s 440ms, eta: 02h 46m 33s 640ms\n",
            "\u001b[32m2022-04-28T17:28:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T17:28:09 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T17:28:16 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T17:28:16 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T17:28:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T17:28:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T17:28:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T17:28:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.5611, val/total_loss: 2.5611, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.5194, val/hateful_memes/roc_auc: 0.6953, num_updates: 14000, epoch: 53, iterations: 14000, max_updates: 22000, val_time: 35s 587ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T17:30:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1016, train/total_loss: 0.0005, train/total_loss/avg: 0.1016, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0., ups: 1.03, time: 01m 37s 922ms, time_since_start: 04h 01m 03s 951ms, eta: 02h 11m 07s 360ms\n",
            "\u001b[32m2022-04-28T17:31:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1009, train/total_loss: 0.0005, train/total_loss/avg: 0.1009, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 532ms, time_since_start: 04h 02m 40s 483ms, eta: 02h 07m 37s 519ms\n",
            "\u001b[32m2022-04-28T17:33:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1002, train/total_loss: 0.0005, train/total_loss/avg: 0.1002, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 539ms, time_since_start: 04h 04m 17s 022ms, eta: 02h 05m 59s 882ms\n",
            "\u001b[32m2022-04-28T17:35:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0995, train/total_loss: 0.0005, train/total_loss/avg: 0.0995, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 041ms, time_since_start: 04h 05m 53s 063ms, eta: 02h 03m 43s 213ms\n",
            "\u001b[32m2022-04-28T17:36:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0988, train/total_loss: 0.0004, train/total_loss/avg: 0.0988, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 067ms, time_since_start: 04h 07m 29s 131ms, eta: 02h 02m 07s 568ms\n",
            "\u001b[32m2022-04-28T17:38:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0982, train/total_loss: 0.0005, train/total_loss/avg: 0.0982, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 992ms, time_since_start: 04h 09m 05s 124ms, eta: 02h 24s 222ms\n",
            "\u001b[32m2022-04-28T17:40:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0975, train/total_loss: 0.0005, train/total_loss/avg: 0.0975, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 929ms, time_since_start: 04h 10m 41s 053ms, eta: 01h 58m 41s 871ms\n",
            "\u001b[32m2022-04-28T17:41:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.0969, train/total_loss: 0.0008, train/total_loss/avg: 0.0969, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 982ms, time_since_start: 04h 12m 17s 036ms, eta: 01h 57m 08s 223ms\n",
            "\u001b[32m2022-04-28T17:43:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0962, train/total_loss: 0.0005, train/total_loss/avg: 0.0962, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 371ms, time_since_start: 04h 13m 52s 407ms, eta: 01h 54m 46s 498ms\n",
            "\u001b[32m2022-04-28T17:44:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T17:44:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T17:45:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T17:45:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T17:45:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0956, train/total_loss: 0.0004, train/total_loss/avg: 0.0956, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0., ups: 0.83, time: 02m 378ms, time_since_start: 04h 15m 52s 785ms, eta: 02h 22m 49s 722ms\n",
            "\u001b[32m2022-04-28T17:45:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T17:45:12 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T17:45:19 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T17:45:19 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T17:45:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T17:45:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T17:45:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T17:45:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.8100, val/total_loss: 2.8100, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4437, val/hateful_memes/roc_auc: 0.6939, num_updates: 15000, epoch: 57, iterations: 15000, max_updates: 22000, val_time: 32s 226ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T17:47:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0949, train/total_loss: 0.0004, train/total_loss/avg: 0.0949, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 140ms, time_since_start: 04h 18m 03s 154ms, eta: 01h 54m 46s 819ms\n",
            "\u001b[32m2022-04-28T17:48:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0943, train/total_loss: 0.0003, train/total_loss/avg: 0.0943, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 994ms, time_since_start: 04h 19m 40s 149ms, eta: 01h 51m 47s 758ms\n",
            "\u001b[32m2022-04-28T17:50:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0937, train/total_loss: 0.0002, train/total_loss/avg: 0.0937, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 381ms, time_since_start: 04h 21m 16s 530ms, eta: 01h 49m 27s 323ms\n",
            "\u001b[32m2022-04-28T17:52:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0931, train/total_loss: 0.0003, train/total_loss/avg: 0.0931, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 365ms, time_since_start: 04h 22m 52s 896ms, eta: 01h 47m 48s 252ms\n",
            "\u001b[32m2022-04-28T17:53:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0925, train/total_loss: 0.0002, train/total_loss/avg: 0.0925, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 648ms, time_since_start: 04h 24m 28s 544ms, eta: 01h 45m 22s 842ms\n",
            "\u001b[32m2022-04-28T17:55:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0919, train/total_loss: 0.0001, train/total_loss/avg: 0.0919, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 210ms, time_since_start: 04h 26m 04s 755ms, eta: 01h 44m 22s 148ms\n",
            "\u001b[32m2022-04-28T17:57:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0913, train/total_loss: 0.0001, train/total_loss/avg: 0.0913, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 119ms, time_since_start: 04h 27m 40s 875ms, eta: 01h 42m 38s 493ms\n",
            "\u001b[32m2022-04-28T17:58:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0907, train/total_loss: 0.0001, train/total_loss/avg: 0.0907, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 167ms, time_since_start: 04h 29m 17s 042ms, eta: 01h 41m 03s 729ms\n",
            "\u001b[32m2022-04-28T18:00:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0902, train/total_loss: 0.0001, train/total_loss/avg: 0.0902, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 217ms, time_since_start: 04h 30m 53s 259ms, eta: 01h 39m 29s 037ms\n",
            "\u001b[32m2022-04-28T18:01:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T18:01:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T18:02:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T18:02:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T18:02:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0896, train/total_loss: 0.0001, train/total_loss/avg: 0.0896, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0., ups: 0.83, time: 02m 01s 266ms, time_since_start: 04h 32m 54s 526ms, eta: 02h 03m 19s 711ms\n",
            "\u001b[32m2022-04-28T18:02:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T18:02:13 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T18:02:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T18:02:20 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T18:02:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T18:02:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T18:02:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T18:02:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 2.7213, val/total_loss: 2.7213, val/hateful_memes/accuracy: 0.7037, val/hateful_memes/binary_f1: 0.5000, val/hateful_memes/roc_auc: 0.6952, num_updates: 16000, epoch: 61, iterations: 16000, max_updates: 22000, val_time: 34s 103ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T18:04:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0891, train/total_loss: 0.0001, train/total_loss/avg: 0.0891, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 424ms, time_since_start: 04h 35m 07s 056ms, eta: 01h 38m 25s 794ms\n",
            "\u001b[32m2022-04-28T18:06:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0885, train/total_loss: 0.0001, train/total_loss/avg: 0.0885, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 883ms, time_since_start: 04h 36m 43s 939ms, eta: 01h 35m 14s 757ms\n",
            "\u001b[32m2022-04-28T18:07:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0880, train/total_loss: 0.0001, train/total_loss/avg: 0.0880, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 541ms, time_since_start: 04h 38m 20s 480ms, eta: 01h 33m 16s 390ms\n",
            "\u001b[32m2022-04-28T18:09:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0874, train/total_loss: 0.0001, train/total_loss/avg: 0.0874, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 662ms, time_since_start: 04h 39m 57s 143ms, eta: 01h 31m 45s 125ms\n",
            "\u001b[32m2022-04-28T18:10:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0869, train/total_loss: 0.0001, train/total_loss/avg: 0.0869, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 075ms, time_since_start: 04h 41m 33s 218ms, eta: 01h 29m 33s 975ms\n",
            "\u001b[32m2022-04-28T18:12:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0864, train/total_loss: 0.0001, train/total_loss/avg: 0.0864, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 116ms, time_since_start: 04h 43m 09s 335ms, eta: 01h 27m 58s 553ms\n",
            "\u001b[32m2022-04-28T18:14:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0859, train/total_loss: 0.0001, train/total_loss/avg: 0.0859, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 188ms, time_since_start: 04h 44m 45s 524ms, eta: 01h 26m 24s 669ms\n",
            "\u001b[32m2022-04-28T18:15:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0853, train/total_loss: 0.0001, train/total_loss/avg: 0.0853, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 945ms, time_since_start: 04h 46m 21s 470ms, eta: 01h 24m 34s 005ms\n",
            "\u001b[32m2022-04-28T18:17:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0848, train/total_loss: 0.0001, train/total_loss/avg: 0.0848, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 157ms, time_since_start: 04h 47m 57s 628ms, eta: 01h 23m 07s 414ms\n",
            "\u001b[32m2022-04-28T18:18:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T18:18:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T18:19:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T18:19:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T18:19:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0843, train/total_loss: 0.0001, train/total_loss/avg: 0.0843, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0., ups: 0.83, time: 02m 786ms, time_since_start: 04h 49m 58s 414ms, eta: 01h 42m 21s 996ms\n",
            "\u001b[32m2022-04-28T18:19:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T18:19:17 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T18:19:24 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T18:19:24 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T18:19:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T18:19:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T18:19:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T18:19:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 2.6695, val/total_loss: 2.6695, val/hateful_memes/accuracy: 0.7074, val/hateful_memes/binary_f1: 0.5000, val/hateful_memes/roc_auc: 0.6981, num_updates: 17000, epoch: 64, iterations: 17000, max_updates: 22000, val_time: 34s 506ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T18:21:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0838, train/total_loss: 0.0001, train/total_loss/avg: 0.0838, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 175ms, time_since_start: 04h 52m 11s 104ms, eta: 01h 21m 32s 378ms\n",
            "\u001b[32m2022-04-28T18:23:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0834, train/total_loss: 0.0001, train/total_loss/avg: 0.0834, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 712ms, time_since_start: 04h 53m 47s 817ms, eta: 01h 18m 41s 127ms\n",
            "\u001b[32m2022-04-28T18:24:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0829, train/total_loss: 0.0001, train/total_loss/avg: 0.0829, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 258ms, time_since_start: 04h 55m 24s 075ms, eta: 01h 16m 41s 041ms\n",
            "\u001b[32m2022-04-28T18:26:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0824, train/total_loss: 0.0001, train/total_loss/avg: 0.0824, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 336ms, time_since_start: 04h 57m 412ms, eta: 01h 15m 06s 811ms\n",
            "\u001b[32m2022-04-28T18:27:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0819, train/total_loss: 0.0001, train/total_loss/avg: 0.0819, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 395ms, time_since_start: 04h 58m 36s 807ms, eta: 01h 13m 31s 555ms\n",
            "\u001b[32m2022-04-28T18:29:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0815, train/total_loss: 0.0001, train/total_loss/avg: 0.0815, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 902ms, time_since_start: 05h 12s 710ms, eta: 01h 11m 31s 459ms\n",
            "\u001b[32m2022-04-28T18:31:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0810, train/total_loss: 0.0001, train/total_loss/avg: 0.0810, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 629ms, time_since_start: 05h 01m 49s 340ms, eta: 01h 10m 25s 720ms\n",
            "\u001b[32m2022-04-28T18:32:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0805, train/total_loss: 0.0001, train/total_loss/avg: 0.0805, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 360ms, time_since_start: 05h 03m 25s 701ms, eta: 01h 08m 35s 943ms\n",
            "\u001b[32m2022-04-28T18:34:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0801, train/total_loss: 0.0001, train/total_loss/avg: 0.0801, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 097ms, time_since_start: 05h 05m 01s 798ms, eta: 01h 06m 46s 957ms\n",
            "\u001b[32m2022-04-28T18:35:57 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T18:35:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T18:36:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T18:36:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T18:36:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0797, train/total_loss: 0.0001, train/total_loss/avg: 0.0797, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0., ups: 0.82, time: 02m 02s 068ms, time_since_start: 05h 07m 03s 866ms, eta: 01h 22m 45s 749ms\n",
            "\u001b[32m2022-04-28T18:36:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T18:36:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T18:36:30 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T18:36:30 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T18:36:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T18:36:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T18:36:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T18:36:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 2.7220, val/total_loss: 2.7220, val/hateful_memes/accuracy: 0.6981, val/hateful_memes/binary_f1: 0.4792, val/hateful_memes/roc_auc: 0.6966, num_updates: 18000, epoch: 68, iterations: 18000, max_updates: 22000, val_time: 33s 356ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T18:38:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0792, train/total_loss: 0.0001, train/total_loss/avg: 0.0792, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 297ms, time_since_start: 05h 09m 15s 522ms, eta: 01h 04m 58s 756ms\n",
            "\u001b[32m2022-04-28T18:40:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0788, train/total_loss: 0.0001, train/total_loss/avg: 0.0788, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 860ms, time_since_start: 05h 10m 52s 382ms, eta: 01h 02m 23s 268ms\n",
            "\u001b[32m2022-04-28T18:41:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0783, train/total_loss: 0.0000, train/total_loss/avg: 0.0783, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 645ms, time_since_start: 05h 12m 29s 027ms, eta: 01h 36s 659ms\n",
            "\u001b[32m2022-04-28T18:43:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0779, train/total_loss: 0.0000, train/total_loss/avg: 0.0779, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 173ms, time_since_start: 05h 14m 05s 201ms, eta: 58m 41s 108ms\n",
            "\u001b[32m2022-04-28T18:45:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0775, train/total_loss: 0.0000, train/total_loss/avg: 0.0775, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 411ms, time_since_start: 05h 15m 41s 612ms, eta: 57m 11s 765ms\n",
            "\u001b[32m2022-04-28T18:46:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0771, train/total_loss: 0.0000, train/total_loss/avg: 0.0771, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 610ms, time_since_start: 05h 17m 18s 222ms, eta: 55m 40s 584ms\n",
            "\u001b[32m2022-04-28T18:48:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0767, train/total_loss: 0.0000, train/total_loss/avg: 0.0767, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 802ms, time_since_start: 05h 18m 54s 024ms, eta: 53m 35s 213ms\n",
            "\u001b[32m2022-04-28T18:49:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0763, train/total_loss: 0.0000, train/total_loss/avg: 0.0763, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 218ms, time_since_start: 05h 20m 30s 243ms, eta: 52m 11s 329ms\n",
            "\u001b[32m2022-04-28T18:51:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0759, train/total_loss: 0.0000, train/total_loss/avg: 0.0759, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 093ms, time_since_start: 05h 22m 06s 336ms, eta: 50m 29s 526ms\n",
            "\u001b[32m2022-04-28T18:53:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T18:53:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T18:53:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T18:53:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T18:53:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0755, train/total_loss: 0.0000, train/total_loss/avg: 0.0755, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0., ups: 0.82, time: 02m 02s 520ms, time_since_start: 05h 24m 08s 857ms, eta: 01h 02m 18s 111ms\n",
            "\u001b[32m2022-04-28T18:53:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T18:53:28 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T18:53:35 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T18:53:35 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T18:53:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T18:53:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T18:54:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T18:54:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 2.7909, val/total_loss: 2.7909, val/hateful_memes/accuracy: 0.7000, val/hateful_memes/binary_f1: 0.4938, val/hateful_memes/roc_auc: 0.6992, num_updates: 19000, epoch: 72, iterations: 19000, max_updates: 22000, val_time: 32s 899ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T18:55:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0751, train/total_loss: 0.0000, train/total_loss/avg: 0.0751, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 609ms, time_since_start: 05h 26m 20s 368ms, eta: 48m 28s 286ms\n",
            "\u001b[32m2022-04-28T18:57:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0747, train/total_loss: 0.0000, train/total_loss/avg: 0.0747, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 555ms, time_since_start: 05h 27m 56s 923ms, eta: 45m 49s 503ms\n",
            "\u001b[32m2022-04-28T18:58:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0743, train/total_loss: 0.0000, train/total_loss/avg: 0.0743, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 491ms, time_since_start: 05h 29m 33s 414ms, eta: 44m 09s 548ms\n",
            "\u001b[32m2022-04-28T19:00:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0739, train/total_loss: 0.0000, train/total_loss/avg: 0.0739, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 508ms, time_since_start: 05h 31m 09s 923ms, eta: 42m 31s 885ms\n",
            "\u001b[32m2022-04-28T19:02:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0735, train/total_loss: 0.0000, train/total_loss/avg: 0.0735, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 953ms, time_since_start: 05h 32m 45s 876ms, eta: 40m 39s 622ms\n",
            "\u001b[32m2022-04-28T19:03:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0731, train/total_loss: 0.0000, train/total_loss/avg: 0.0731, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 001ms, time_since_start: 05h 34m 21s 878ms, eta: 39m 03s 213ms\n",
            "\u001b[32m2022-04-28T19:05:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0728, train/total_loss: 0.0000, train/total_loss/avg: 0.0728, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 615ms, time_since_start: 05h 35m 57s 494ms, eta: 37m 16s 541ms\n",
            "\u001b[32m2022-04-28T19:06:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0724, train/total_loss: 0.0000, train/total_loss/avg: 0.0724, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 954ms, time_since_start: 05h 37m 33s 448ms, eta: 35m 46s 877ms\n",
            "\u001b[32m2022-04-28T19:08:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0720, train/total_loss: 0.0000, train/total_loss/avg: 0.0720, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 008ms, time_since_start: 05h 39m 09s 456ms, eta: 34m 10s 450ms\n",
            "\u001b[32m2022-04-28T19:10:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T19:10:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T19:10:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T19:10:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T19:10:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0717, train/total_loss: 0.0000, train/total_loss/avg: 0.0717, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0., ups: 0.83, time: 02m 01s 081ms, time_since_start: 05h 41m 10s 538ms, eta: 41m 02s 799ms\n",
            "\u001b[32m2022-04-28T19:10:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T19:10:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T19:10:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T19:10:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T19:10:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T19:10:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T19:11:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T19:11:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 2.7649, val/total_loss: 2.7649, val/hateful_memes/accuracy: 0.7093, val/hateful_memes/binary_f1: 0.5078, val/hateful_memes/roc_auc: 0.6999, num_updates: 20000, epoch: 76, iterations: 20000, max_updates: 22000, val_time: 31s 289ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T19:12:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0713, train/total_loss: 0.0000, train/total_loss/avg: 0.0713, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.03, time: 01m 37s 540ms, time_since_start: 05h 43m 19s 368ms, eta: 31m 24s 771ms\n",
            "\u001b[32m2022-04-28T19:14:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0710, train/total_loss: 0.0000, train/total_loss/avg: 0.0710, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.03, time: 01m 37s 074ms, time_since_start: 05h 44m 56s 443ms, eta: 29m 37s 037ms\n",
            "\u001b[32m2022-04-28T19:15:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0706, train/total_loss: 0.0000, train/total_loss/avg: 0.0706, max mem: 10794.0, experiment: run, epoch: 77, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 880ms, time_since_start: 05h 46m 32s 323ms, eta: 27m 37s 674ms\n",
            "\u001b[32m2022-04-28T19:17:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0703, train/total_loss: 0.0000, train/total_loss/avg: 0.0703, max mem: 10794.0, experiment: run, epoch: 77, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 314ms, time_since_start: 05h 48m 08s 638ms, eta: 26m 07s 232ms\n",
            "\u001b[32m2022-04-28T19:19:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0699, train/total_loss: 0.0000, train/total_loss/avg: 0.0699, max mem: 10794.0, experiment: run, epoch: 78, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 183ms, time_since_start: 05h 49m 44s 821ms, eta: 24m 27s 277ms\n",
            "\u001b[32m2022-04-28T19:20:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0696, train/total_loss: 0.0000, train/total_loss/avg: 0.0696, max mem: 10794.0, experiment: run, epoch: 78, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 985ms, time_since_start: 05h 51m 20s 807ms, eta: 22m 46s 647ms\n",
            "\u001b[32m2022-04-28T19:22:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0692, train/total_loss: 0.0000, train/total_loss/avg: 0.0692, max mem: 10794.0, experiment: run, epoch: 78, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 020ms, time_since_start: 05h 52m 56s 827ms, eta: 21m 09s 483ms\n",
            "\u001b[32m2022-04-28T19:23:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0689, train/total_loss: 0.0000, train/total_loss/avg: 0.0689, max mem: 10794.0, experiment: run, epoch: 79, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 535ms, time_since_start: 05h 54m 32s 362ms, eta: 19m 25s 913ms\n",
            "\u001b[32m2022-04-28T19:25:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0686, train/total_loss: 0.0000, train/total_loss/avg: 0.0686, max mem: 10794.0, experiment: run, epoch: 79, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 103ms, time_since_start: 05h 56m 08s 466ms, eta: 17m 55s 108ms\n",
            "\u001b[32m2022-04-28T19:27:03 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T19:27:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T19:27:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T19:27:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T19:27:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0683, train/total_loss: 0.0000, train/total_loss/avg: 0.0683, max mem: 10794.0, experiment: run, epoch: 79, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 0.83, time: 02m 01s 678ms, time_since_start: 05h 58m 10s 144ms, eta: 20m 37s 467ms\n",
            "\u001b[32m2022-04-28T19:27:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T19:27:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T19:27:36 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T19:27:36 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T19:27:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T19:27:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T19:28:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T19:28:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 2.8480, val/total_loss: 2.8480, val/hateful_memes/accuracy: 0.7000, val/hateful_memes/binary_f1: 0.4706, val/hateful_memes/roc_auc: 0.6995, num_updates: 21000, epoch: 79, iterations: 21000, max_updates: 22000, val_time: 32s 392ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T19:29:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0679, train/total_loss: 0.0000, train/total_loss/avg: 0.0679, max mem: 10794.0, experiment: run, epoch: 80, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 523ms, time_since_start: 06h 21s 062ms, eta: 15m 01s 781ms\n",
            "\u001b[32m2022-04-28T19:31:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0676, train/total_loss: 0.0000, train/total_loss/avg: 0.0676, max mem: 10794.0, experiment: run, epoch: 80, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.03, time: 01m 37s 143ms, time_since_start: 06h 01m 58s 206ms, eta: 13m 10s 363ms\n",
            "\u001b[32m2022-04-28T19:32:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0673, train/total_loss: 0.0000, train/total_loss/avg: 0.0673, max mem: 10794.0, experiment: run, epoch: 81, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 128ms, time_since_start: 06h 03m 34s 335ms, eta: 11m 24s 340ms\n",
            "\u001b[32m2022-04-28T19:34:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0670, train/total_loss: 0.0000, train/total_loss/avg: 0.0670, max mem: 10794.0, experiment: run, epoch: 81, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 493ms, time_since_start: 06h 05m 10s 829ms, eta: 09m 48s 804ms\n",
            "\u001b[32m2022-04-28T19:36:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0667, train/total_loss: 0.0000, train/total_loss/avg: 0.0667, max mem: 10794.0, experiment: run, epoch: 81, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 454ms, time_since_start: 06h 06m 47s 283ms, eta: 08m 10s 469ms\n",
            "\u001b[32m2022-04-28T19:37:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0664, train/total_loss: 0.0000, train/total_loss/avg: 0.0664, max mem: 10794.0, experiment: run, epoch: 82, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 085ms, time_since_start: 06h 08m 23s 369ms, eta: 06m 30s 877ms\n",
            "\u001b[32m2022-04-28T19:39:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0661, train/total_loss: 0.0000, train/total_loss/avg: 0.0661, max mem: 10794.0, experiment: run, epoch: 82, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 367ms, time_since_start: 06h 09m 59s 737ms, eta: 04m 54s 018ms\n",
            "\u001b[32m2022-04-28T19:40:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0657, train/total_loss: 0.0000, train/total_loss/avg: 0.0657, max mem: 10794.0, experiment: run, epoch: 82, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 474ms, time_since_start: 06h 11m 36s 212ms, eta: 03m 16s 230ms\n",
            "\u001b[32m2022-04-28T19:42:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0654, train/total_loss: 0.0000, train/total_loss/avg: 0.0654, max mem: 10794.0, experiment: run, epoch: 83, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 817ms, time_since_start: 06h 13m 12s 029ms, eta: 01m 37s 445ms\n",
            "\u001b[32m2022-04-28T19:44:07 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2022-04-28T19:44:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T19:44:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T19:44:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T19:44:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0652, train/total_loss: 0.0000, train/total_loss/avg: 0.0652, max mem: 10794.0, experiment: run, epoch: 83, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 0.81, time: 02m 03s 179ms, time_since_start: 06h 15m 15s 208ms, eta: 0ms\n",
            "\u001b[32m2022-04-28T19:44:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-04-28T19:44:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2022-04-28T19:44:41 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T19:44:41 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T19:44:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2022-04-28T19:44:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2022-04-28T19:45:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2022-04-28T19:45:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/hateful_memes/cross_entropy: 2.8743, val/total_loss: 2.8743, val/hateful_memes/accuracy: 0.7056, val/hateful_memes/binary_f1: 0.4920, val/hateful_memes/roc_auc: 0.6993, num_updates: 22000, epoch: 83, iterations: 22000, max_updates: 22000, val_time: 34s 198ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.704904\n",
            "\u001b[32m2022-04-28T19:45:09 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-04-28T19:45:09 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-04-28T19:45:09 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-04-28T19:45:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-04-28T19:45:35 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 7000\n",
            "\u001b[32m2022-04-28T19:45:35 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 7000\n",
            "\u001b[32m2022-04-28T19:45:35 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 27\n",
            "\u001b[32m2022-04-28T19:45:47 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n",
            "\u001b[32m2022-04-28T19:45:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "100% 63/63 [00:20<00:00,  3.02it/s]\n",
            "\u001b[32m2022-04-28T19:46:08 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 63\n",
            "\u001b[32m2022-04-28T19:46:08 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T19:46:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, test/hateful_memes/cross_entropy: 1.9610, test/total_loss: 1.9610, test/hateful_memes/accuracy: 0.7030, test/hateful_memes/binary_f1: 0.5058, test/hateful_memes/roc_auc: 0.7324\n",
            "\u001b[32m2022-04-28T19:46:08 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 06h 16m 48s 926ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "Vpb05LA7OQjI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8e3a1fa7-6f89-45a8-cb04-3ac40f3ab5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/vilbert_hugo/hateful-memes/hateful-memes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\" \\\n",
        "model=vilbert \\\n",
        "dataset=hateful_memes \\\n",
        "run_type=val \\\n",
        "checkpoint.resume_file=save/best.ckpt \\\n",
        "checkpoint.resume_pretrained=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ3lmwL85WfL",
        "outputId": "02366836-8887-4ce4-ec66-401d7b3ccab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "\u001b[32m2022-04-28T20:25:07 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\n",
            "\u001b[32m2022-04-28T20:25:07 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-04-28T20:25:07 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2022-04-28T20:25:07 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
            "\u001b[32m2022-04-28T20:25:07 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n",
            "\u001b[32m2022-04-28T20:25:07 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "\u001b[32m2022-04-28T20:25:07 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-04-28T20:25:07 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False'])\n",
            "\u001b[32m2022-04-28T20:25:07 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-04-28T20:25:07 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-04-28T20:25:07 | mmf_cli.run: \u001b[0mUsing seed 7886600\n",
            "\u001b[32m2022-04-28T20:25:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[32m2022-04-28T20:25:15 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T20:25:15 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T20:25:15 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T20:25:15 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bi_attention_type\": 1,\n",
            "  \"bi_hidden_size\": 1024,\n",
            "  \"bi_intermediate_size\": 1024,\n",
            "  \"bi_num_attention_heads\": 8,\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cut_first\": \"text\",\n",
            "  \"dynamic_attention\": false,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"fast_mode\": false,\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"fixed_t_layer\": 0,\n",
            "  \"fixed_v_layer\": 0,\n",
            "  \"freeze_base\": false,\n",
            "  \"fusion_method\": \"mul\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hard_cap_seq_len\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"in_batch_pairs\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"vilbert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_negative\": 128,\n",
            "  \"objective\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooling_method\": \"mul\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"t_biattention_id\": [\n",
            "    6,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    11\n",
            "  ],\n",
            "  \"task_specific_tokens\": false,\n",
            "  \"text_only\": false,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"v_attention_probs_dropout_prob\": 0.1,\n",
            "  \"v_biattention_id\": [\n",
            "    0,\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    4,\n",
            "    5\n",
            "  ],\n",
            "  \"v_feature_size\": 2048,\n",
            "  \"v_hidden_act\": \"gelu\",\n",
            "  \"v_hidden_dropout_prob\": 0.1,\n",
            "  \"v_hidden_size\": 1024,\n",
            "  \"v_initializer_range\": 0.02,\n",
            "  \"v_intermediate_size\": 1024,\n",
            "  \"v_num_attention_heads\": 8,\n",
            "  \"v_num_hidden_layers\": 6,\n",
            "  \"v_target_size\": 1601,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"visual_target\": 0,\n",
            "  \"visualization\": false,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"with_coattention\": true\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.t_pooler.dense.weight', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.v_pooler.dense.weight', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.v_pooler.dense.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.3.v_output.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-04-28T20:25:26 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-04-28T20:25:26 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-04-28T20:25:27 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-28T20:25:40 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-28T20:25:40 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-04-28T20:25:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-04-28T20:25:40 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 7000\n",
            "\u001b[32m2022-04-28T20:25:40 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 7000\n",
            "\u001b[32m2022-04-28T20:25:40 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 27\n",
            "\u001b[32m2022-04-28T20:25:40 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-04-28T20:25:40 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
            "  (model): ViLBERTForClassification(\n",
            "    (bert): ViLBERTBase(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (v_embeddings): BertImageFeatureEmbeddings(\n",
            "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (v_layer): ModuleList(\n",
            "          (0): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (c_layer): ModuleList(\n",
            "          (0): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (t_pooler): BertTextPooler(\n",
            "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (v_pooler): BertImagePooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-04-28T20:25:41 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
            "\u001b[32m2022-04-28T20:25:41 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "\u001b[32m2022-04-28T20:25:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "100% 17/17 [00:06<00:00,  2.54it/s]\n",
            "\u001b[32m2022-04-28T20:25:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 17\n",
            "\u001b[32m2022-04-28T20:25:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T20:25:48 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 1.8556, val/total_loss: 1.8556, val/hateful_memes/accuracy: 0.7130, val/hateful_memes/binary_f1: 0.5401, val/hateful_memes/roc_auc: 0.7049\n",
            "\u001b[32m2022-04-28T20:25:48 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 20s 700ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_run config=\"mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\" \\\n",
        "model=vilbert \\\n",
        "dataset=hateful_memes \\\n",
        "run_type=test \\\n",
        "checkpoint.resume_file=save/best.ckpt \\\n",
        "checkpoint.resume_pretrained=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKm7qP2jXVoa",
        "outputId": "7fad43b2-5870-4d76-84a5-c1fac22e80cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "\u001b[32m2022-04-28T20:28:20 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\n",
            "\u001b[32m2022-04-28T20:28:20 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-04-28T20:28:20 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2022-04-28T20:28:20 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
            "\u001b[32m2022-04-28T20:28:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n",
            "\u001b[32m2022-04-28T20:28:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "\u001b[32m2022-04-28T20:28:20 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-04-28T20:28:20 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False'])\n",
            "\u001b[32m2022-04-28T20:28:20 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-04-28T20:28:20 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-04-28T20:28:20 | mmf_cli.run: \u001b[0mUsing seed 20455463\n",
            "\u001b[32m2022-04-28T20:28:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[32m2022-04-28T20:28:27 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T20:28:27 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T20:28:27 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T20:28:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bi_attention_type\": 1,\n",
            "  \"bi_hidden_size\": 1024,\n",
            "  \"bi_intermediate_size\": 1024,\n",
            "  \"bi_num_attention_heads\": 8,\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cut_first\": \"text\",\n",
            "  \"dynamic_attention\": false,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"fast_mode\": false,\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"fixed_t_layer\": 0,\n",
            "  \"fixed_v_layer\": 0,\n",
            "  \"freeze_base\": false,\n",
            "  \"fusion_method\": \"mul\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hard_cap_seq_len\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"in_batch_pairs\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"vilbert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_negative\": 128,\n",
            "  \"objective\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooling_method\": \"mul\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"t_biattention_id\": [\n",
            "    6,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    11\n",
            "  ],\n",
            "  \"task_specific_tokens\": false,\n",
            "  \"text_only\": false,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"v_attention_probs_dropout_prob\": 0.1,\n",
            "  \"v_biattention_id\": [\n",
            "    0,\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    4,\n",
            "    5\n",
            "  ],\n",
            "  \"v_feature_size\": 2048,\n",
            "  \"v_hidden_act\": \"gelu\",\n",
            "  \"v_hidden_dropout_prob\": 0.1,\n",
            "  \"v_hidden_size\": 1024,\n",
            "  \"v_initializer_range\": 0.02,\n",
            "  \"v_intermediate_size\": 1024,\n",
            "  \"v_num_attention_heads\": 8,\n",
            "  \"v_num_hidden_layers\": 6,\n",
            "  \"v_target_size\": 1601,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"visual_target\": 0,\n",
            "  \"visualization\": false,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"with_coattention\": true\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.v_pooler.dense.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.v_pooler.dense.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.t_pooler.dense.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-04-28T20:28:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-04-28T20:28:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-04-28T20:28:33 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-28T20:28:37 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-28T20:28:37 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-04-28T20:28:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-04-28T20:28:38 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 7000\n",
            "\u001b[32m2022-04-28T20:28:38 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 7000\n",
            "\u001b[32m2022-04-28T20:28:38 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 27\n",
            "\u001b[32m2022-04-28T20:28:38 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-04-28T20:28:38 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
            "  (model): ViLBERTForClassification(\n",
            "    (bert): ViLBERTBase(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (v_embeddings): BertImageFeatureEmbeddings(\n",
            "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (v_layer): ModuleList(\n",
            "          (0): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (c_layer): ModuleList(\n",
            "          (0): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (t_pooler): BertTextPooler(\n",
            "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (v_pooler): BertImagePooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-04-28T20:28:38 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
            "\u001b[32m2022-04-28T20:28:38 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n",
            "\u001b[32m2022-04-28T20:28:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "100% 63/63 [00:19<00:00,  3.16it/s]\n",
            "\u001b[32m2022-04-28T20:28:58 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished training. Loaded 63\n",
            "\u001b[32m2022-04-28T20:28:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
            "\u001b[32m2022-04-28T20:28:58 | mmf.trainers.callbacks.logistics: \u001b[0mtest/hateful_memes/cross_entropy: 1.9610, test/total_loss: 1.9610, test/hateful_memes/accuracy: 0.7030, test/hateful_memes/binary_f1: 0.5058, test/hateful_memes/roc_auc: 0.7324\n",
            "\u001b[32m2022-04-28T20:28:58 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 24s 503ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pde3jQE06IkW",
        "outputId": "959148c3-5afc-425d-fb20-ece8e218acd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find -type f -name '*.json' \n",
        "# -printf '.' | wc -c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IUYvnSLyUn8",
        "outputId": "ef11e443-4f67-4d49-976f-756b27c998e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./mmf-hateful-memes/website/package.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_predict config=\"mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\" \\\n",
        "model=vilbert \\\n",
        "dataset=hateful_memes \\\n",
        "run_type=test \\\n",
        "checkpoint.resume_file=save/best.ckpt \\\n",
        "checkpoint.resume_pretrained=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2okQRfxiyWg2",
        "outputId": "ba37aa30-69fa-489e-da28-c0d3bc2f86dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf_cli.run: \u001b[0mUsing seed 15662202\n",
            "\u001b[32m2022-04-28T20:33:15 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[32m2022-04-28T20:33:22 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T20:33:22 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T20:33:22 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T20:33:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bi_attention_type\": 1,\n",
            "  \"bi_hidden_size\": 1024,\n",
            "  \"bi_intermediate_size\": 1024,\n",
            "  \"bi_num_attention_heads\": 8,\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cut_first\": \"text\",\n",
            "  \"dynamic_attention\": false,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"fast_mode\": false,\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"fixed_t_layer\": 0,\n",
            "  \"fixed_v_layer\": 0,\n",
            "  \"freeze_base\": false,\n",
            "  \"fusion_method\": \"mul\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hard_cap_seq_len\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"in_batch_pairs\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"vilbert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_negative\": 128,\n",
            "  \"objective\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooling_method\": \"mul\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"t_biattention_id\": [\n",
            "    6,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    11\n",
            "  ],\n",
            "  \"task_specific_tokens\": false,\n",
            "  \"text_only\": false,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"v_attention_probs_dropout_prob\": 0.1,\n",
            "  \"v_biattention_id\": [\n",
            "    0,\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    4,\n",
            "    5\n",
            "  ],\n",
            "  \"v_feature_size\": 2048,\n",
            "  \"v_hidden_act\": \"gelu\",\n",
            "  \"v_hidden_dropout_prob\": 0.1,\n",
            "  \"v_hidden_size\": 1024,\n",
            "  \"v_initializer_range\": 0.02,\n",
            "  \"v_intermediate_size\": 1024,\n",
            "  \"v_num_attention_heads\": 8,\n",
            "  \"v_num_hidden_layers\": 6,\n",
            "  \"v_target_size\": 1601,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"visual_target\": 0,\n",
            "  \"visualization\": false,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"with_coattention\": true\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.v_pooler.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.t_pooler.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.t_pooler.dense.weight', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.v_pooler.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biattention.query1.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-04-28T20:33:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-04-28T20:33:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-04-28T20:33:28 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-28T20:33:34 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-28T20:33:34 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-04-28T20:33:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-04-28T20:33:34 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 7000\n",
            "\u001b[32m2022-04-28T20:33:34 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 7000\n",
            "\u001b[32m2022-04-28T20:33:34 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 27\n",
            "\u001b[32m2022-04-28T20:33:34 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
            "\u001b[32m2022-04-28T20:33:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "100% 63/63 [00:26<00:00,  2.42it/s]\n",
            "\u001b[32m2022-04-28T20:34:00 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/vilbert_hugo/hateful-memes/hateful-memes/save/hateful_memes_vilbert_15662202/reports/hateful_memes_run_test_2022-04-28T20:34:00.csv\n",
            "\u001b[32m2022-04-28T20:34:00 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 63\n",
            "\u001b[32m2022-04-28T20:34:00 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "QrGKwIy7qSye",
        "outputId": "c20b8173-9614-43f8-bf14-090983eab5e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/DL7643_Group_project/vilbert/hateful-memes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_predict config=\"mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\" \\\n",
        "model=vilbert \\\n",
        "dataset=hateful_memes \\\n",
        "run_type=val \\\n",
        "checkpoint.resume_file=save/best.ckpt \\\n",
        "checkpoint.resume_pretrained=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYcrwAfSDbHe",
        "outputId": "f009b2ae-5917-4693-908a-1f45ab0e7eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf.utils.configuration: \u001b[0mOverriding option config to mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to save/best.ckpt\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  category=UserWarning,\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=mmf-hateful-memes/projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=save/best.ckpt', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf_cli.run: \u001b[0mUsing seed 25867941\n",
            "\u001b[32m2022-04-28T20:46:25 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "\u001b[32m2022-04-28T20:46:33 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T20:46:33 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T20:46:33 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2022-04-28T20:46:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bert_model_name\": \"bert-base-uncased\",\n",
            "  \"bi_attention_type\": 1,\n",
            "  \"bi_hidden_size\": 1024,\n",
            "  \"bi_intermediate_size\": 1024,\n",
            "  \"bi_num_attention_heads\": 8,\n",
            "  \"bypass_transformer\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cut_first\": \"text\",\n",
            "  \"dynamic_attention\": false,\n",
            "  \"embedding_strategy\": \"plain\",\n",
            "  \"fast_mode\": false,\n",
            "  \"finetune_lr_multiplier\": 1,\n",
            "  \"fixed_t_layer\": 0,\n",
            "  \"fixed_v_layer\": 0,\n",
            "  \"freeze_base\": false,\n",
            "  \"fusion_method\": \"mul\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hard_cap_seq_len\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"in_batch_pairs\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"losses\": [\n",
            "    \"cross_entropy\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model\": \"vilbert\",\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_negative\": 128,\n",
            "  \"objective\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooling_method\": \"mul\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"random_initialize\": false,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"t_biattention_id\": [\n",
            "    6,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    11\n",
            "  ],\n",
            "  \"task_specific_tokens\": false,\n",
            "  \"text_only\": false,\n",
            "  \"training_head_type\": \"classification\",\n",
            "  \"transformers_version\": \"4.10.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"v_attention_probs_dropout_prob\": 0.1,\n",
            "  \"v_biattention_id\": [\n",
            "    0,\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    4,\n",
            "    5\n",
            "  ],\n",
            "  \"v_feature_size\": 2048,\n",
            "  \"v_hidden_act\": \"gelu\",\n",
            "  \"v_hidden_dropout_prob\": 0.1,\n",
            "  \"v_hidden_size\": 1024,\n",
            "  \"v_initializer_range\": 0.02,\n",
            "  \"v_intermediate_size\": 1024,\n",
            "  \"v_num_attention_heads\": 8,\n",
            "  \"v_num_hidden_layers\": 6,\n",
            "  \"v_target_size\": 1601,\n",
            "  \"visual_embedding_dim\": 2048,\n",
            "  \"visual_target\": 0,\n",
            "  \"visualization\": false,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"with_coattention\": true\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.t_pooler.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.v_embeddings.image_embeddings.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.t_pooler.dense.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.v_embeddings.image_embeddings.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.v_pooler.dense.bias', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.v_pooler.dense.weight', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-04-28T20:46:39 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-04-28T20:46:39 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-04-28T20:46:39 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-28T20:46:43 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-04-28T20:46:43 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-04-28T20:46:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-04-28T20:46:43 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 7000\n",
            "\u001b[32m2022-04-28T20:46:43 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 7000\n",
            "\u001b[32m2022-04-28T20:46:43 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 27\n",
            "\u001b[32m2022-04-28T20:46:43 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n",
            "\u001b[32m2022-04-28T20:46:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "100% 17/17 [00:07<00:00,  2.33it/s]\n",
            "\u001b[32m2022-04-28T20:46:50 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/vilbert_hugo/hateful-memes/hateful-memes/save/hateful_memes_vilbert_25867941/reports/hateful_memes_run_val_2022-04-28T20:46:50.csv\n",
            "\u001b[32m2022-04-28T20:46:50 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 17\n",
            "\u001b[32m2022-04-28T20:46:50 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. roberta\n",
        "# 2. data aug\n"
      ],
      "metadata": {
        "id": "pnQU7yMqEJun"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}